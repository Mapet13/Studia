{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci neuronowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp\n",
    "\n",
    "Celem laboratorium jest zapoznanie się z podstawami sieci neuronowych oraz uczeniem głębokim (*deep learning*). Zapoznasz się na nim z następującymi tematami:\n",
    "- treningiem prostych sieci neuronowych, w szczególności z:\n",
    "  - regresją liniową w sieciach neuronowych\n",
    "  - optymalizacją funkcji kosztu\n",
    "  - algorytmem spadku wzdłuż gradientu\n",
    "  - siecią typu Multilayer Perceptron (MLP)\n",
    "- frameworkiem PyTorch, w szczególności z:\n",
    "  - ładowaniem danych\n",
    "  - preprocessingiem danych\n",
    "  - pisaniem pętli treningowej i walidacyjnej\n",
    "  - walidacją modeli\n",
    "- architekturą i hiperaprametrami sieci MLP, w szczególności z:\n",
    "  - warstwami gęstymi (w pełni połączonymi)\n",
    "  - funkcjami aktywacji\n",
    "  - regularyzacją: L2, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystywane biblioteki\n",
    "\n",
    "Zaczniemy od pisania ręcznie prostych sieci w bibliotece Numpy, służącej do obliczeń numerycznych na CPU. Później przejdziemy do wykorzystywania frameworka PyTorch, służącego do obliczeń numerycznych na CPU, GPU oraz automatycznego różniczkowania, wykorzystywanego głównie do treningu sieci neuronowych.\n",
    "\n",
    "Wykorzystamy PyTorcha ze względu na popularność, łatwość instalacji i użycia, oraz dużą kontrolę nad niskopoziomowymi aspektami budowy i treningu sieci neuronowych. Framework ten został stworzony do zastosowań badawczych i naukowych, ale ze względu na wygodę użycia stał się bardzo popularny także w przemyśle. W szczególności całkowicie zdominował przetwarzanie języka naturalnego (NLP) oraz uczenie na grafach.\n",
    "\n",
    "Pierwszy duży framework do deep learningu, oraz obecnie najpopularniejszy, to TensorFlow, wraz z wysokopoziomową nakładką Keras. Są jednak szanse, że Google (autorzy) będzie go powoli porzucać na rzecz ich nowego frameworka JAX ([dyskusja](https://www.reddit.com/r/MachineLearning/comments/vfl57t/d_google_quietly_moving_its_products_from/), [artykuł Business Insidera](https://www.businessinsider.com/facebook-pytorch-beat-google-tensorflow-jax-meta-ai-2022-6?IR=T)), który jest bardzo świeżym, ale ciekawym narzędziem.\n",
    "\n",
    "Trzecia, ale znacznie mniej popularna od powyższych opcja to Apache MXNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja własnego komputera\n",
    "\n",
    "Jeżeli korzystasz z własnego komputera, to musisz zainstalować trochę więcej bibliotek (Google Colab ma je już zainstalowane).\n",
    "\n",
    "Jeżeli nie masz GPU lub nie chcesz z niego korzystać, to wystarczy znaleźć odpowiednią komendę CPU [na stronie PyTorcha](https://pytorch.org/get-started/locally/). Dla Anacondy odpowiednia komenda została podana poniżej, dla pip'a znajdź ją na stronie.\n",
    "\n",
    "Jeżeli chcesz korzystać ze wsparcia GPU (na tym laboratorium nie będzie potrzebne, na kolejnych może przyspieszyć nieco obliczenia), to musi być to odpowiednio nowa karta NVidii, mająca CUDA compatibility ([lista](https://developer.nvidia.com/cuda-gpus)). Poza PyTorchem będzie potrzebne narzędzie NVidia CUDA w wersji 11.6 lub 11.7. Instalacja na Windowsie jest bardzo prosta (wystarczy ściągnąć plik EXE i zainstalować jak każdy inny program). Instalacja na Linuxie jest trudna i można względnie łatwo zepsuć sobie system, ale jeżeli chcesz spróbować, to [ten tutorial](https://www.youtube.com/results?search_query=nvidia+cuda+install+ubuntu+20.04) jest bardzo dobry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for conda users\n",
    "#!conda install -y matplotlib pandas pytorch torchvision torchaudio -c pytorch -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Othm3C2lLAsj"
   },
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Zanim zaczniemy naszą przygodę z sieciami neuronowymi, przyjrzyjmy się prostemu przykładowi regresji liniowej na syntetycznych danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "rnJsfxbnLAsj"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "EaYpEXzBLAsl",
    "outputId": "2f8d2922-72f0-4d38-8548-d1262adf522e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7faf2605f6a0>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x = np.linspace(0, 1, 100, dtype=np.float64)\n",
    "y = x + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEM_-yKELAsl"
   },
   "source": [
    "W przeciwieństwie do laboratorium 1, tym razem będziemy chcieli rozwiązać ten problem własnoręcznie, bez użycia wysokopoziomowego interfejsu Scikit-learn'a. W tym celu musimy sobie przypomnieć sformułowanie naszego **problemu optymalizacyjnego (optimization problem)**.\n",
    "\n",
    "W przypadku prostej regresji liniowej (1 zmienna) mamy model postaci $\\hat{y} = \\alpha x + \\beta$, z dwoma parametrami, których będziemy się uczyć. Miarą niedopasowania modelu o danych parametrach jest **funkcja kosztu (cost function)**, nazywana też funkcją celu. Najczęściej używa się **błędu średniokwadratowego (mean squared error, MSE)**:\n",
    "$$\\large\n",
    "MSE = \\frac{1}{N} \\sum_{i}^{N} (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Od jakich $\\alpha$ i $\\beta$ zacząć? W najprostszym wypadku wystarczy po prostu je wylosować jako niewielkie liczby zmiennoprzecinkowe.\n",
    "\n",
    "#### Zadanie 1 (0.5 punkt)\n",
    "\n",
    "Uzupełnij kod funkcji `mse`, obliczającej błąd średniokwadratowy. Wykorzystaj Numpy'a w celu wektoryzacji obliczeń dla wydajności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaA7Q46TLAsm",
    "outputId": "5c57fe58-1934-4d21-9a7b-d14e9a23140b"
   },
   "outputs": [],
   "source": [
    "def mse(y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    N = len(y)\n",
    "    errors = y - y_hat\n",
    "    return np.mean(np.multiply(errors, errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "qSGfamGbLAsm",
    "outputId": "733ce15f-ae75-466b-e7ee-eb3c9d9d534c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf26038a30>]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.random.rand()\n",
    "b = np.random.rand()\n",
    "\n",
    "print(f\"MSE: {mse(y, a * x + b):.3f}\")\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, a * x + b, color=\"g\", linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y--E9Mp9LAsn"
   },
   "source": [
    "Losowe parametry radzą sobie nie najlepiej. Jak lepiej dopasować naszą prostą do danych? Zawsze możemy starać się wyprowadzić rozwiązanie analitycznie, i w tym wypadku nawet nam się uda. Jest to jednak szczególny i dość rzadki przypadek, a w szczególności nie będzie to możliwe w większych sieciach neuronowych.\n",
    "\n",
    "Potrzebna nam będzie **metoda optymalizacji (optimization method)**, dającą wartości parametrów minimalizujące dowolną różniczkowalną funkcję kosztu. Zdecydowanie najpopularniejszy jest tutaj **spadek wzdłuż gradientu (gradient descent)**.\n",
    "\n",
    "Metoda ta wywodzi się z prostych obserwacji, które tutaj przedstawimy. Bardziej szczegółowe rozwinięcie dla zainteresowanych: [sekcja 4.3 \"Deep Learning Book\"](https://www.deeplearningbook.org/contents/numerical.html), [ten praktyczny kurs](https://cs231n.github.io/optimization-1/), [analiza oryginalnej publikacji Cauchy'ego](https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf) (oryginał w języku francuskim).\n",
    "\n",
    "Pochodna jest dokładnie równa granicy funkcji. Dla małego $\\epsilon$ można ją przybliżyć jako:\n",
    "$$\\large\n",
    "\\frac{f(x)}{dx} \\approx \\frac{f(x) - f(x+\\epsilon)}{\\epsilon}\n",
    "$$\n",
    "\n",
    "Przyglądając się temu równaniu widzimy, że: \n",
    "* dla funkcji rosnącej ($f(x+\\epsilon) > f(x)$) wyrażenie $\\frac{f(x)}{dx}$ będzie miało znak ujemny \n",
    "* dla funkcji malejącej ($f(x+\\epsilon) < f(x)$) wyrażenie $\\frac{f(x)}{dx}$ będzie miało znak dodatni \n",
    "\n",
    "Widzimy więc, że potrafimy wskazać kierunek zmniejszenia wartości funkcji, patrząc na znak pochodnej. Zaobserwowano także, że amplituda wartości w $\\frac{f(x)}{dx}$ jest tym większa, im dalej jesteśmy od minimum (maximum). Pochodna wyznacza więc, w jakim kierunku funkcja najszybciej rośnie, więc kierunek o przeciwnym zwrocie to kierunek, w którym funkcja najszybciej spada.\n",
    "\n",
    "Stosując powyższe do optymalizacji, mamy:\n",
    "$$\\large\n",
    "x_{t+1} = x_{t} -  \\alpha * \\frac{f(x)}{dx}\n",
    "$$\n",
    "\n",
    "$\\alpha$ to niewielka wartość (rzędu zwykle $10^{-5}$ - $10^{-2}$), wprowadzona, aby trzymać się założenia o małej zmianie parametrów ($\\epsilon$). Nazywa się ją **stałą uczącą (learning rate)** i jest zwykle najważniejszym hiperparametrem podczas nauki sieci.\n",
    "\n",
    "Metoda ta zakłada, że używamy całego zbioru danych do aktualizacji parametrów w każdym kroku, co nazywa się po prostu GD (od *gradient descent*) albo *full batch GD*. Wtedy każdy krok optymalizacji nazywa się **epoką (epoch)**.\n",
    "\n",
    "Im większa stała ucząca, tym większe nasze kroki podczas minimalizacji. Możemy więc uczyć szybciej, ale istnieje ryzyko, że będziemy \"przeskakiwać\" minima. Mniejsza stała ucząca to wolniejszy trening, ale dokładniejszy. Można także zmieniać ją podczas treningu, co nazywa się **learning rate scheduling (LR scheduling)**. Obrazowo:\n",
    "\n",
    "![learning_rate](http://www.bdhammel.com/assets/learning-rate/lr-types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "496qEjkVLAso"
   },
   "source": [
    "![interactive LR](http://cdn-images-1.medium.com/max/640/1*eeIvlwkMNG1wSmj3FR6M2g.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYkyAHKzLAsp"
   },
   "source": [
    "Policzmy więc pochodną dla naszej funkcji kosztu MSE. Pochodną liczymy po predykcjach naszego modelu, czyli de facto po jego parametrach, bo to od nich zależą predykcje.\n",
    "$$\\large\n",
    "\\frac{\\text{d} MSE}{\\text{d} \\hat{y}} = -2 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i) = -2 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (ax + b))\n",
    "$$\n",
    "\n",
    "Musimy jeszcze się dowiedzieć, jak zaktualizować każdy z naszych parametrów. Możemy wykorzystać tutaj regułę łańcuchową (*chain rule*) i policzyć ponownie pochodną, tylko że po naszych parametrach. Dzięki temu dostajemy informację, jak każdy z parametrów wpływa na funkcję kosztu i jak zmodyfikować każdy z nich w kolejnym kroku.\n",
    "$$\\large\n",
    "\\frac{\\text{d} \\hat{y}}{\\text{d} a} = x\n",
    "$$\n",
    "\n",
    "$$\\large\n",
    "\\frac{\\text{d} \\hat{y}}{\\text{d} b} = 1\n",
    "$$\n",
    "\n",
    "Pełna aktualizacja to zatem:\n",
    "$$\\large\n",
    "a' = a + \\alpha * \\left( \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i) * (-x) \\right)\n",
    "$$\n",
    "$$\\large\n",
    "b' = b + \\alpha * \\left( \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i) * (-1) \\right)\n",
    "$$\n",
    "\n",
    "Liczymy więc pochodną funkcji kosztu, a potem za pomocą reguły łańcuchowej \"cofamy się\", dochodząc do tego, jak każdy z parametrów wpływa na błąd i w jaki sposób powinniśmy go zmienić. Nazywa się to **propagacją wsteczną (backpropagation)** i jest podstawowym mechanizmem umożliwiającym naukę sieci neuronowych za pomocą spadku wzdłuż gradientu. Więcej możesz o tym przeczytać [tutaj](https://cs231n.github.io/optimization-2/).\n",
    "\n",
    "Obliczenie pochodnych cząstkowych ze względu na każdy \n",
    "\n",
    "\n",
    "#### Zadanie 2 (1.5 punkty)\n",
    "\n",
    "Zaimplementuj funkcję realizującą jedną epokę treningową. Oblicz predykcję przy aktualnych parametrach oraz zaktualizuj je zgodnie z powyższymi wzorami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qbdWOSULAsp",
    "outputId": "055607ae-87aa-470a-e6da-25682c82d470"
   },
   "outputs": [],
   "source": [
    "def optimize(\n",
    "    x: np.ndarray, y: np.ndarray, a: float, b: float, learning_rate: float = 0.1\n",
    "):\n",
    "    y_hat = a * x + b\n",
    "    errors = y - y_hat\n",
    "\n",
    "    N = len(y)\n",
    "\n",
    "    new_a = a + (learning_rate * -2 / N * ( np.sum(np.multiply(errors, x * -1)))) \n",
    "    new_b = b + (learning_rate * -2 / N * ( np.sum(np.multiply(errors, -1))))\n",
    "\n",
    "    return new_a, new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss:  0.1330225119404028\n",
      "step 100 loss:  0.012673197778527677\n",
      "step 200 loss:  0.010257153540857817\n",
      "step 300 loss:  0.0100948037549359\n",
      "step 400 loss:  0.010083894412889118\n",
      "step 500 loss:  0.010083161342973332\n",
      "step 600 loss:  0.010083112083219709\n",
      "step 700 loss:  0.010083108773135261\n",
      "step 800 loss:  0.010083108550709076\n",
      "step 900 loss:  0.01008310853576281\n",
      "final loss: 0.010083108534760455\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    loss = mse(y, a * x + b)\n",
    "    a, b = optimize(x, y, a, b)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"step {i} loss: \", loss)\n",
    "\n",
    "print(\"final loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "xOgRcPC1LAsq",
    "outputId": "85b0b3e4-aa0d-467a-d8ff-5f01be17b243",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf25fa0430>]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, a * x + b, color=\"g\", linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOr2fWYpLAsq"
   },
   "source": [
    "Udało ci się wytrenować swoją pierwszą sieć neuronową. Czemu? Otóż neuron to po prostu wektor parametrów, a zwykle robimy iloczyn skalarny tych parametrów z wejściem. Dodatkowo na wyjście nakłada się **funkcję aktywacji (activation function)**, która przekształca wyjście. Tutaj takiej nie było, a właściwie była to po prostu funkcja identyczności.\n",
    "\n",
    "Oczywiście w praktyce korzystamy z odpowiedniego frameworka, który w szczególności:\n",
    "- ułatwia budowanie sieci, np. ma gotowe klasy dla warstw neuronów\n",
    "- ma zaimplementowane funkcje kosztu oraz ich pochodne\n",
    "- sam różniczkuje ze względu na odpowiednie parametry i aktualizuje je odpowiednio podczas treningu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJBYJabuLAsr"
   },
   "source": [
    "## Wprowadzenie do PyTorcha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB-99XqhLAsr"
   },
   "source": [
    "PyTorch to w gruncie rzeczy narzędzie do algebry liniowej z [automatycznym rożniczkowaniem](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html), z możliwością przyspieszenia obliczeń z pomocą GPU. Na tych fundamentach zbudowany jest pełny framework do uczenia głębokiego. Można spotkać się ze stwierdzenie, że PyTorch to NumPy + GPU + opcjonalne różniczkowanie, co jest całkiem celne. Plus można łatwo debugować printem :)\n",
    "\n",
    "PyTorch używa dynamicznego grafu obliczeń, który sami definiujemy w kodzie. Takie podejście jest bardzo wygodne, elastyczne i pozwala na łatwe eksperymentowanie. Odbywa się to potencjalnie kosztem wydajności, ponieważ pozostawia kwestię optymalizacji programiście. Więcej na ten temat dla zainteresowanych na końcu laboratorium.\n",
    "\n",
    "Samo API PyTorcha bardzo przypomina Numpy'a, a podstawowym obiektem jest `Tensor`, klasa reprezentująca tensory dowolnego wymiaru. Dodatkowo niektóre tensory będą miały automatycznie obliczony gradient. Co ważne, tensor jest na pewnym urządzeniu, CPU lub GPU, a przenosić między nimi trzeba explicite.\n",
    "\n",
    "Najważniejsze moduły:\n",
    "- `torch` - podstawowe klasy oraz funkcje, np. `Tensor`, `from_numpy()`\n",
    "- `torch.nn` - klasy związane z sieciami neuronowymi, np. `Linear`, `Sigmoid`\n",
    "- `torch.optim` - wszystko związane z optymalizacją, głównie spadkiem wzdłuż gradientu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "FwuIt8S-LAss"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfCiUFXULAss",
    "outputId": "83f6231d-ecc4-461a-b758-fdc4bc2a88a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4895, 1.3150, 1.2865, 1.0210, 1.4868, 1.3357, 1.7666, 1.0233, 1.6025,\n",
      "        1.6635])\n",
      "tensor([0.4895, 0.3150, 0.2865, 0.0210, 0.4868, 0.3357, 0.7666, 0.0233, 0.6025,\n",
      "        0.6635])\n",
      "tensor(3.9902)\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(10)\n",
    "noise = torch.ones(10) * torch.rand(10)\n",
    "\n",
    "# elementwise sum\n",
    "print(ones + noise)\n",
    "\n",
    "# elementwise multiplication\n",
    "print(ones * noise)\n",
    "\n",
    "# dot product\n",
    "print(ones @ noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "ynNd_kD0LAst"
   },
   "outputs": [],
   "source": [
    "# beware - shares memory with original Numpy array!\n",
    "# very fast, but modifications are visible to original variable\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9kkxczELAsu"
   },
   "source": [
    "Jeżeli dla stworzonych przez nas tensorów chcemy śledzić operacje i obliczać gradient, to musimy oznaczyć `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HtZL-KfLAsu",
    "outputId": "47c6d930-5678-452a-95bc-227935138b40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1436], requires_grad=True), tensor([0.1541], requires_grad=True))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nl1guWZ_LAsv"
   },
   "source": [
    "PyTorch zawiera większość powszechnie używanych funkcji kosztu, np. MSE. Mogą być one używane na 2 sposoby, z czego pierwszy jest popularniejszy:\n",
    "- jako klasy wywoływalne z modułu `torch.nn`\n",
    "- jako funkcje z modułu `torch.nn.functional`\n",
    "\n",
    "Po wykonaniu poniższego kodu widzimy, że zwraca on nam tensor z dodatkowymi atrybutami. Co ważne, jest to skalar (0-wymiarowy tensor), bo potrzebujemy zwyczajnej liczby do obliczania propagacji wstecznych (pochodnych czątkowych)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1466, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "mse(y, a * x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS35r49nLAsw"
   },
   "source": [
    "Atrybutu `grad_fn` nie używamy wprost, bo korzysta z niego w środku PyTorch, ale widać, że tensor jest \"świadomy\", że liczy się na nim pochodną. Możemy natomiast skorzystać z atrybutu `grad`, który zawiera faktyczny gradient. Zanim go jednak dostaniemy, to trzeba powiedzieć PyTorchowi, żeby policzył gradient. Służy do tego metoda `.backward()`, wywoływana na obiekcie zwracanym przez funkcję kosztu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Qb7l6Xg1LAsx"
   },
   "outputs": [],
   "source": [
    "loss = mse(y, a * x + b)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LfQbLVoLAsx",
    "outputId": "d5b87fb7-d284-423c-f467-b677384b2f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3112])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdf1iweELAsy"
   },
   "source": [
    "Ważne jest, że PyTorch nie liczy za każdym razem nowego gradientu, tylko dodaje go do istniejącego, czyli go akumuluje. Jest to przydatne w niektórych sieciach neuronowych, ale zazwyczaj trzeba go zerować. Jeżeli tego nie zrobimy, to dostaniemy coraz większe gradienty.\n",
    "\n",
    "Do zerowania służy metoda `.zero_()`. W PyTorchu wszystkie metody modyfikujące tensor w miejscu mają `_` na końcu nazwy. Jest to dość niskopoziomowa operacja dla pojedynczych tensorów - zobaczymy za chwilę, jak to robić łatwiej dla całej sieci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiCQZKJsLAsy",
    "outputId": "2f779622-480d-43fc-b9d0-a0e36ff4b28b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6225])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(y, a * x + b)\n",
    "loss.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNC3Ag8uLAsz"
   },
   "source": [
    "Zobaczmy, jak wyglądałaby regresja liniowa, ale napisana w PyTorchu. Jest to oczywiście bardzo niskopoziomowa implementacja - za chwilę zobaczymy, jak to wygląda w praktyce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKnxyeboLAsz",
    "outputId": "2f939474-901a-4773-9704-686a40ae6e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss:  tensor(0.1720, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 100 loss:  tensor(0.0119, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 200 loss:  tensor(0.0102, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 300 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 400 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 500 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 600 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 700 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 800 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 900 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "final loss: tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "for i in range(1000):\n",
    "    loss = mse(y, a * x + b)\n",
    "\n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    a.data -= learning_rate * a.grad\n",
    "    b.data -= learning_rate * b.grad\n",
    "\n",
    "    # zero gradients\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"step {i} loss: \", loss)\n",
    "\n",
    "print(\"final loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DXNVhshmmI-"
   },
   "source": [
    "Trening modeli w PyTorchu jest dosyć schematyczny i najczęściej rozdziela się go na kilka bloków, dających razem **pętlę uczącą (training loop)**, powtarzaną w każdej epoce:\n",
    "1. Forward pass - obliczenie predykcji sieci\n",
    "2. Loss calculation\n",
    "3. Backpropagation - obliczenie pochodnych oraz zerowanie gradientów\n",
    "4. Optimalization - aktualizacja wag\n",
    "5. Other - ewaluacja na zbiorze walidacyjnym, logging etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2etpw7TNLAs0",
    "outputId": "8ac35c12-6c70-41ec-bf57-414456fc3c96",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss: 0.2087\n",
      "step 100 loss: 0.0135\n",
      "step 200 loss: 0.0103\n",
      "step 300 loss: 0.0101\n",
      "step 400 loss: 0.0101\n",
      "step 500 loss: 0.0101\n",
      "step 600 loss: 0.0101\n",
      "step 700 loss: 0.0101\n",
      "step 800 loss: 0.0101\n",
      "step 900 loss: 0.0101\n",
      "final loss: tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "learning_rate = 0.1\n",
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([a, b], lr=learning_rate)\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "# training loop in each epoch\n",
    "for i in range(1000):\n",
    "    # forward pass\n",
    "    y_hat = a * x + b\n",
    "\n",
    "    # loss calculation\n",
    "    loss = mse(y, y_hat)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()  # zeroes all gradients - very convenient!\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        if loss < best_loss:\n",
    "            best_model = (a.clone(), b.clone())\n",
    "            best_loss = loss\n",
    "        print(f\"step {i} loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"final loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdziemy teraz do budowy sieci neuronowej do klasyfikacji. Typowo implementuje się ją po prostu jako sieć dla regresji, ale zwracającą tyle wyników, ile mamy klas, a potem aplikuje się na tym funkcję sigmoidalną (2 klasy) lub softmax (>2 klasy). W przypadku klasyfikacji binarnej zwraca się czasem tylko 1 wartość, przepuszczaną przez sigmoidę - wtedy wyjście z sieci to prawdopodobieństwo klasy pozytywnej.\n",
    "\n",
    "Funkcją kosztu zwykle jest **entropia krzyżowa (cross-entropy)**, stosowana też w klasycznej regresji logistycznej. Co ważne, sieci neuronowe, nawet tak proste, uczą się szybciej i stabilniej, gdy dane na wejściu (a przynajmniej zmienne numeryczne) są **ustandaryzowane (standardized)**. Operacja ta polega na odjęciu średniej i podzieleniu przez odchylenie standardowe (tzw. *Z-score transformation*).\n",
    "\n",
    "**Uwaga - PyTorch wymaga tensora klas będącego liczbami zmiennoprzecinkowymi!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbiór danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na tym laboratorium wykorzystamy zbiór [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult). Dotyczy on przewidywania na podstawie danych demograficznych, czy dany człowiek zarabia powyżej 50 tysięcy dolarów miesięcznie, czy też mniej. Jest to cenna informacja np. przy planowaniu kampanii marketingowych. Jak możesz się domyślić, zbiór pochodzi z czasów, kiedy inflacja była dużo niższa :)\n",
    "\n",
    "Poniżej znajduje się kod do ściągnięcia i preprocessingu zbioru. Nie musisz go dokładnie analizować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DNsaZAnLAs0",
    "outputId": "70822008-530d-4173-deb9-8149a9fe5b41",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K', nan], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"wage\"\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "age: continuous.\n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "fnlwgt: continuous.\n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "education-num: continuous.\n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "sex: Female, Male.\n",
    "capital-gain: continuous.\n",
    "capital-loss: continuous.\n",
    "hours-per-week: continuous.\n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"adult.data\", header=None, names=columns)\n",
    "df.wage.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribution: https://www.kaggle.com/code/royshih23/topic7-classification-in-python\n",
    "df['education'].replace('Preschool', 'dropout',inplace=True)\n",
    "df['education'].replace('10th', 'dropout',inplace=True)\n",
    "df['education'].replace('11th', 'dropout',inplace=True)\n",
    "df['education'].replace('12th', 'dropout',inplace=True)\n",
    "df['education'].replace('1st-4th', 'dropout',inplace=True)\n",
    "df['education'].replace('5th-6th', 'dropout',inplace=True)\n",
    "df['education'].replace('7th-8th', 'dropout',inplace=True)\n",
    "df['education'].replace('9th', 'dropout',inplace=True)\n",
    "df['education'].replace('HS-Grad', 'HighGrad',inplace=True)\n",
    "df['education'].replace('HS-grad', 'HighGrad',inplace=True)\n",
    "df['education'].replace('Some-college', 'CommunityCollege',inplace=True)\n",
    "df['education'].replace('Assoc-acdm', 'CommunityCollege',inplace=True)\n",
    "df['education'].replace('Assoc-voc', 'CommunityCollege',inplace=True)\n",
    "df['education'].replace('Bachelors', 'Bachelors',inplace=True)\n",
    "df['education'].replace('Masters', 'Masters',inplace=True)\n",
    "df['education'].replace('Prof-school', 'Masters',inplace=True)\n",
    "df['education'].replace('Doctorate', 'Doctorate',inplace=True)\n",
    "\n",
    "df['marital-status'].replace('Never-married', 'NotMarried',inplace=True)\n",
    "df['marital-status'].replace(['Married-AF-spouse'], 'Married',inplace=True)\n",
    "df['marital-status'].replace(['Married-civ-spouse'], 'Married',inplace=True)\n",
    "df['marital-status'].replace(['Married-spouse-absent'], 'NotMarried',inplace=True)\n",
    "df['marital-status'].replace(['Separated'], 'Separated',inplace=True)\n",
    "df['marital-status'].replace(['Divorced'], 'Separated',inplace=True)\n",
    "df['marital-status'].replace(['Widowed'], 'Widowed',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiOxs_6mLAs1",
    "outputId": "c95418cf-2632-41d0-de0a-9caf109de113",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12736, 108), (12736,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "X = df.copy()\n",
    "y = (X.pop(\"wage\") == ' >50K').astype(int).values\n",
    "\n",
    "train_valid_size = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=train_valid_size, \n",
    "    random_state=0, \n",
    "    shuffle=True, \n",
    "    stratify=y\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=train_valid_size, \n",
    "    random_state=0, \n",
    "    shuffle=True, \n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "continuous_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "continuous_X_train = X_train[continuous_cols]\n",
    "categorical_X_train = X_train.loc[:, ~X_train.columns.isin(continuous_cols)]\n",
    "\n",
    "continuous_X_valid = X_valid[continuous_cols]\n",
    "categorical_X_valid = X_valid.loc[:, ~X_valid.columns.isin(continuous_cols)]\n",
    "\n",
    "continuous_X_test = X_test[continuous_cols]\n",
    "categorical_X_test = X_test.loc[:, ~X_test.columns.isin(continuous_cols)]\n",
    "\n",
    "categorical_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "continuous_scaler = StandardScaler() #MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "categorical_encoder.fit(categorical_X_train)\n",
    "continuous_scaler.fit(continuous_X_train)\n",
    "\n",
    "continuous_X_train = continuous_scaler.transform(continuous_X_train)\n",
    "continuous_X_valid = continuous_scaler.transform(continuous_X_valid)\n",
    "continuous_X_test = continuous_scaler.transform(continuous_X_test)\n",
    "\n",
    "categorical_X_train = categorical_encoder.transform(categorical_X_train)\n",
    "categorical_X_valid = categorical_encoder.transform(categorical_X_valid)\n",
    "categorical_X_test = categorical_encoder.transform(categorical_X_test)\n",
    "\n",
    "X_train = np.concatenate([continuous_X_train, categorical_X_train], axis=1)\n",
    "X_valid = np.concatenate([continuous_X_valid, categorical_X_valid], axis=1)\n",
    "X_test = np.concatenate([continuous_X_test, categorical_X_test], axis=1)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga co do typów - PyTorchu wszystko w sieci neuronowej musi być typu `float32`. W szczególności trzeba uważać na konwersje z Numpy'a, który używa domyślnie typu `float64`. Może ci się przydać metoda `.float()`.\n",
    "\n",
    "Uwaga co do kształtów wyjścia - wejścia do `nn.BCELoss` muszą być tego samego kształtu. Może ci się przydać metoda `.squeeze()` lub `.unsqueeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "qfRA3xEoLAs1"
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float().unsqueeze(-1)\n",
    "\n",
    "X_valid = torch.from_numpy(X_valid).float()\n",
    "y_valid = torch.from_numpy(y_valid).float().unsqueeze(-1)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobnie jak w laboratorium 2, mamy tu do czynienia z klasyfikacją niezbalansowaną:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAToUlEQVR4nO3df7RdZX3n8feHBCqiQDA3afhRUtuIP7oqdW5Rh7EzNWUGkGkyq9IF/mik2FhHpnZ0rNHV5WpdtU2nUy1d/WWW1Kaj1kYUk8FayaSidWmFi6ZWCRpEBDQkFxQQGH+A3/nj7FsPl5Pcc3Pvyc0T3q+1ztp7P8/eZ3/PyVmfPOc55+ybqkKS1J6jFroASdLBMcAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgGteJfmtJO9a6DqkxwIDXLOW5EVJJpLcn2RPkg8n+XcLXdfhKsm1SV6+0HXoyGOAa1aSvAb4I+B3geXAjwB/BqxZwLLmLMniha5Bmi0DXENLcgLwZuBVVfWBqnqgqr5XVf+nql63n2Pel+TOJPcm+XiSZ/T1nZ/kxiTfSvK1JP+ja1+a5Ook9yT5RpJ/TDLwtZqkkvxakluS3JXkD/r3TfLLSXYl+WaSjyQ5fdqxr0qyG9jdta1JsjPJfUm+nOTcqcee5IruHcfXkvxOkkVd38uSfCLJ/+rO85Uk53V9bwGeB/xJ947lT7r2y5Pc3p3nhiTP66vr2CSbu/valeQ3ktzR139ykvcnmezO9Wt9fWd1747uS7I3yVuH/gdWe6rKm7ehbsC5wEPA4gPs81vAu/q2fxl4IvBD9EbuO/v69gDP69aXAM/q1n8P+Avg6O72PCD7OV8BHwVOovdu4EvAy7u+tcDNwNOAxcBvAp+cduz27thjgbOAe4Fz6A1uTgGe2u37QeDtwHHAMuA64BVd38uA7wG/AiwCXgl8fapm4NqpmvrO/RLgSV1drwXuBB7X9W0EPtY9J6cCnwPu6PqOAm4A3gQcAzwZuAX4T13/p4CXdutPAJ6z0K8bb6O7LXgB3tq5AS8G7pxhn0cE+LS+E7vQPKHbvg14BXD8tP3eDGwFfnyImgo4t2/7vwI7uvUPA5f29R0FPAic3nfs8/v63w68bcA5lgPfAY7ta7sY+Gi3/jLg5r6+x3f3/cPd9qMCfMA5vgk8s1v/10Dutl/eF+DPBm6bduwbgHd26x8HfhtYutCvF2+jvzmFotm4G1g67HxxkkVJNnZTEfcBt3ZdS7vlLwDnA19N8rEkz+3a/4DeyPmabmpkwwynur1v/avAyd366cDl3VTMPcA3gNAbWQ869jTgywPu/3R67wT29N3X2+mNxKfcObVSVQ92q0/YX8FJXttNj9zb3d8J/OB5OXlaXf3rpwMnT9XRHftGev/JAFwKPAW4Kcn1SS7YXw1qnwGu2fgU8G16UxPDeBG9Dzd/jl5ArezaA1BV11fVGnpB+EFgS9f+rap6bVU9GfjPwGuSrD7AeU7rW/8RetMX0Au+V1TViX23Y6vqk33791+O83bgxwbc/+30RuBL++7n+Kp6xoB9B3nEJT+7+e7XA78ILKmqE+lN3aTbZQ+9qZNBj+924CvTHtMTq+p8gKraXVUX03tOfx+4MslxQ9apxhjgGlpV3Utv7vVPk6xN8vgkRyc5L8n/HHDIE+kF3930phV+d6ojyTFJXpzkhKr6HnAf8HDXd0GSH0+SvvaHD1Da65IsSXIa8Grgb7v2vwDeMPXBafdB5IUHuJ8rgEuSrE5yVJJTkjy1qvYA1wB/mOT4ru/Hkvz7GZ6yKXvpzVX3Py8PAZPA4iRvAo7v69/S1b0kySnAZX191wH3JXl992HnoiQ/keSnu8f4kiRjVfV94J7umAM9d2qYAa5Zqaq3Aq+h94HgJL0R4WX0RtDT/TW9KY2vATcC/zSt/6XArd30yq/S+2APYBXwf4H76Y36/6yqrj1AWVvpfbC3E/gQvSCmqq6iNwp9b3eOzwPnHeCxXQdcAryN3oj4Y/SmLAB+id6HhjfSm6++ElhxgJr6XQ68sPtWyR8DH6E3P/8les/Pt3nkNMmbgTuAr9B7Hq6k9x8hVfUwvXclZ3b9dwHvoPcOB3ofNH8hyf3deS+qqm8PWacaM/UpudSkJAWsqqqbF7qWUUnySnpBPOyIX48RjsClw0ySFUnO7qZqzqD3NcOrFrouHX789Zl0+DmG3rdcfpTePPZ76f3aVXoEp1AkqVFOoUhSow7pFMrSpUtr5cqVh/KUktS8G2644a6qGpvefkgDfOXKlUxMTBzKU0pS85J8dVC7UyiS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktSoZq5GuHLDhxa6BB2mbt34goUuQVoQjsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1KgZAzzJGUl29t3uS/LrSU5Ksj3J7m655FAULEnqmTHAq+qLVXVmVZ0J/BvgQeAqYAOwo6pWATu6bUnSITLbKZTVwJer6qvAGmBz174ZWDuPdUmSZjDbAL8I+JtufXlV7QHolssGHZBkfZKJJBOTk5MHX6kk6RGGDvAkxwA/D7xvNieoqk1VNV5V42NjY7OtT5K0H7MZgZ8HfKaq9nbbe5OsAOiW++a7OEnS/s0mwC/mB9MnANuAdd36OmDrfBUlSZrZUAGe5PHAOcAH+po3Auck2d31bZz/8iRJ+zPUHzWuqgeBJ01ru5vet1IkSQvAX2JKUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjRr2jxqfmOTKJDcl2ZXkuUlOSrI9ye5uuWTUxUqSfmDYEfjlwN9X1VOBZwK7gA3AjqpaBezotiVJh8iMAZ7keOBngCsAquq7VXUPsAbY3O22GVg7mhIlSYMMMwJ/MjAJvDPJZ5O8I8lxwPKq2gPQLZcNOjjJ+iQTSSYmJyfnrXBJeqwbJsAXA88C/ryqfgp4gFlMl1TVpqoar6rxsbGxgyxTkjTdMAF+B3BHVX26276SXqDvTbICoFvuG02JkqRBZgzwqroTuD3JGV3TauBGYBuwrmtbB2wdSYWSpIEWD7nffwPeneQY4BbgEnrhvyXJpcBtwIWjKVGSNMhQAV5VO4HxAV2r57UaSdLQ/CWmJDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGDfUn1ZLcCnwLeBh4qKrGk5wE/C2wErgV+MWq+uZoypQkTTebEfjPVtWZVTX1tzE3ADuqahWwo9uWJB0ic5lCWQNs7tY3A2vnXI0kaWjDBngB1yS5Icn6rm15Ve0B6JbLBh2YZH2SiSQTk5OTc69YkgQMOQcOnF1VX0+yDNie5KZhT1BVm4BNAOPj43UQNUqSBhhqBF5VX++W+4CrgLOAvUlWAHTLfaMqUpL0aDMGeJLjkjxxah34j8DngW3Aum63dcDWURUpSXq0YaZQlgNXJZna/z1V9fdJrge2JLkUuA24cHRlSpKmmzHAq+oW4JkD2u8GVo+iKEnSzPwlpiQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRg0d4EkWJflskqu77ZOSbE+yu1suGV2ZkqTpZjMCfzWwq297A7CjqlYBO7ptSdIhMlSAJzkVeAHwjr7mNcDmbn0zsHZeK5MkHdCwI/A/An4D+H5f2/Kq2gPQLZcNOjDJ+iQTSSYmJyfnUqskqc+MAZ7kAmBfVd1wMCeoqk1VNV5V42NjYwdzF5KkARYPsc/ZwM8nOR94HHB8kncBe5OsqKo9SVYA+0ZZqCTpkWYcgVfVG6rq1KpaCVwE/ENVvQTYBqzrdlsHbB1ZlZKkR5nL98A3Auck2Q2c021Lkg6RYaZQ/lVVXQtc263fDaye/5IkScPwl5iS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckho1Y4AneVyS65L8c5IvJPntrv2kJNuT7O6WS0ZfriRpyjAj8O8Az6+qZwJnAucmeQ6wAdhRVauAHd22JOkQmTHAq+f+bvPo7lbAGmBz174ZWDuKAiVJgw01B55kUZKdwD5ge1V9GlheVXsAuuWy/Ry7PslEkonJycl5KluSNFSAV9XDVXUmcCpwVpKfGPYEVbWpqsaranxsbOwgy5QkTTerb6FU1T3AtcC5wN4kKwC65b75Lk6StH/DfAtlLMmJ3fqxwM8BNwHbgHXdbuuArSOqUZI0wOIh9lkBbE6yiF7gb6mqq5N8CtiS5FLgNuDCEdYpSZpmxgCvqs8BPzWg/W5g9SiKkiTNzF9iSlKjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjRrmYlaShrByw4cWugQdxm7d+IJ5v09H4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRw/xR49OSfDTJriRfSPLqrv2kJNuT7O6WS0ZfriRpyjAj8IeA11bV04DnAK9K8nRgA7CjqlYBO7ptSdIhMmOAV9WeqvpMt/4tYBdwCrAG2NztthlYO6IaJUkDzGoOPMlKen+h/tPA8qraA72QB5bt55j1SSaSTExOTs6xXEnSlKEDPMkTgPcDv15V9w17XFVtqqrxqhofGxs7mBolSQMMFeBJjqYX3u+uqg90zXuTrOj6VwD7RlOiJGmQYb6FEuAKYFdVvbWvaxuwrltfB2yd//IkSfszzNUIzwZeCvxLkp1d2xuBjcCWJJcCtwEXjqRCSdJAMwZ4VX0CyH66V89vOZKkYflLTElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjRrmjxr/ZZJ9ST7f13ZSku1JdnfLJaMtU5I03TAj8L8Czp3WtgHYUVWrgB3dtiTpEJoxwKvq48A3pjWvATZ365uBtfNbliRpJgc7B768qvYAdMtl81eSJGkYI/8QM8n6JBNJJiYnJ0d9Okl6zDjYAN+bZAVAt9y3vx2ralNVjVfV+NjY2EGeTpI03cEG+DZgXbe+Dtg6P+VIkoY1zNcI/wb4FHBGkjuSXApsBM5Jshs4p9uWJB1Ci2faoaou3k/X6nmuRZI0C/4SU5IaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWrUnAI8yblJvpjk5iQb5qsoSdLMDjrAkywC/hQ4D3g6cHGSp89XYZKkA5vLCPws4OaquqWqvgu8F1gzP2VJkmayeA7HngLc3rd9B/Ds6TslWQ+s7zbvT/LFOZxTP7AUuGuhizgc5PcXugLth6/RPnN8nZ4+qHEuAZ4BbfWohqpNwKY5nEcDJJmoqvGFrkPaH1+jozeXKZQ7gNP6tk8Fvj63ciRJw5pLgF8PrEryo0mOAS4Cts1PWZKkmRz0FEpVPZTkMuAjwCLgL6vqC/NWmWbitJQOd75GRyxVj5q2liQ1wF9iSlKjDHBJapQB3oAkf5XkK0l2drczu/Yk+ePuUgafS/Ksrn1lks8vaNF6TEvyH5Lc2/eafVNf38BLcCS5NolfO5yFuXwPXCPUfbPn6Kp6oGt6XVVdOW2384BV3e3ZwJ8z4MdU0mwlWVJV35zj3fxjVV0w7X6nLsFxDr2vIl+fZFtV3TjHcz0mOQI/zCR5WpI/BL4IPGWG3dcAf109/wScmGTFtPt7cpLPJvnpEZWsI9NEkvckeX6SQT/aO1gzXoIjyVFJNif5nXk87xHJAD8MJDkuySVJPgG8A9gF/GRVfbZvt7d00yRvS/JDXdugyxmc0ne/ZwDvBy6pqutH+yh0hHkK8B7gMuDGJG9McvJUZ/c63Dng1n9V0ucm+eckH07yjK7tgK9ZerMC7wa+VFW/OZJHdgRxCuXwsAf4HPDyqrppQP8bgDuBY+h9t/b1wJs58OUMxoCtwC/4/XzNVlU9DFwNXJ1kDPg94LYk/7aqrquq/z7DXXwGOL2q7k9yPvBBelN9M12C4+3Alqp6y5wfxGOAI/DDwwuBrwFXJXlTkkdcuKaq9nTTJN8B3knvbSgc+HIG99Ib6Zw90sp1xEpyQncxum30RuSX0htozDgCr6r7qur+bv3vgKOTLGXmS3B8EvjZJI8b+QM8AjgCPwxU1TXANUmeBLwE2JrkLnoj8luTrKiqPd1c5Fpg6hsm24DLkryX3oeX93b7rQS+2+37kST3V9V7Du2jUsuSvAt4LvA+4Jeqand//0wj8CQ/DOytqkpyFr3B4t3APXSX4KA3aLkIeFHfoVcAPwO8L8l/qaqH5ukhHZEM8MNIVd0NXA5c3r3oH+663t29jQ2wE/jVrv3vgPOBm4EHgUum3d8DSS4Atid5oKq2jv5R6AixBXjZHAL0hcArkzwE/D/gour97HvGS3BU1VuTnAD87yQvrqrvH/zDOLL5U3pJapRz4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNer/A18yKovOeStuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pos_perc = 100 * y_train.sum().item() / len(y_train)\n",
    "y_neg_perc = 100 - y_pos_perc\n",
    "\n",
    "plt.title(\"Class percentages\")\n",
    "plt.bar([\"<50k\", \">=50k\"], [y_neg_perc, y_pos_perc])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W związku z powyższym będziemy używać odpowiednich metryk, czyli AUROC, precyzji i czułości."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLexWff-LAs0"
   },
   "source": [
    "#### Zadanie 3 (1 punkt)\n",
    "\n",
    "Zaimplementuj regresję logistyczną dla tego zbioru danych, używając PyTorcha. Dane wejściowe zostały dla ciebie przygotowane w komórkach poniżej.\n",
    "\n",
    "Sama sieć składa się z 2 elementów:\n",
    "- warstwa liniowa `nn.Linear`, przekształcająca wektor wejściowy na 1 wyjście - logit\n",
    "- aktywacja sigmoidalna `nn.Sigmoid`, przekształcająca logit na prawdopodobieństwo klasy pozytywnej\n",
    "\n",
    "Użyj binarnej entropii krzyżowej `nn.BCELoss` jako funkcji kosztu. Użyj optymalizatora SGD ze stałą uczącą `1e-3`. Trenuj przez 3000 epok. Pamiętaj, aby przekazać do optymalizatora `torch.optim.SGD` parametry sieci (metoda `.parameters()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbABKz5-LAs2",
    "outputId": "086dc0f3-0184-4072-9fd3-275b60dee2e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.668212890625\n",
      "Epoch 1 loss: 0.6678934693336487\n",
      "Epoch 2 loss: 0.6675746440887451\n",
      "Epoch 3 loss: 0.6672563552856445\n",
      "Epoch 4 loss: 0.6669387221336365\n",
      "Epoch 5 loss: 0.666621744632721\n",
      "Epoch 6 loss: 0.6663052439689636\n",
      "Epoch 7 loss: 0.665989339351654\n",
      "Epoch 8 loss: 0.665674090385437\n",
      "Epoch 9 loss: 0.6653594374656677\n",
      "Epoch 10 loss: 0.6650452613830566\n",
      "Epoch 11 loss: 0.6647317409515381\n",
      "Epoch 12 loss: 0.6644188761711121\n",
      "Epoch 13 loss: 0.6641064286231995\n",
      "Epoch 14 loss: 0.6637946367263794\n",
      "Epoch 15 loss: 0.6634835004806519\n",
      "Epoch 16 loss: 0.6631729006767273\n",
      "Epoch 17 loss: 0.6628629565238953\n",
      "Epoch 18 loss: 0.6625534296035767\n",
      "Epoch 19 loss: 0.6622444987297058\n",
      "Epoch 20 loss: 0.6619362235069275\n",
      "Epoch 21 loss: 0.6616285443305969\n",
      "Epoch 22 loss: 0.6613213419914246\n",
      "Epoch 23 loss: 0.6610147356987\n",
      "Epoch 24 loss: 0.6607086062431335\n",
      "Epoch 25 loss: 0.6604032516479492\n",
      "Epoch 26 loss: 0.6600983738899231\n",
      "Epoch 27 loss: 0.6597940325737\n",
      "Epoch 28 loss: 0.6594902276992798\n",
      "Epoch 29 loss: 0.6591870188713074\n",
      "Epoch 30 loss: 0.6588844060897827\n",
      "Epoch 31 loss: 0.6585822701454163\n",
      "Epoch 32 loss: 0.6582807898521423\n",
      "Epoch 33 loss: 0.6579798460006714\n",
      "Epoch 34 loss: 0.6576794385910034\n",
      "Epoch 35 loss: 0.6573796272277832\n",
      "Epoch 36 loss: 0.6570802330970764\n",
      "Epoch 37 loss: 0.6567815542221069\n",
      "Epoch 38 loss: 0.6564833521842957\n",
      "Epoch 39 loss: 0.6561856865882874\n",
      "Epoch 40 loss: 0.655888557434082\n",
      "Epoch 41 loss: 0.6555920243263245\n",
      "Epoch 42 loss: 0.6552959680557251\n",
      "Epoch 43 loss: 0.6550004482269287\n",
      "Epoch 44 loss: 0.6547056436538696\n",
      "Epoch 45 loss: 0.654411256313324\n",
      "Epoch 46 loss: 0.6541174650192261\n",
      "Epoch 47 loss: 0.6538241505622864\n",
      "Epoch 48 loss: 0.6535313129425049\n",
      "Epoch 49 loss: 0.6532391905784607\n",
      "Epoch 50 loss: 0.6529474258422852\n",
      "Epoch 51 loss: 0.6526562571525574\n",
      "Epoch 52 loss: 0.6523657441139221\n",
      "Epoch 53 loss: 0.6520755887031555\n",
      "Epoch 54 loss: 0.6517861485481262\n",
      "Epoch 55 loss: 0.6514971852302551\n",
      "Epoch 56 loss: 0.6512086391448975\n",
      "Epoch 57 loss: 0.6509206891059875\n",
      "Epoch 58 loss: 0.6506332159042358\n",
      "Epoch 59 loss: 0.6503463983535767\n",
      "Epoch 60 loss: 0.6500599384307861\n",
      "Epoch 61 loss: 0.6497741341590881\n",
      "Epoch 62 loss: 0.6494887471199036\n",
      "Epoch 63 loss: 0.6492039561271667\n",
      "Epoch 64 loss: 0.6489197015762329\n",
      "Epoch 65 loss: 0.6486358642578125\n",
      "Epoch 66 loss: 0.6483526825904846\n",
      "Epoch 67 loss: 0.6480699181556702\n",
      "Epoch 68 loss: 0.6477877497673035\n",
      "Epoch 69 loss: 0.6475059986114502\n",
      "Epoch 70 loss: 0.6472249031066895\n",
      "Epoch 71 loss: 0.6469442248344421\n",
      "Epoch 72 loss: 0.646664023399353\n",
      "Epoch 73 loss: 0.6463843584060669\n",
      "Epoch 74 loss: 0.6461052298545837\n",
      "Epoch 75 loss: 0.6458266377449036\n",
      "Epoch 76 loss: 0.6455484628677368\n",
      "Epoch 77 loss: 0.6452708840370178\n",
      "Epoch 78 loss: 0.644993782043457\n",
      "Epoch 79 loss: 0.6447170972824097\n",
      "Epoch 80 loss: 0.6444409489631653\n",
      "Epoch 81 loss: 0.6441653370857239\n",
      "Epoch 82 loss: 0.6438903212547302\n",
      "Epoch 83 loss: 0.6436156630516052\n",
      "Epoch 84 loss: 0.6433415412902832\n",
      "Epoch 85 loss: 0.6430679559707642\n",
      "Epoch 86 loss: 0.6427947878837585\n",
      "Epoch 87 loss: 0.6425222158432007\n",
      "Epoch 88 loss: 0.6422500014305115\n",
      "Epoch 89 loss: 0.64197838306427\n",
      "Epoch 90 loss: 0.641707181930542\n",
      "Epoch 91 loss: 0.6414365768432617\n",
      "Epoch 92 loss: 0.6411663889884949\n",
      "Epoch 93 loss: 0.640896737575531\n",
      "Epoch 94 loss: 0.6406275033950806\n",
      "Epoch 95 loss: 0.6403588056564331\n",
      "Epoch 96 loss: 0.6400905847549438\n",
      "Epoch 97 loss: 0.6398228406906128\n",
      "Epoch 98 loss: 0.6395555734634399\n",
      "Epoch 99 loss: 0.6392887830734253\n",
      "Epoch 100 loss: 0.6390225291252136\n",
      "Epoch 101 loss: 0.6387566328048706\n",
      "Epoch 102 loss: 0.6384913325309753\n",
      "Epoch 103 loss: 0.6382265090942383\n",
      "Epoch 104 loss: 0.6379620432853699\n",
      "Epoch 105 loss: 0.637698233127594\n",
      "Epoch 106 loss: 0.6374347805976868\n",
      "Epoch 107 loss: 0.6371718049049377\n",
      "Epoch 108 loss: 0.6369093060493469\n",
      "Epoch 109 loss: 0.6366472840309143\n",
      "Epoch 110 loss: 0.6363857388496399\n",
      "Epoch 111 loss: 0.6361246705055237\n",
      "Epoch 112 loss: 0.6358640789985657\n",
      "Epoch 113 loss: 0.6356038451194763\n",
      "Epoch 114 loss: 0.6353442072868347\n",
      "Epoch 115 loss: 0.6350849866867065\n",
      "Epoch 116 loss: 0.6348261833190918\n",
      "Epoch 117 loss: 0.6345679759979248\n",
      "Epoch 118 loss: 0.6343101859092712\n",
      "Epoch 119 loss: 0.6340527534484863\n",
      "Epoch 120 loss: 0.6337958574295044\n",
      "Epoch 121 loss: 0.6335393786430359\n",
      "Epoch 122 loss: 0.6332833766937256\n",
      "Epoch 123 loss: 0.6330279111862183\n",
      "Epoch 124 loss: 0.6327728033065796\n",
      "Epoch 125 loss: 0.6325181126594543\n",
      "Epoch 126 loss: 0.6322640180587769\n",
      "Epoch 127 loss: 0.632010281085968\n",
      "Epoch 128 loss: 0.6317570209503174\n",
      "Epoch 129 loss: 0.6315041780471802\n",
      "Epoch 130 loss: 0.631251871585846\n",
      "Epoch 131 loss: 0.6309998631477356\n",
      "Epoch 132 loss: 0.630748450756073\n",
      "Epoch 133 loss: 0.630497395992279\n",
      "Epoch 134 loss: 0.6302468776702881\n",
      "Epoch 135 loss: 0.6299967169761658\n",
      "Epoch 136 loss: 0.6297470331192017\n",
      "Epoch 137 loss: 0.6294978260993958\n",
      "Epoch 138 loss: 0.6292489767074585\n",
      "Epoch 139 loss: 0.6290006041526794\n",
      "Epoch 140 loss: 0.6287526488304138\n",
      "Epoch 141 loss: 0.6285051703453064\n",
      "Epoch 142 loss: 0.6282581090927124\n",
      "Epoch 143 loss: 0.6280115246772766\n",
      "Epoch 144 loss: 0.6277653574943542\n",
      "Epoch 145 loss: 0.6275196075439453\n",
      "Epoch 146 loss: 0.6272743344306946\n",
      "Epoch 147 loss: 0.6270294189453125\n",
      "Epoch 148 loss: 0.6267849802970886\n",
      "Epoch 149 loss: 0.6265409588813782\n",
      "Epoch 150 loss: 0.6262972950935364\n",
      "Epoch 151 loss: 0.6260542273521423\n",
      "Epoch 152 loss: 0.6258115172386169\n",
      "Epoch 153 loss: 0.6255691647529602\n",
      "Epoch 154 loss: 0.6253273487091064\n",
      "Epoch 155 loss: 0.6250858902931213\n",
      "Epoch 156 loss: 0.6248449087142944\n",
      "Epoch 157 loss: 0.6246042847633362\n",
      "Epoch 158 loss: 0.6243640780448914\n",
      "Epoch 159 loss: 0.62412428855896\n",
      "Epoch 160 loss: 0.6238850355148315\n",
      "Epoch 161 loss: 0.623646080493927\n",
      "Epoch 162 loss: 0.6234075427055359\n",
      "Epoch 163 loss: 0.6231695413589478\n",
      "Epoch 164 loss: 0.6229318380355835\n",
      "Epoch 165 loss: 0.6226946115493774\n",
      "Epoch 166 loss: 0.6224578022956848\n",
      "Epoch 167 loss: 0.6222213506698608\n",
      "Epoch 168 loss: 0.6219853758811951\n",
      "Epoch 169 loss: 0.6217496991157532\n",
      "Epoch 170 loss: 0.621514618396759\n",
      "Epoch 171 loss: 0.6212798357009888\n",
      "Epoch 172 loss: 0.6210454702377319\n",
      "Epoch 173 loss: 0.6208115220069885\n",
      "Epoch 174 loss: 0.6205779910087585\n",
      "Epoch 175 loss: 0.6203448176383972\n",
      "Epoch 176 loss: 0.6201121211051941\n",
      "Epoch 177 loss: 0.6198797821998596\n",
      "Epoch 178 loss: 0.6196478605270386\n",
      "Epoch 179 loss: 0.619416356086731\n",
      "Epoch 180 loss: 0.6191852688789368\n",
      "Epoch 181 loss: 0.6189545392990112\n",
      "Epoch 182 loss: 0.6187242269515991\n",
      "Epoch 183 loss: 0.6184943318367004\n",
      "Epoch 184 loss: 0.6182647943496704\n",
      "Epoch 185 loss: 0.6180357336997986\n",
      "Epoch 186 loss: 0.6178069114685059\n",
      "Epoch 187 loss: 0.6175786852836609\n",
      "Epoch 188 loss: 0.617350697517395\n",
      "Epoch 189 loss: 0.6171231865882874\n",
      "Epoch 190 loss: 0.6168960332870483\n",
      "Epoch 191 loss: 0.6166692972183228\n",
      "Epoch 192 loss: 0.6164429187774658\n",
      "Epoch 193 loss: 0.6162170171737671\n",
      "Epoch 194 loss: 0.6159914135932922\n",
      "Epoch 195 loss: 0.6157662272453308\n",
      "Epoch 196 loss: 0.6155414581298828\n",
      "Epoch 197 loss: 0.6153170466423035\n",
      "Epoch 198 loss: 0.6150929927825928\n",
      "Epoch 199 loss: 0.6148693561553955\n",
      "Epoch 200 loss: 0.6146460771560669\n",
      "Epoch 201 loss: 0.6144232749938965\n",
      "Epoch 202 loss: 0.61420077085495\n",
      "Epoch 203 loss: 0.6139786839485168\n",
      "Epoch 204 loss: 0.6137569546699524\n",
      "Epoch 205 loss: 0.6135356426239014\n",
      "Epoch 206 loss: 0.613314688205719\n",
      "Epoch 207 loss: 0.6130940914154053\n",
      "Epoch 208 loss: 0.6128738522529602\n",
      "Epoch 209 loss: 0.6126540899276733\n",
      "Epoch 210 loss: 0.6124345660209656\n",
      "Epoch 211 loss: 0.6122155785560608\n",
      "Epoch 212 loss: 0.6119968295097351\n",
      "Epoch 213 loss: 0.6117785573005676\n",
      "Epoch 214 loss: 0.611560583114624\n",
      "Epoch 215 loss: 0.6113430261611938\n",
      "Epoch 216 loss: 0.6111258268356323\n",
      "Epoch 217 loss: 0.6109089851379395\n",
      "Epoch 218 loss: 0.6106925010681152\n",
      "Epoch 219 loss: 0.6104764938354492\n",
      "Epoch 220 loss: 0.6102607250213623\n",
      "Epoch 221 loss: 0.6100453734397888\n",
      "Epoch 222 loss: 0.609830379486084\n",
      "Epoch 223 loss: 0.6096157431602478\n",
      "Epoch 224 loss: 0.6094014644622803\n",
      "Epoch 225 loss: 0.6091875433921814\n",
      "Epoch 226 loss: 0.608974039554596\n",
      "Epoch 227 loss: 0.6087608933448792\n",
      "Epoch 228 loss: 0.6085480451583862\n",
      "Epoch 229 loss: 0.6083356142044067\n",
      "Epoch 230 loss: 0.6081235408782959\n",
      "Epoch 231 loss: 0.6079118251800537\n",
      "Epoch 232 loss: 0.6077004075050354\n",
      "Epoch 233 loss: 0.6074894070625305\n",
      "Epoch 234 loss: 0.6072788238525391\n",
      "Epoch 235 loss: 0.6070684790611267\n",
      "Epoch 236 loss: 0.6068585515022278\n",
      "Epoch 237 loss: 0.6066489815711975\n",
      "Epoch 238 loss: 0.6064397096633911\n",
      "Epoch 239 loss: 0.6062308549880981\n",
      "Epoch 240 loss: 0.606022298336029\n",
      "Epoch 241 loss: 0.6058140993118286\n",
      "Epoch 242 loss: 0.6056063175201416\n",
      "Epoch 243 loss: 0.6053988337516785\n",
      "Epoch 244 loss: 0.6051917672157288\n",
      "Epoch 245 loss: 0.6049849987030029\n",
      "Epoch 246 loss: 0.6047785878181458\n",
      "Epoch 247 loss: 0.6045724749565125\n",
      "Epoch 248 loss: 0.6043667197227478\n",
      "Epoch 249 loss: 0.6041613817214966\n",
      "Epoch 250 loss: 0.6039562821388245\n",
      "Epoch 251 loss: 0.603751540184021\n",
      "Epoch 252 loss: 0.6035472750663757\n",
      "Epoch 253 loss: 0.6033432483673096\n",
      "Epoch 254 loss: 0.6031395196914673\n",
      "Epoch 255 loss: 0.6029362082481384\n",
      "Epoch 256 loss: 0.6027331948280334\n",
      "Epoch 257 loss: 0.6025305986404419\n",
      "Epoch 258 loss: 0.6023283004760742\n",
      "Epoch 259 loss: 0.6021263003349304\n",
      "Epoch 260 loss: 0.6019246578216553\n",
      "Epoch 261 loss: 0.601723313331604\n",
      "Epoch 262 loss: 0.6015223860740662\n",
      "Epoch 263 loss: 0.6013217568397522\n",
      "Epoch 264 loss: 0.6011214852333069\n",
      "Epoch 265 loss: 0.6009215116500854\n",
      "Epoch 266 loss: 0.6007218956947327\n",
      "Epoch 267 loss: 0.6005225777626038\n",
      "Epoch 268 loss: 0.6003236174583435\n",
      "Epoch 269 loss: 0.6001250147819519\n",
      "Epoch 270 loss: 0.5999267101287842\n",
      "Epoch 271 loss: 0.5997287631034851\n",
      "Epoch 272 loss: 0.5995311141014099\n",
      "Epoch 273 loss: 0.5993337035179138\n",
      "Epoch 274 loss: 0.5991367101669312\n",
      "Epoch 275 loss: 0.5989401340484619\n",
      "Epoch 276 loss: 0.598743736743927\n",
      "Epoch 277 loss: 0.5985477566719055\n",
      "Epoch 278 loss: 0.5983520746231079\n",
      "Epoch 279 loss: 0.598156750202179\n",
      "Epoch 280 loss: 0.5979616641998291\n",
      "Epoch 281 loss: 0.5977669954299927\n",
      "Epoch 282 loss: 0.5975726246833801\n",
      "Epoch 283 loss: 0.5973784327507019\n",
      "Epoch 284 loss: 0.5971847176551819\n",
      "Epoch 285 loss: 0.5969913005828857\n",
      "Epoch 286 loss: 0.5967981815338135\n",
      "Epoch 287 loss: 0.5966053605079651\n",
      "Epoch 288 loss: 0.5964128971099854\n",
      "Epoch 289 loss: 0.5962206721305847\n",
      "Epoch 290 loss: 0.5960288643836975\n",
      "Epoch 291 loss: 0.5958372950553894\n",
      "Epoch 292 loss: 0.5956461429595947\n",
      "Epoch 293 loss: 0.5954551696777344\n",
      "Epoch 294 loss: 0.5952646136283875\n",
      "Epoch 295 loss: 0.5950742959976196\n",
      "Epoch 296 loss: 0.5948843359947205\n",
      "Epoch 297 loss: 0.5946946740150452\n",
      "Epoch 298 loss: 0.5945053100585938\n",
      "Epoch 299 loss: 0.5943163633346558\n",
      "Epoch 300 loss: 0.5941275954246521\n",
      "Epoch 301 loss: 0.5939391851425171\n",
      "Epoch 302 loss: 0.593751072883606\n",
      "Epoch 303 loss: 0.5935631990432739\n",
      "Epoch 304 loss: 0.5933756828308105\n",
      "Epoch 305 loss: 0.5931885242462158\n",
      "Epoch 306 loss: 0.593001663684845\n",
      "Epoch 307 loss: 0.592815101146698\n",
      "Epoch 308 loss: 0.5926288366317749\n",
      "Epoch 309 loss: 0.5924428105354309\n",
      "Epoch 310 loss: 0.5922571420669556\n",
      "Epoch 311 loss: 0.5920717716217041\n",
      "Epoch 312 loss: 0.5918866395950317\n",
      "Epoch 313 loss: 0.5917019248008728\n",
      "Epoch 314 loss: 0.591517448425293\n",
      "Epoch 315 loss: 0.5913333296775818\n",
      "Epoch 316 loss: 0.5911493897438049\n",
      "Epoch 317 loss: 0.5909658074378967\n",
      "Epoch 318 loss: 0.5907825827598572\n",
      "Epoch 319 loss: 0.590599536895752\n",
      "Epoch 320 loss: 0.5904169082641602\n",
      "Epoch 321 loss: 0.5902345180511475\n",
      "Epoch 322 loss: 0.5900523662567139\n",
      "Epoch 323 loss: 0.5898706316947937\n",
      "Epoch 324 loss: 0.5896890759468079\n",
      "Epoch 325 loss: 0.5895078778266907\n",
      "Epoch 326 loss: 0.5893269777297974\n",
      "Epoch 327 loss: 0.5891463160514832\n",
      "Epoch 328 loss: 0.5889660120010376\n",
      "Epoch 329 loss: 0.5887858867645264\n",
      "Epoch 330 loss: 0.5886061787605286\n",
      "Epoch 331 loss: 0.5884267091751099\n",
      "Epoch 332 loss: 0.5882474780082703\n",
      "Epoch 333 loss: 0.5880686640739441\n",
      "Epoch 334 loss: 0.5878900289535522\n",
      "Epoch 335 loss: 0.5877116918563843\n",
      "Epoch 336 loss: 0.5875335931777954\n",
      "Epoch 337 loss: 0.5873558521270752\n",
      "Epoch 338 loss: 0.5871784687042236\n",
      "Epoch 339 loss: 0.5870012640953064\n",
      "Epoch 340 loss: 0.586824357509613\n",
      "Epoch 341 loss: 0.5866477489471436\n",
      "Epoch 342 loss: 0.5864713788032532\n",
      "Epoch 343 loss: 0.5862953066825867\n",
      "Epoch 344 loss: 0.586119532585144\n",
      "Epoch 345 loss: 0.5859441161155701\n",
      "Epoch 346 loss: 0.5857688188552856\n",
      "Epoch 347 loss: 0.5855939388275146\n",
      "Epoch 348 loss: 0.5854192972183228\n",
      "Epoch 349 loss: 0.58524489402771\n",
      "Epoch 350 loss: 0.5850708484649658\n",
      "Epoch 351 loss: 0.584896981716156\n",
      "Epoch 352 loss: 0.5847234129905701\n",
      "Epoch 353 loss: 0.584550142288208\n",
      "Epoch 354 loss: 0.5843771696090698\n",
      "Epoch 355 loss: 0.5842044949531555\n",
      "Epoch 356 loss: 0.5840319991111755\n",
      "Epoch 357 loss: 0.5838598012924194\n",
      "Epoch 358 loss: 0.583687961101532\n",
      "Epoch 359 loss: 0.5835162997245789\n",
      "Epoch 360 loss: 0.5833449363708496\n",
      "Epoch 361 loss: 0.5831738710403442\n",
      "Epoch 362 loss: 0.583003044128418\n",
      "Epoch 363 loss: 0.5828325152397156\n",
      "Epoch 364 loss: 0.5826622843742371\n",
      "Epoch 365 loss: 0.5824921727180481\n",
      "Epoch 366 loss: 0.5823224782943726\n",
      "Epoch 367 loss: 0.5821530222892761\n",
      "Epoch 368 loss: 0.5819838047027588\n",
      "Epoch 369 loss: 0.5818148851394653\n",
      "Epoch 370 loss: 0.5816461443901062\n",
      "Epoch 371 loss: 0.5814778208732605\n",
      "Epoch 372 loss: 0.5813096165657043\n",
      "Epoch 373 loss: 0.5811417102813721\n",
      "Epoch 374 loss: 0.5809741616249084\n",
      "Epoch 375 loss: 0.5808067321777344\n",
      "Epoch 376 loss: 0.580639660358429\n",
      "Epoch 377 loss: 0.5804728865623474\n",
      "Epoch 378 loss: 0.5803062915802002\n",
      "Epoch 379 loss: 0.5801400542259216\n",
      "Epoch 380 loss: 0.5799739360809326\n",
      "Epoch 381 loss: 0.5798081159591675\n",
      "Epoch 382 loss: 0.579642653465271\n",
      "Epoch 383 loss: 0.5794773101806641\n",
      "Epoch 384 loss: 0.5793123245239258\n",
      "Epoch 385 loss: 0.5791475772857666\n",
      "Epoch 386 loss: 0.5789830684661865\n",
      "Epoch 387 loss: 0.5788188576698303\n",
      "Epoch 388 loss: 0.5786548256874084\n",
      "Epoch 389 loss: 0.5784911513328552\n",
      "Epoch 390 loss: 0.5783275961875916\n",
      "Epoch 391 loss: 0.5781643986701965\n",
      "Epoch 392 loss: 0.5780013799667358\n",
      "Epoch 393 loss: 0.577838659286499\n",
      "Epoch 394 loss: 0.5776762366294861\n",
      "Epoch 395 loss: 0.5775140523910522\n",
      "Epoch 396 loss: 0.5773521065711975\n",
      "Epoch 397 loss: 0.5771903991699219\n",
      "Epoch 398 loss: 0.5770288705825806\n",
      "Epoch 399 loss: 0.5768676400184631\n",
      "Epoch 400 loss: 0.5767067670822144\n",
      "Epoch 401 loss: 0.5765460133552551\n",
      "Epoch 402 loss: 0.5763855576515198\n",
      "Epoch 403 loss: 0.5762253403663635\n",
      "Epoch 404 loss: 0.5760653614997864\n",
      "Epoch 405 loss: 0.5759056210517883\n",
      "Epoch 406 loss: 0.5757461190223694\n",
      "Epoch 407 loss: 0.5755869150161743\n",
      "Epoch 408 loss: 0.5754278898239136\n",
      "Epoch 409 loss: 0.5752691626548767\n",
      "Epoch 410 loss: 0.575110673904419\n",
      "Epoch 411 loss: 0.5749524235725403\n",
      "Epoch 412 loss: 0.5747944712638855\n",
      "Epoch 413 loss: 0.5746365785598755\n",
      "Epoch 414 loss: 0.5744791626930237\n",
      "Epoch 415 loss: 0.5743218064308167\n",
      "Epoch 416 loss: 0.5741647481918335\n",
      "Epoch 417 loss: 0.5740079879760742\n",
      "Epoch 418 loss: 0.5738514065742493\n",
      "Epoch 419 loss: 0.5736950635910034\n",
      "Epoch 420 loss: 0.5735390186309814\n",
      "Epoch 421 loss: 0.5733831524848938\n",
      "Epoch 422 loss: 0.5732275247573853\n",
      "Epoch 423 loss: 0.573072075843811\n",
      "Epoch 424 loss: 0.5729169845581055\n",
      "Epoch 425 loss: 0.5727620720863342\n",
      "Epoch 426 loss: 0.5726074576377869\n",
      "Epoch 427 loss: 0.572452962398529\n",
      "Epoch 428 loss: 0.5722988247871399\n",
      "Epoch 429 loss: 0.5721448063850403\n",
      "Epoch 430 loss: 0.5719911456108093\n",
      "Epoch 431 loss: 0.5718375444412231\n",
      "Epoch 432 loss: 0.5716843605041504\n",
      "Epoch 433 loss: 0.5715312957763672\n",
      "Epoch 434 loss: 0.5713785290718079\n",
      "Epoch 435 loss: 0.5712259411811829\n",
      "Epoch 436 loss: 0.571073591709137\n",
      "Epoch 437 loss: 0.5709214806556702\n",
      "Epoch 438 loss: 0.5707696080207825\n",
      "Epoch 439 loss: 0.5706179738044739\n",
      "Epoch 440 loss: 0.5704665184020996\n",
      "Epoch 441 loss: 0.5703153610229492\n",
      "Epoch 442 loss: 0.5701643824577332\n",
      "Epoch 443 loss: 0.570013701915741\n",
      "Epoch 444 loss: 0.5698632001876831\n",
      "Epoch 445 loss: 0.5697128176689148\n",
      "Epoch 446 loss: 0.5695627927780151\n",
      "Epoch 447 loss: 0.5694130659103394\n",
      "Epoch 448 loss: 0.5692633986473083\n",
      "Epoch 449 loss: 0.5691140294075012\n",
      "Epoch 450 loss: 0.5689648389816284\n",
      "Epoch 451 loss: 0.5688158869743347\n",
      "Epoch 452 loss: 0.5686671733856201\n",
      "Epoch 453 loss: 0.5685187578201294\n",
      "Epoch 454 loss: 0.568370521068573\n",
      "Epoch 455 loss: 0.5682224035263062\n",
      "Epoch 456 loss: 0.5680745840072632\n",
      "Epoch 457 loss: 0.5679269433021545\n",
      "Epoch 458 loss: 0.5677796006202698\n",
      "Epoch 459 loss: 0.5676324963569641\n",
      "Epoch 460 loss: 0.567485511302948\n",
      "Epoch 461 loss: 0.5673388242721558\n",
      "Epoch 462 loss: 0.5671922564506531\n",
      "Epoch 463 loss: 0.5670459866523743\n",
      "Epoch 464 loss: 0.5668999552726746\n",
      "Epoch 465 loss: 0.566754162311554\n",
      "Epoch 466 loss: 0.5666084289550781\n",
      "Epoch 467 loss: 0.5664629936218262\n",
      "Epoch 468 loss: 0.5663178563117981\n",
      "Epoch 469 loss: 0.5661728978157043\n",
      "Epoch 470 loss: 0.5660280585289001\n",
      "Epoch 471 loss: 0.5658835172653198\n",
      "Epoch 472 loss: 0.5657392144203186\n",
      "Epoch 473 loss: 0.5655950903892517\n",
      "Epoch 474 loss: 0.5654511451721191\n",
      "Epoch 475 loss: 0.5653074383735657\n",
      "Epoch 476 loss: 0.5651639103889465\n",
      "Epoch 477 loss: 0.5650206804275513\n",
      "Epoch 478 loss: 0.5648776292800903\n",
      "Epoch 479 loss: 0.5647347569465637\n",
      "Epoch 480 loss: 0.5645921230316162\n",
      "Epoch 481 loss: 0.5644496083259583\n",
      "Epoch 482 loss: 0.564307451248169\n",
      "Epoch 483 loss: 0.5641654133796692\n",
      "Epoch 484 loss: 0.5640235543251038\n",
      "Epoch 485 loss: 0.5638819932937622\n",
      "Epoch 486 loss: 0.563740611076355\n",
      "Epoch 487 loss: 0.5635994672775269\n",
      "Epoch 488 loss: 0.5634584426879883\n",
      "Epoch 489 loss: 0.5633176565170288\n",
      "Epoch 490 loss: 0.5631771087646484\n",
      "Epoch 491 loss: 0.5630367994308472\n",
      "Epoch 492 loss: 0.5628966093063354\n",
      "Epoch 493 loss: 0.5627566576004028\n",
      "Epoch 494 loss: 0.5626169443130493\n",
      "Epoch 495 loss: 0.5624773502349854\n",
      "Epoch 496 loss: 0.5623379945755005\n",
      "Epoch 497 loss: 0.5621988773345947\n",
      "Epoch 498 loss: 0.5620599985122681\n",
      "Epoch 499 loss: 0.561921238899231\n",
      "Epoch 500 loss: 0.561782717704773\n",
      "Epoch 501 loss: 0.561644434928894\n",
      "Epoch 502 loss: 0.5615062713623047\n",
      "Epoch 503 loss: 0.5613683462142944\n",
      "Epoch 504 loss: 0.5612305998802185\n",
      "Epoch 505 loss: 0.5610930919647217\n",
      "Epoch 506 loss: 0.5609557628631592\n",
      "Epoch 507 loss: 0.560818612575531\n",
      "Epoch 508 loss: 0.5606817007064819\n",
      "Epoch 509 loss: 0.560545027256012\n",
      "Epoch 510 loss: 0.5604084134101868\n",
      "Epoch 511 loss: 0.5602721571922302\n",
      "Epoch 512 loss: 0.5601360201835632\n",
      "Epoch 513 loss: 0.559999942779541\n",
      "Epoch 514 loss: 0.5598642826080322\n",
      "Epoch 515 loss: 0.559728741645813\n",
      "Epoch 516 loss: 0.5595933794975281\n",
      "Epoch 517 loss: 0.5594581961631775\n",
      "Epoch 518 loss: 0.5593233108520508\n",
      "Epoch 519 loss: 0.5591884851455688\n",
      "Epoch 520 loss: 0.559053897857666\n",
      "Epoch 521 loss: 0.5589194893836975\n",
      "Epoch 522 loss: 0.5587853193283081\n",
      "Epoch 523 loss: 0.5586513876914978\n",
      "Epoch 524 loss: 0.5585175156593323\n",
      "Epoch 525 loss: 0.5583839416503906\n",
      "Epoch 526 loss: 0.5582504868507385\n",
      "Epoch 527 loss: 0.5581173300743103\n",
      "Epoch 528 loss: 0.5579842329025269\n",
      "Epoch 529 loss: 0.5578513741493225\n",
      "Epoch 530 loss: 0.5577186942100525\n",
      "Epoch 531 loss: 0.5575863122940063\n",
      "Epoch 532 loss: 0.557453989982605\n",
      "Epoch 533 loss: 0.5573219060897827\n",
      "Epoch 534 loss: 0.5571900010108948\n",
      "Epoch 535 loss: 0.5570582747459412\n",
      "Epoch 536 loss: 0.5569266676902771\n",
      "Epoch 537 loss: 0.5567953586578369\n",
      "Epoch 538 loss: 0.556664228439331\n",
      "Epoch 539 loss: 0.5565332174301147\n",
      "Epoch 540 loss: 0.5564024448394775\n",
      "Epoch 541 loss: 0.5562718510627747\n",
      "Epoch 542 loss: 0.5561414361000061\n",
      "Epoch 543 loss: 0.5560112595558167\n",
      "Epoch 544 loss: 0.5558812022209167\n",
      "Epoch 545 loss: 0.5557513236999512\n",
      "Epoch 546 loss: 0.5556216239929199\n",
      "Epoch 547 loss: 0.5554921627044678\n",
      "Epoch 548 loss: 0.5553628206253052\n",
      "Epoch 549 loss: 0.5552336573600769\n",
      "Epoch 550 loss: 0.555104672908783\n",
      "Epoch 551 loss: 0.5549759864807129\n",
      "Epoch 552 loss: 0.5548474192619324\n",
      "Epoch 553 loss: 0.5547189712524414\n",
      "Epoch 554 loss: 0.5545908212661743\n",
      "Epoch 555 loss: 0.554462730884552\n",
      "Epoch 556 loss: 0.5543348789215088\n",
      "Epoch 557 loss: 0.5542072057723999\n",
      "Epoch 558 loss: 0.5540796518325806\n",
      "Epoch 559 loss: 0.5539523959159851\n",
      "Epoch 560 loss: 0.5538251996040344\n",
      "Epoch 561 loss: 0.5536983013153076\n",
      "Epoch 562 loss: 0.5535714626312256\n",
      "Epoch 563 loss: 0.5534448623657227\n",
      "Epoch 564 loss: 0.553318440914154\n",
      "Epoch 565 loss: 0.5531921982765198\n",
      "Epoch 566 loss: 0.5530661344528198\n",
      "Epoch 567 loss: 0.5529401898384094\n",
      "Epoch 568 loss: 0.5528144836425781\n",
      "Epoch 569 loss: 0.5526888966560364\n",
      "Epoch 570 loss: 0.552563488483429\n",
      "Epoch 571 loss: 0.5524383187294006\n",
      "Epoch 572 loss: 0.5523132681846619\n",
      "Epoch 573 loss: 0.5521883964538574\n",
      "Epoch 574 loss: 0.5520637631416321\n",
      "Epoch 575 loss: 0.5519392490386963\n",
      "Epoch 576 loss: 0.55181485414505\n",
      "Epoch 577 loss: 0.5516906976699829\n",
      "Epoch 578 loss: 0.5515667200088501\n",
      "Epoch 579 loss: 0.5514429211616516\n",
      "Epoch 580 loss: 0.5513192415237427\n",
      "Epoch 581 loss: 0.5511958003044128\n",
      "Epoch 582 loss: 0.5510724782943726\n",
      "Epoch 583 loss: 0.5509493350982666\n",
      "Epoch 584 loss: 0.550826370716095\n",
      "Epoch 585 loss: 0.5507035255432129\n",
      "Epoch 586 loss: 0.5505809187889099\n",
      "Epoch 587 loss: 0.5504584908485413\n",
      "Epoch 588 loss: 0.5503361225128174\n",
      "Epoch 589 loss: 0.5502139925956726\n",
      "Epoch 590 loss: 0.5500921010971069\n",
      "Epoch 591 loss: 0.5499703288078308\n",
      "Epoch 592 loss: 0.5498486757278442\n",
      "Epoch 593 loss: 0.5497272610664368\n",
      "Epoch 594 loss: 0.5496059060096741\n",
      "Epoch 595 loss: 0.5494848489761353\n",
      "Epoch 596 loss: 0.5493638515472412\n",
      "Epoch 597 loss: 0.5492430925369263\n",
      "Epoch 598 loss: 0.5491223931312561\n",
      "Epoch 599 loss: 0.549001932144165\n",
      "Epoch 600 loss: 0.5488817095756531\n",
      "Epoch 601 loss: 0.5487615466117859\n",
      "Epoch 602 loss: 0.548641562461853\n",
      "Epoch 603 loss: 0.5485218167304993\n",
      "Epoch 604 loss: 0.5484021306037903\n",
      "Epoch 605 loss: 0.5482826828956604\n",
      "Epoch 606 loss: 0.5481633543968201\n",
      "Epoch 607 loss: 0.5480442643165588\n",
      "Epoch 608 loss: 0.5479251742362976\n",
      "Epoch 609 loss: 0.5478063821792603\n",
      "Epoch 610 loss: 0.5476877093315125\n",
      "Epoch 611 loss: 0.547569215297699\n",
      "Epoch 612 loss: 0.547450840473175\n",
      "Epoch 613 loss: 0.5473327040672302\n",
      "Epoch 614 loss: 0.5472146272659302\n",
      "Epoch 615 loss: 0.5470967888832092\n",
      "Epoch 616 loss: 0.5469790697097778\n",
      "Epoch 617 loss: 0.5468615293502808\n",
      "Epoch 618 loss: 0.546744167804718\n",
      "Epoch 619 loss: 0.5466269254684448\n",
      "Epoch 620 loss: 0.546509861946106\n",
      "Epoch 621 loss: 0.5463929176330566\n",
      "Epoch 622 loss: 0.5462761521339417\n",
      "Epoch 623 loss: 0.546159565448761\n",
      "Epoch 624 loss: 0.5460430979728699\n",
      "Epoch 625 loss: 0.5459268689155579\n",
      "Epoch 626 loss: 0.5458106994628906\n",
      "Epoch 627 loss: 0.5456947088241577\n",
      "Epoch 628 loss: 0.5455789566040039\n",
      "Epoch 629 loss: 0.5454632043838501\n",
      "Epoch 630 loss: 0.5453477501869202\n",
      "Epoch 631 loss: 0.5452324151992798\n",
      "Epoch 632 loss: 0.545117199420929\n",
      "Epoch 633 loss: 0.5450021624565125\n",
      "Epoch 634 loss: 0.5448872447013855\n",
      "Epoch 635 loss: 0.5447725057601929\n",
      "Epoch 636 loss: 0.5446580052375793\n",
      "Epoch 637 loss: 0.544543445110321\n",
      "Epoch 638 loss: 0.5444292426109314\n",
      "Epoch 639 loss: 0.5443150997161865\n",
      "Epoch 640 loss: 0.544201135635376\n",
      "Epoch 641 loss: 0.544087290763855\n",
      "Epoch 642 loss: 0.5439736843109131\n",
      "Epoch 643 loss: 0.543860137462616\n",
      "Epoch 644 loss: 0.5437467694282532\n",
      "Epoch 645 loss: 0.5436334609985352\n",
      "Epoch 646 loss: 0.543520450592041\n",
      "Epoch 647 loss: 0.5434075593948364\n",
      "Epoch 648 loss: 0.5432947874069214\n",
      "Epoch 649 loss: 0.5431821346282959\n",
      "Epoch 650 loss: 0.5430697202682495\n",
      "Epoch 651 loss: 0.5429573059082031\n",
      "Epoch 652 loss: 0.5428452491760254\n",
      "Epoch 653 loss: 0.5427331328392029\n",
      "Epoch 654 loss: 0.5426212549209595\n",
      "Epoch 655 loss: 0.5425095558166504\n",
      "Epoch 656 loss: 0.5423979163169861\n",
      "Epoch 657 loss: 0.5422865152359009\n",
      "Epoch 658 loss: 0.5421752333641052\n",
      "Epoch 659 loss: 0.5420640707015991\n",
      "Epoch 660 loss: 0.5419531464576721\n",
      "Epoch 661 loss: 0.5418422222137451\n",
      "Epoch 662 loss: 0.5417315363883972\n",
      "Epoch 663 loss: 0.5416209697723389\n",
      "Epoch 664 loss: 0.5415105819702148\n",
      "Epoch 665 loss: 0.5414002537727356\n",
      "Epoch 666 loss: 0.5412901639938354\n",
      "Epoch 667 loss: 0.5411801934242249\n",
      "Epoch 668 loss: 0.5410704016685486\n",
      "Epoch 669 loss: 0.5409606695175171\n",
      "Epoch 670 loss: 0.5408510565757751\n",
      "Epoch 671 loss: 0.5407416820526123\n",
      "Epoch 672 loss: 0.5406323671340942\n",
      "Epoch 673 loss: 0.5405232310295105\n",
      "Epoch 674 loss: 0.5404143333435059\n",
      "Epoch 675 loss: 0.540305495262146\n",
      "Epoch 676 loss: 0.5401967167854309\n",
      "Epoch 677 loss: 0.5400882363319397\n",
      "Epoch 678 loss: 0.5399797558784485\n",
      "Epoch 679 loss: 0.5398715138435364\n",
      "Epoch 680 loss: 0.5397633910179138\n",
      "Epoch 681 loss: 0.5396553874015808\n",
      "Epoch 682 loss: 0.5395475029945374\n",
      "Epoch 683 loss: 0.539439857006073\n",
      "Epoch 684 loss: 0.5393322110176086\n",
      "Epoch 685 loss: 0.5392248034477234\n",
      "Epoch 686 loss: 0.5391174554824829\n",
      "Epoch 687 loss: 0.5390103459358215\n",
      "Epoch 688 loss: 0.5389032959938049\n",
      "Epoch 689 loss: 0.5387964248657227\n",
      "Epoch 690 loss: 0.5386896729469299\n",
      "Epoch 691 loss: 0.5385830402374268\n",
      "Epoch 692 loss: 0.5384765863418579\n",
      "Epoch 693 loss: 0.5383702516555786\n",
      "Epoch 694 loss: 0.5382640361785889\n",
      "Epoch 695 loss: 0.5381579995155334\n",
      "Epoch 696 loss: 0.5380520224571228\n",
      "Epoch 697 loss: 0.5379462838172913\n",
      "Epoch 698 loss: 0.5378406643867493\n",
      "Epoch 699 loss: 0.537735104560852\n",
      "Epoch 700 loss: 0.5376297831535339\n",
      "Epoch 701 loss: 0.5375245213508606\n",
      "Epoch 702 loss: 0.5374194383621216\n",
      "Epoch 703 loss: 0.5373143553733826\n",
      "Epoch 704 loss: 0.5372095704078674\n",
      "Epoch 705 loss: 0.5371048450469971\n",
      "Epoch 706 loss: 0.5370002388954163\n",
      "Epoch 707 loss: 0.5368958115577698\n",
      "Epoch 708 loss: 0.5367915630340576\n",
      "Epoch 709 loss: 0.5366873741149902\n",
      "Epoch 710 loss: 0.5365832448005676\n",
      "Epoch 711 loss: 0.5364794135093689\n",
      "Epoch 712 loss: 0.5363755822181702\n",
      "Epoch 713 loss: 0.5362719297409058\n",
      "Epoch 714 loss: 0.5361684560775757\n",
      "Epoch 715 loss: 0.5360650420188904\n",
      "Epoch 716 loss: 0.5359618067741394\n",
      "Epoch 717 loss: 0.5358586311340332\n",
      "Epoch 718 loss: 0.5357556343078613\n",
      "Epoch 719 loss: 0.535652756690979\n",
      "Epoch 720 loss: 0.5355499982833862\n",
      "Epoch 721 loss: 0.5354474186897278\n",
      "Epoch 722 loss: 0.5353449583053589\n",
      "Epoch 723 loss: 0.5352426171302795\n",
      "Epoch 724 loss: 0.5351403951644897\n",
      "Epoch 725 loss: 0.5350382924079895\n",
      "Epoch 726 loss: 0.5349363684654236\n",
      "Epoch 727 loss: 0.5348345637321472\n",
      "Epoch 728 loss: 0.5347327589988708\n",
      "Epoch 729 loss: 0.5346311926841736\n",
      "Epoch 730 loss: 0.5345298051834106\n",
      "Epoch 731 loss: 0.5344284176826477\n",
      "Epoch 732 loss: 0.5343272686004639\n",
      "Epoch 733 loss: 0.5342262387275696\n",
      "Epoch 734 loss: 0.5341252684593201\n",
      "Epoch 735 loss: 0.5340244174003601\n",
      "Epoch 736 loss: 0.5339237451553345\n",
      "Epoch 737 loss: 0.5338232517242432\n",
      "Epoch 738 loss: 0.5337227582931519\n",
      "Epoch 739 loss: 0.5336223840713501\n",
      "Epoch 740 loss: 0.5335222482681274\n",
      "Epoch 741 loss: 0.5334221720695496\n",
      "Epoch 742 loss: 0.533322274684906\n",
      "Epoch 743 loss: 0.5332224369049072\n",
      "Epoch 744 loss: 0.533122718334198\n",
      "Epoch 745 loss: 0.5330231785774231\n",
      "Epoch 746 loss: 0.5329237580299377\n",
      "Epoch 747 loss: 0.5328243970870972\n",
      "Epoch 748 loss: 0.5327252149581909\n",
      "Epoch 749 loss: 0.532626211643219\n",
      "Epoch 750 loss: 0.5325272083282471\n",
      "Epoch 751 loss: 0.5324283838272095\n",
      "Epoch 752 loss: 0.5323296785354614\n",
      "Epoch 753 loss: 0.5322310328483582\n",
      "Epoch 754 loss: 0.532132625579834\n",
      "Epoch 755 loss: 0.5320342779159546\n",
      "Epoch 756 loss: 0.5319360494613647\n",
      "Epoch 757 loss: 0.5318379402160645\n",
      "Epoch 758 loss: 0.5317400097846985\n",
      "Epoch 759 loss: 0.5316421389579773\n",
      "Epoch 760 loss: 0.5315443873405457\n",
      "Epoch 761 loss: 0.5314468145370483\n",
      "Epoch 762 loss: 0.531349241733551\n",
      "Epoch 763 loss: 0.5312519073486328\n",
      "Epoch 764 loss: 0.5311546325683594\n",
      "Epoch 765 loss: 0.5310575366020203\n",
      "Epoch 766 loss: 0.5309604406356812\n",
      "Epoch 767 loss: 0.5308636426925659\n",
      "Epoch 768 loss: 0.5307668447494507\n",
      "Epoch 769 loss: 0.5306701064109802\n",
      "Epoch 770 loss: 0.5305736064910889\n",
      "Epoch 771 loss: 0.5304772257804871\n",
      "Epoch 772 loss: 0.5303808450698853\n",
      "Epoch 773 loss: 0.5302847027778625\n",
      "Epoch 774 loss: 0.5301886200904846\n",
      "Epoch 775 loss: 0.5300926566123962\n",
      "Epoch 776 loss: 0.5299968123435974\n",
      "Epoch 777 loss: 0.5299010276794434\n",
      "Epoch 778 loss: 0.5298054814338684\n",
      "Epoch 779 loss: 0.5297099947929382\n",
      "Epoch 780 loss: 0.5296146273612976\n",
      "Epoch 781 loss: 0.5295193791389465\n",
      "Epoch 782 loss: 0.529424250125885\n",
      "Epoch 783 loss: 0.5293291807174683\n",
      "Epoch 784 loss: 0.5292343497276306\n",
      "Epoch 785 loss: 0.529139518737793\n",
      "Epoch 786 loss: 0.5290448069572449\n",
      "Epoch 787 loss: 0.5289502143859863\n",
      "Epoch 788 loss: 0.5288558006286621\n",
      "Epoch 789 loss: 0.5287614464759827\n",
      "Epoch 790 loss: 0.5286672115325928\n",
      "Epoch 791 loss: 0.5285730957984924\n",
      "Epoch 792 loss: 0.5284790992736816\n",
      "Epoch 793 loss: 0.5283852815628052\n",
      "Epoch 794 loss: 0.5282914638519287\n",
      "Epoch 795 loss: 0.5281977653503418\n",
      "Epoch 796 loss: 0.5281042456626892\n",
      "Epoch 797 loss: 0.5280107855796814\n",
      "Epoch 798 loss: 0.5279174447059631\n",
      "Epoch 799 loss: 0.5278242826461792\n",
      "Epoch 800 loss: 0.52773118019104\n",
      "Epoch 801 loss: 0.5276381969451904\n",
      "Epoch 802 loss: 0.5275452733039856\n",
      "Epoch 803 loss: 0.5274525284767151\n",
      "Epoch 804 loss: 0.5273599028587341\n",
      "Epoch 805 loss: 0.5272672772407532\n",
      "Epoch 806 loss: 0.5271748900413513\n",
      "Epoch 807 loss: 0.5270825624465942\n",
      "Epoch 808 loss: 0.5269903540611267\n",
      "Epoch 809 loss: 0.5268982648849487\n",
      "Epoch 810 loss: 0.5268062949180603\n",
      "Epoch 811 loss: 0.5267143249511719\n",
      "Epoch 812 loss: 0.5266226530075073\n",
      "Epoch 813 loss: 0.5265308618545532\n",
      "Epoch 814 loss: 0.526439368724823\n",
      "Epoch 815 loss: 0.5263478755950928\n",
      "Epoch 816 loss: 0.5262565612792969\n",
      "Epoch 817 loss: 0.5261653065681458\n",
      "Epoch 818 loss: 0.526074230670929\n",
      "Epoch 819 loss: 0.5259832143783569\n",
      "Epoch 820 loss: 0.5258922576904297\n",
      "Epoch 821 loss: 0.5258014798164368\n",
      "Epoch 822 loss: 0.5257107615470886\n",
      "Epoch 823 loss: 0.5256202220916748\n",
      "Epoch 824 loss: 0.5255297422409058\n",
      "Epoch 825 loss: 0.5254393815994263\n",
      "Epoch 826 loss: 0.5253490805625916\n",
      "Epoch 827 loss: 0.5252588987350464\n",
      "Epoch 828 loss: 0.5251688957214355\n",
      "Epoch 829 loss: 0.5250788927078247\n",
      "Epoch 830 loss: 0.5249890089035034\n",
      "Epoch 831 loss: 0.5248993039131165\n",
      "Epoch 832 loss: 0.5248096585273743\n",
      "Epoch 833 loss: 0.5247201919555664\n",
      "Epoch 834 loss: 0.5246307253837585\n",
      "Epoch 835 loss: 0.5245413780212402\n",
      "Epoch 836 loss: 0.5244521498680115\n",
      "Epoch 837 loss: 0.5243630409240723\n",
      "Epoch 838 loss: 0.5242741107940674\n",
      "Epoch 839 loss: 0.5241851806640625\n",
      "Epoch 840 loss: 0.5240963697433472\n",
      "Epoch 841 loss: 0.5240076780319214\n",
      "Epoch 842 loss: 0.5239191055297852\n",
      "Epoch 843 loss: 0.5238306522369385\n",
      "Epoch 844 loss: 0.5237421989440918\n",
      "Epoch 845 loss: 0.5236539244651794\n",
      "Epoch 846 loss: 0.5235657691955566\n",
      "Epoch 847 loss: 0.5234776735305786\n",
      "Epoch 848 loss: 0.5233896970748901\n",
      "Epoch 849 loss: 0.5233017802238464\n",
      "Epoch 850 loss: 0.5232140421867371\n",
      "Epoch 851 loss: 0.5231263637542725\n",
      "Epoch 852 loss: 0.5230388641357422\n",
      "Epoch 853 loss: 0.5229513049125671\n",
      "Epoch 854 loss: 0.5228639841079712\n",
      "Epoch 855 loss: 0.52277672290802\n",
      "Epoch 856 loss: 0.5226895213127136\n",
      "Epoch 857 loss: 0.522602379322052\n",
      "Epoch 858 loss: 0.5225154757499695\n",
      "Epoch 859 loss: 0.5224286317825317\n",
      "Epoch 860 loss: 0.5223418474197388\n",
      "Epoch 861 loss: 0.5222552418708801\n",
      "Epoch 862 loss: 0.5221686363220215\n",
      "Epoch 863 loss: 0.5220822095870972\n",
      "Epoch 864 loss: 0.5219958424568176\n",
      "Epoch 865 loss: 0.5219095349311829\n",
      "Epoch 866 loss: 0.5218234062194824\n",
      "Epoch 867 loss: 0.521737277507782\n",
      "Epoch 868 loss: 0.5216513276100159\n",
      "Epoch 869 loss: 0.5215654373168945\n",
      "Epoch 870 loss: 0.5214797258377075\n",
      "Epoch 871 loss: 0.5213939547538757\n",
      "Epoch 872 loss: 0.521308422088623\n",
      "Epoch 873 loss: 0.5212230086326599\n",
      "Epoch 874 loss: 0.521137535572052\n",
      "Epoch 875 loss: 0.5210523009300232\n",
      "Epoch 876 loss: 0.5209671258926392\n",
      "Epoch 877 loss: 0.5208820104598999\n",
      "Epoch 878 loss: 0.5207970142364502\n",
      "Epoch 879 loss: 0.52071213722229\n",
      "Epoch 880 loss: 0.5206273198127747\n",
      "Epoch 881 loss: 0.520542562007904\n",
      "Epoch 882 loss: 0.5204580426216125\n",
      "Epoch 883 loss: 0.5203734636306763\n",
      "Epoch 884 loss: 0.5202890634536743\n",
      "Epoch 885 loss: 0.5202047228813171\n",
      "Epoch 886 loss: 0.5201205015182495\n",
      "Epoch 887 loss: 0.5200363993644714\n",
      "Epoch 888 loss: 0.5199522972106934\n",
      "Epoch 889 loss: 0.5198684334754944\n",
      "Epoch 890 loss: 0.5197845697402954\n",
      "Epoch 891 loss: 0.5197008848190308\n",
      "Epoch 892 loss: 0.5196171402931213\n",
      "Epoch 893 loss: 0.5195335745811462\n",
      "Epoch 894 loss: 0.5194500684738159\n",
      "Epoch 895 loss: 0.5193667411804199\n",
      "Epoch 896 loss: 0.5192834138870239\n",
      "Epoch 897 loss: 0.519200325012207\n",
      "Epoch 898 loss: 0.5191171765327454\n",
      "Epoch 899 loss: 0.519034206867218\n",
      "Epoch 900 loss: 0.5189512372016907\n",
      "Epoch 901 loss: 0.5188684463500977\n",
      "Epoch 902 loss: 0.5187857151031494\n",
      "Epoch 903 loss: 0.518703043460846\n",
      "Epoch 904 loss: 0.5186205506324768\n",
      "Epoch 905 loss: 0.5185380578041077\n",
      "Epoch 906 loss: 0.5184558033943176\n",
      "Epoch 907 loss: 0.5183734893798828\n",
      "Epoch 908 loss: 0.5182912945747375\n",
      "Epoch 909 loss: 0.5182092785835266\n",
      "Epoch 910 loss: 0.5181272029876709\n",
      "Epoch 911 loss: 0.5180453658103943\n",
      "Epoch 912 loss: 0.5179635286331177\n",
      "Epoch 913 loss: 0.5178818106651306\n",
      "Epoch 914 loss: 0.5178002119064331\n",
      "Epoch 915 loss: 0.5177186727523804\n",
      "Epoch 916 loss: 0.5176372528076172\n",
      "Epoch 917 loss: 0.5175558924674988\n",
      "Epoch 918 loss: 0.5174745917320251\n",
      "Epoch 919 loss: 0.5173934698104858\n",
      "Epoch 920 loss: 0.5173124074935913\n",
      "Epoch 921 loss: 0.5172314047813416\n",
      "Epoch 922 loss: 0.5171505212783813\n",
      "Epoch 923 loss: 0.5170696973800659\n",
      "Epoch 924 loss: 0.5169888734817505\n",
      "Epoch 925 loss: 0.5169082880020142\n",
      "Epoch 926 loss: 0.5168277025222778\n",
      "Epoch 927 loss: 0.5167472958564758\n",
      "Epoch 928 loss: 0.5166669487953186\n",
      "Epoch 929 loss: 0.5165866017341614\n",
      "Epoch 930 loss: 0.5165064930915833\n",
      "Epoch 931 loss: 0.5164263844490051\n",
      "Epoch 932 loss: 0.516346275806427\n",
      "Epoch 933 loss: 0.516266405582428\n",
      "Epoch 934 loss: 0.516186535358429\n",
      "Epoch 935 loss: 0.5161067843437195\n",
      "Epoch 936 loss: 0.51602703332901\n",
      "Epoch 937 loss: 0.5159475207328796\n",
      "Epoch 938 loss: 0.515868067741394\n",
      "Epoch 939 loss: 0.5157886147499084\n",
      "Epoch 940 loss: 0.5157093405723572\n",
      "Epoch 941 loss: 0.5156300663948059\n",
      "Epoch 942 loss: 0.5155509114265442\n",
      "Epoch 943 loss: 0.5154718160629272\n",
      "Epoch 944 loss: 0.5153928995132446\n",
      "Epoch 945 loss: 0.515313982963562\n",
      "Epoch 946 loss: 0.515235185623169\n",
      "Epoch 947 loss: 0.5151564478874207\n",
      "Epoch 948 loss: 0.5150778293609619\n",
      "Epoch 949 loss: 0.514999270439148\n",
      "Epoch 950 loss: 0.5149208307266235\n",
      "Epoch 951 loss: 0.5148424506187439\n",
      "Epoch 952 loss: 0.514764130115509\n",
      "Epoch 953 loss: 0.5146859884262085\n",
      "Epoch 954 loss: 0.514607846736908\n",
      "Epoch 955 loss: 0.5145297646522522\n",
      "Epoch 956 loss: 0.5144518613815308\n",
      "Epoch 957 loss: 0.5143739581108093\n",
      "Epoch 958 loss: 0.5142961740493774\n",
      "Epoch 959 loss: 0.5142184495925903\n",
      "Epoch 960 loss: 0.5141408443450928\n",
      "Epoch 961 loss: 0.51406329870224\n",
      "Epoch 962 loss: 0.513985812664032\n",
      "Epoch 963 loss: 0.5139084458351135\n",
      "Epoch 964 loss: 0.5138311982154846\n",
      "Epoch 965 loss: 0.5137540102005005\n",
      "Epoch 966 loss: 0.5136768817901611\n",
      "Epoch 967 loss: 0.5135998129844666\n",
      "Epoch 968 loss: 0.5135228633880615\n",
      "Epoch 969 loss: 0.5134459733963013\n",
      "Epoch 970 loss: 0.5133692026138306\n",
      "Epoch 971 loss: 0.5132924914360046\n",
      "Epoch 972 loss: 0.5132158994674683\n",
      "Epoch 973 loss: 0.5131393671035767\n",
      "Epoch 974 loss: 0.5130628943443298\n",
      "Epoch 975 loss: 0.5129864811897278\n",
      "Epoch 976 loss: 0.5129101872444153\n",
      "Epoch 977 loss: 0.5128340125083923\n",
      "Epoch 978 loss: 0.5127578973770142\n",
      "Epoch 979 loss: 0.512681782245636\n",
      "Epoch 980 loss: 0.5126057863235474\n",
      "Epoch 981 loss: 0.5125299096107483\n",
      "Epoch 982 loss: 0.5124541521072388\n",
      "Epoch 983 loss: 0.5123783946037292\n",
      "Epoch 984 loss: 0.5123026967048645\n",
      "Epoch 985 loss: 0.5122271180152893\n",
      "Epoch 986 loss: 0.5121515989303589\n",
      "Epoch 987 loss: 0.512076199054718\n",
      "Epoch 988 loss: 0.5120008587837219\n",
      "Epoch 989 loss: 0.5119256377220154\n",
      "Epoch 990 loss: 0.5118504166603088\n",
      "Epoch 991 loss: 0.5117753148078918\n",
      "Epoch 992 loss: 0.5117003321647644\n",
      "Epoch 993 loss: 0.5116254091262817\n",
      "Epoch 994 loss: 0.5115505456924438\n",
      "Epoch 995 loss: 0.5114757418632507\n",
      "Epoch 996 loss: 0.5114010572433472\n",
      "Epoch 997 loss: 0.5113264322280884\n",
      "Epoch 998 loss: 0.5112518668174744\n",
      "Epoch 999 loss: 0.5111774206161499\n",
      "Epoch 1000 loss: 0.5111030340194702\n",
      "Epoch 1001 loss: 0.5110287070274353\n",
      "Epoch 1002 loss: 0.5109544992446899\n",
      "Epoch 1003 loss: 0.5108803510665894\n",
      "Epoch 1004 loss: 0.5108062624931335\n",
      "Epoch 1005 loss: 0.5107322335243225\n",
      "Epoch 1006 loss: 0.510658323764801\n",
      "Epoch 1007 loss: 0.5105844736099243\n",
      "Epoch 1008 loss: 0.5105107426643372\n",
      "Epoch 1009 loss: 0.51043701171875\n",
      "Epoch 1010 loss: 0.5103633403778076\n",
      "Epoch 1011 loss: 0.5102899074554443\n",
      "Epoch 1012 loss: 0.5102164149284363\n",
      "Epoch 1013 loss: 0.5101430416107178\n",
      "Epoch 1014 loss: 0.510069727897644\n",
      "Epoch 1015 loss: 0.5099964737892151\n",
      "Epoch 1016 loss: 0.5099233388900757\n",
      "Epoch 1017 loss: 0.5098502039909363\n",
      "Epoch 1018 loss: 0.509777307510376\n",
      "Epoch 1019 loss: 0.5097042918205261\n",
      "Epoch 1020 loss: 0.5096314549446106\n",
      "Epoch 1021 loss: 0.5095586776733398\n",
      "Epoch 1022 loss: 0.5094859600067139\n",
      "Epoch 1023 loss: 0.5094133019447327\n",
      "Epoch 1024 loss: 0.5093408226966858\n",
      "Epoch 1025 loss: 0.5092682838439941\n",
      "Epoch 1026 loss: 0.509195864200592\n",
      "Epoch 1027 loss: 0.5091235637664795\n",
      "Epoch 1028 loss: 0.5090513229370117\n",
      "Epoch 1029 loss: 0.508979082107544\n",
      "Epoch 1030 loss: 0.5089070200920105\n",
      "Epoch 1031 loss: 0.5088350176811218\n",
      "Epoch 1032 loss: 0.5087630152702332\n",
      "Epoch 1033 loss: 0.508691132068634\n",
      "Epoch 1034 loss: 0.5086193084716797\n",
      "Epoch 1035 loss: 0.5085476040840149\n",
      "Epoch 1036 loss: 0.5084758996963501\n",
      "Epoch 1037 loss: 0.5084043145179749\n",
      "Epoch 1038 loss: 0.5083328485488892\n",
      "Epoch 1039 loss: 0.5082613825798035\n",
      "Epoch 1040 loss: 0.5081899166107178\n",
      "Epoch 1041 loss: 0.5081186890602112\n",
      "Epoch 1042 loss: 0.5080474615097046\n",
      "Epoch 1043 loss: 0.5079762935638428\n",
      "Epoch 1044 loss: 0.5079051852226257\n",
      "Epoch 1045 loss: 0.5078341364860535\n",
      "Epoch 1046 loss: 0.5077632665634155\n",
      "Epoch 1047 loss: 0.5076923966407776\n",
      "Epoch 1048 loss: 0.5076216459274292\n",
      "Epoch 1049 loss: 0.5075508952140808\n",
      "Epoch 1050 loss: 0.507480263710022\n",
      "Epoch 1051 loss: 0.5074096918106079\n",
      "Epoch 1052 loss: 0.5073391199111938\n",
      "Epoch 1053 loss: 0.5072687268257141\n",
      "Epoch 1054 loss: 0.5071983337402344\n",
      "Epoch 1055 loss: 0.5071280598640442\n",
      "Epoch 1056 loss: 0.507057785987854\n",
      "Epoch 1057 loss: 0.5069876909255981\n",
      "Epoch 1058 loss: 0.5069175958633423\n",
      "Epoch 1059 loss: 0.506847620010376\n",
      "Epoch 1060 loss: 0.5067776441574097\n",
      "Epoch 1061 loss: 0.5067077279090881\n",
      "Epoch 1062 loss: 0.5066379904747009\n",
      "Epoch 1063 loss: 0.5065682530403137\n",
      "Epoch 1064 loss: 0.5064985752105713\n",
      "Epoch 1065 loss: 0.5064290761947632\n",
      "Epoch 1066 loss: 0.5063594579696655\n",
      "Epoch 1067 loss: 0.5062900185585022\n",
      "Epoch 1068 loss: 0.5062206387519836\n",
      "Epoch 1069 loss: 0.5061513781547546\n",
      "Epoch 1070 loss: 0.5060820579528809\n",
      "Epoch 1071 loss: 0.5060129761695862\n",
      "Epoch 1072 loss: 0.5059438347816467\n",
      "Epoch 1073 loss: 0.5058748126029968\n",
      "Epoch 1074 loss: 0.5058058500289917\n",
      "Epoch 1075 loss: 0.5057369470596313\n",
      "Epoch 1076 loss: 0.5056681632995605\n",
      "Epoch 1077 loss: 0.5055993795394897\n",
      "Epoch 1078 loss: 0.5055306553840637\n",
      "Epoch 1079 loss: 0.5054620504379272\n",
      "Epoch 1080 loss: 0.5053935050964355\n",
      "Epoch 1081 loss: 0.5053249597549438\n",
      "Epoch 1082 loss: 0.5052565932273865\n",
      "Epoch 1083 loss: 0.5051882863044739\n",
      "Epoch 1084 loss: 0.5051199793815613\n",
      "Epoch 1085 loss: 0.5050517320632935\n",
      "Epoch 1086 loss: 0.5049835443496704\n",
      "Epoch 1087 loss: 0.5049155354499817\n",
      "Epoch 1088 loss: 0.504847526550293\n",
      "Epoch 1089 loss: 0.5047795176506042\n",
      "Epoch 1090 loss: 0.5047116875648499\n",
      "Epoch 1091 loss: 0.5046438574790955\n",
      "Epoch 1092 loss: 0.5045760869979858\n",
      "Epoch 1093 loss: 0.5045084357261658\n",
      "Epoch 1094 loss: 0.5044407844543457\n",
      "Epoch 1095 loss: 0.50437331199646\n",
      "Epoch 1096 loss: 0.5043058395385742\n",
      "Epoch 1097 loss: 0.5042384266853333\n",
      "Epoch 1098 loss: 0.5041710734367371\n",
      "Epoch 1099 loss: 0.5041037797927856\n",
      "Epoch 1100 loss: 0.5040366053581238\n",
      "Epoch 1101 loss: 0.5039694309234619\n",
      "Epoch 1102 loss: 0.5039023160934448\n",
      "Epoch 1103 loss: 0.5038353204727173\n",
      "Epoch 1104 loss: 0.5037683248519897\n",
      "Epoch 1105 loss: 0.5037015080451965\n",
      "Epoch 1106 loss: 0.5036346316337585\n",
      "Epoch 1107 loss: 0.5035679340362549\n",
      "Epoch 1108 loss: 0.5035011768341064\n",
      "Epoch 1109 loss: 0.5034345984458923\n",
      "Epoch 1110 loss: 0.503368079662323\n",
      "Epoch 1111 loss: 0.5033015608787537\n",
      "Epoch 1112 loss: 0.5032351016998291\n",
      "Epoch 1113 loss: 0.5031687617301941\n",
      "Epoch 1114 loss: 0.5031024813652039\n",
      "Epoch 1115 loss: 0.5030362010002136\n",
      "Epoch 1116 loss: 0.5029700398445129\n",
      "Epoch 1117 loss: 0.502903938293457\n",
      "Epoch 1118 loss: 0.5028379559516907\n",
      "Epoch 1119 loss: 0.5027719140052795\n",
      "Epoch 1120 loss: 0.5027060508728027\n",
      "Epoch 1121 loss: 0.5026401281356812\n",
      "Epoch 1122 loss: 0.5025743842124939\n",
      "Epoch 1123 loss: 0.5025086402893066\n",
      "Epoch 1124 loss: 0.5024429559707642\n",
      "Epoch 1125 loss: 0.5023773908615112\n",
      "Epoch 1126 loss: 0.5023118853569031\n",
      "Epoch 1127 loss: 0.5022464394569397\n",
      "Epoch 1128 loss: 0.5021809935569763\n",
      "Epoch 1129 loss: 0.5021156668663025\n",
      "Epoch 1130 loss: 0.5020503401756287\n",
      "Epoch 1131 loss: 0.5019851326942444\n",
      "Epoch 1132 loss: 0.5019199848175049\n",
      "Epoch 1133 loss: 0.5018549561500549\n",
      "Epoch 1134 loss: 0.5017898678779602\n",
      "Epoch 1135 loss: 0.5017249584197998\n",
      "Epoch 1136 loss: 0.5016599893569946\n",
      "Epoch 1137 loss: 0.501595139503479\n",
      "Epoch 1138 loss: 0.5015303492546082\n",
      "Epoch 1139 loss: 0.5014656782150269\n",
      "Epoch 1140 loss: 0.5014010071754456\n",
      "Epoch 1141 loss: 0.501336395740509\n",
      "Epoch 1142 loss: 0.5012719035148621\n",
      "Epoch 1143 loss: 0.5012073516845703\n",
      "Epoch 1144 loss: 0.5011429786682129\n",
      "Epoch 1145 loss: 0.5010786056518555\n",
      "Epoch 1146 loss: 0.5010143518447876\n",
      "Epoch 1147 loss: 0.5009500980377197\n",
      "Epoch 1148 loss: 0.5008859634399414\n",
      "Epoch 1149 loss: 0.5008218884468079\n",
      "Epoch 1150 loss: 0.5007578134536743\n",
      "Epoch 1151 loss: 0.5006938576698303\n",
      "Epoch 1152 loss: 0.5006299018859863\n",
      "Epoch 1153 loss: 0.5005660057067871\n",
      "Epoch 1154 loss: 0.5005022883415222\n",
      "Epoch 1155 loss: 0.5004385113716125\n",
      "Epoch 1156 loss: 0.5003747940063477\n",
      "Epoch 1157 loss: 0.5003112554550171\n",
      "Epoch 1158 loss: 0.5002476572990417\n",
      "Epoch 1159 loss: 0.500184178352356\n",
      "Epoch 1160 loss: 0.5001206994056702\n",
      "Epoch 1161 loss: 0.5000572800636292\n",
      "Epoch 1162 loss: 0.4999939501285553\n",
      "Epoch 1163 loss: 0.4999307096004486\n",
      "Epoch 1164 loss: 0.4998674988746643\n",
      "Epoch 1165 loss: 0.4998043477535248\n",
      "Epoch 1166 loss: 0.4997413158416748\n",
      "Epoch 1167 loss: 0.49967825412750244\n",
      "Epoch 1168 loss: 0.49961531162261963\n",
      "Epoch 1169 loss: 0.4995523989200592\n",
      "Epoch 1170 loss: 0.49948951601982117\n",
      "Epoch 1171 loss: 0.49942681193351746\n",
      "Epoch 1172 loss: 0.49936404824256897\n",
      "Epoch 1173 loss: 0.49930140376091003\n",
      "Epoch 1174 loss: 0.4992386996746063\n",
      "Epoch 1175 loss: 0.4991762042045593\n",
      "Epoch 1176 loss: 0.49911367893218994\n",
      "Epoch 1177 loss: 0.4990512430667877\n",
      "Epoch 1178 loss: 0.49898889660835266\n",
      "Epoch 1179 loss: 0.4989265501499176\n",
      "Epoch 1180 loss: 0.4988642930984497\n",
      "Epoch 1181 loss: 0.49880215525627136\n",
      "Epoch 1182 loss: 0.49873995780944824\n",
      "Epoch 1183 loss: 0.4986778497695923\n",
      "Epoch 1184 loss: 0.4986158311367035\n",
      "Epoch 1185 loss: 0.4985538721084595\n",
      "Epoch 1186 loss: 0.49849194288253784\n",
      "Epoch 1187 loss: 0.4984301030635834\n",
      "Epoch 1188 loss: 0.4983682632446289\n",
      "Epoch 1189 loss: 0.4983065724372864\n",
      "Epoch 1190 loss: 0.49824485182762146\n",
      "Epoch 1191 loss: 0.4981832206249237\n",
      "Epoch 1192 loss: 0.4981216490268707\n",
      "Epoch 1193 loss: 0.4980601370334625\n",
      "Epoch 1194 loss: 0.4979986846446991\n",
      "Epoch 1195 loss: 0.49793726205825806\n",
      "Epoch 1196 loss: 0.4978759288787842\n",
      "Epoch 1197 loss: 0.4978146553039551\n",
      "Epoch 1198 loss: 0.49775344133377075\n",
      "Epoch 1199 loss: 0.4976922869682312\n",
      "Epoch 1200 loss: 0.49763113260269165\n",
      "Epoch 1201 loss: 0.49757009744644165\n",
      "Epoch 1202 loss: 0.49750906229019165\n",
      "Epoch 1203 loss: 0.49744805693626404\n",
      "Epoch 1204 loss: 0.49738723039627075\n",
      "Epoch 1205 loss: 0.4973263740539551\n",
      "Epoch 1206 loss: 0.4972655773162842\n",
      "Epoch 1207 loss: 0.49720484018325806\n",
      "Epoch 1208 loss: 0.4971441924571991\n",
      "Epoch 1209 loss: 0.4970835745334625\n",
      "Epoch 1210 loss: 0.49702295660972595\n",
      "Epoch 1211 loss: 0.4969625174999237\n",
      "Epoch 1212 loss: 0.4969020485877991\n",
      "Epoch 1213 loss: 0.49684157967567444\n",
      "Epoch 1214 loss: 0.49678128957748413\n",
      "Epoch 1215 loss: 0.49672096967697144\n",
      "Epoch 1216 loss: 0.4966607689857483\n",
      "Epoch 1217 loss: 0.49660059809684753\n",
      "Epoch 1218 loss: 0.49654048681259155\n",
      "Epoch 1219 loss: 0.49648043513298035\n",
      "Epoch 1220 loss: 0.49642038345336914\n",
      "Epoch 1221 loss: 0.4963604807853699\n",
      "Epoch 1222 loss: 0.4963005483150482\n",
      "Epoch 1223 loss: 0.4962407052516937\n",
      "Epoch 1224 loss: 0.4961808919906616\n",
      "Epoch 1225 loss: 0.49612119793891907\n",
      "Epoch 1226 loss: 0.49606144428253174\n",
      "Epoch 1227 loss: 0.49600180983543396\n",
      "Epoch 1228 loss: 0.49594226479530334\n",
      "Epoch 1229 loss: 0.49588271975517273\n",
      "Epoch 1230 loss: 0.4958232343196869\n",
      "Epoch 1231 loss: 0.4957638084888458\n",
      "Epoch 1232 loss: 0.4957045018672943\n",
      "Epoch 1233 loss: 0.4956451952457428\n",
      "Epoch 1234 loss: 0.49558591842651367\n",
      "Epoch 1235 loss: 0.49552667140960693\n",
      "Epoch 1236 loss: 0.49546751379966736\n",
      "Epoch 1237 loss: 0.49540841579437256\n",
      "Epoch 1238 loss: 0.49534937739372253\n",
      "Epoch 1239 loss: 0.4952903985977173\n",
      "Epoch 1240 loss: 0.4952314496040344\n",
      "Epoch 1241 loss: 0.49517256021499634\n",
      "Epoch 1242 loss: 0.49511370062828064\n",
      "Epoch 1243 loss: 0.4950549602508545\n",
      "Epoch 1244 loss: 0.49499621987342834\n",
      "Epoch 1245 loss: 0.494937539100647\n",
      "Epoch 1246 loss: 0.49487894773483276\n",
      "Epoch 1247 loss: 0.4948202967643738\n",
      "Epoch 1248 loss: 0.4947618544101715\n",
      "Epoch 1249 loss: 0.4947033226490021\n",
      "Epoch 1250 loss: 0.49464496970176697\n",
      "Epoch 1251 loss: 0.4945865869522095\n",
      "Epoch 1252 loss: 0.49452829360961914\n",
      "Epoch 1253 loss: 0.4944700002670288\n",
      "Epoch 1254 loss: 0.494411826133728\n",
      "Epoch 1255 loss: 0.49435362219810486\n",
      "Epoch 1256 loss: 0.49429553747177124\n",
      "Epoch 1257 loss: 0.4942375123500824\n",
      "Epoch 1258 loss: 0.49417948722839355\n",
      "Epoch 1259 loss: 0.4941215217113495\n",
      "Epoch 1260 loss: 0.4940636456012726\n",
      "Epoch 1261 loss: 0.49400582909584045\n",
      "Epoch 1262 loss: 0.4939480125904083\n",
      "Epoch 1263 loss: 0.4938902258872986\n",
      "Epoch 1264 loss: 0.493832528591156\n",
      "Epoch 1265 loss: 0.4937749207019806\n",
      "Epoch 1266 loss: 0.4937172830104828\n",
      "Epoch 1267 loss: 0.4936597943305969\n",
      "Epoch 1268 loss: 0.49360227584838867\n",
      "Epoch 1269 loss: 0.49354487657546997\n",
      "Epoch 1270 loss: 0.4934874475002289\n",
      "Epoch 1271 loss: 0.49343013763427734\n",
      "Epoch 1272 loss: 0.4933728575706482\n",
      "Epoch 1273 loss: 0.49331557750701904\n",
      "Epoch 1274 loss: 0.49325838685035706\n",
      "Epoch 1275 loss: 0.49320125579833984\n",
      "Epoch 1276 loss: 0.49314412474632263\n",
      "Epoch 1277 loss: 0.49308711290359497\n",
      "Epoch 1278 loss: 0.4930301308631897\n",
      "Epoch 1279 loss: 0.4929731786251068\n",
      "Epoch 1280 loss: 0.4929163157939911\n",
      "Epoch 1281 loss: 0.49285945296287537\n",
      "Epoch 1282 loss: 0.4928026795387268\n",
      "Epoch 1283 loss: 0.49274590611457825\n",
      "Epoch 1284 loss: 0.4926892817020416\n",
      "Epoch 1285 loss: 0.49263259768486023\n",
      "Epoch 1286 loss: 0.4925760328769684\n",
      "Epoch 1287 loss: 0.49251946806907654\n",
      "Epoch 1288 loss: 0.49246296286582947\n",
      "Epoch 1289 loss: 0.4924065172672272\n",
      "Epoch 1290 loss: 0.49235013127326965\n",
      "Epoch 1291 loss: 0.4922937750816345\n",
      "Epoch 1292 loss: 0.49223750829696655\n",
      "Epoch 1293 loss: 0.49218130111694336\n",
      "Epoch 1294 loss: 0.492125004529953\n",
      "Epoch 1295 loss: 0.49206891655921936\n",
      "Epoch 1296 loss: 0.49201279878616333\n",
      "Epoch 1297 loss: 0.49195677042007446\n",
      "Epoch 1298 loss: 0.4919007122516632\n",
      "Epoch 1299 loss: 0.4918448030948639\n",
      "Epoch 1300 loss: 0.4917888939380646\n",
      "Epoch 1301 loss: 0.49173298478126526\n",
      "Epoch 1302 loss: 0.4916772246360779\n",
      "Epoch 1303 loss: 0.4916214346885681\n",
      "Epoch 1304 loss: 0.4915657341480255\n",
      "Epoch 1305 loss: 0.4915100634098053\n",
      "Epoch 1306 loss: 0.4914543926715851\n",
      "Epoch 1307 loss: 0.4913988411426544\n",
      "Epoch 1308 loss: 0.49134331941604614\n",
      "Epoch 1309 loss: 0.49128779768943787\n",
      "Epoch 1310 loss: 0.49123236536979675\n",
      "Epoch 1311 loss: 0.4911770522594452\n",
      "Epoch 1312 loss: 0.49112170934677124\n",
      "Epoch 1313 loss: 0.49106642603874207\n",
      "Epoch 1314 loss: 0.4910111129283905\n",
      "Epoch 1315 loss: 0.49095597863197327\n",
      "Epoch 1316 loss: 0.49090081453323364\n",
      "Epoch 1317 loss: 0.4908457100391388\n",
      "Epoch 1318 loss: 0.4907906949520111\n",
      "Epoch 1319 loss: 0.49073565006256104\n",
      "Epoch 1320 loss: 0.4906807243824005\n",
      "Epoch 1321 loss: 0.4906257688999176\n",
      "Epoch 1322 loss: 0.49057090282440186\n",
      "Epoch 1323 loss: 0.49051612615585327\n",
      "Epoch 1324 loss: 0.4904613792896271\n",
      "Epoch 1325 loss: 0.4904066324234009\n",
      "Epoch 1326 loss: 0.49035191535949707\n",
      "Epoch 1327 loss: 0.4902972877025604\n",
      "Epoch 1328 loss: 0.49024268984794617\n",
      "Epoch 1329 loss: 0.4901881814002991\n",
      "Epoch 1330 loss: 0.490133672952652\n",
      "Epoch 1331 loss: 0.49007922410964966\n",
      "Epoch 1332 loss: 0.4900248050689697\n",
      "Epoch 1333 loss: 0.48997044563293457\n",
      "Epoch 1334 loss: 0.4899161756038666\n",
      "Epoch 1335 loss: 0.4898618757724762\n",
      "Epoch 1336 loss: 0.489807665348053\n",
      "Epoch 1337 loss: 0.4897535443305969\n",
      "Epoch 1338 loss: 0.4896993935108185\n",
      "Epoch 1339 loss: 0.4896452724933624\n",
      "Epoch 1340 loss: 0.48959124088287354\n",
      "Epoch 1341 loss: 0.4895372986793518\n",
      "Epoch 1342 loss: 0.4894833266735077\n",
      "Epoch 1343 loss: 0.48942941427230835\n",
      "Epoch 1344 loss: 0.48937562108039856\n",
      "Epoch 1345 loss: 0.489321768283844\n",
      "Epoch 1346 loss: 0.4892680048942566\n",
      "Epoch 1347 loss: 0.48921430110931396\n",
      "Epoch 1348 loss: 0.4891606271266937\n",
      "Epoch 1349 loss: 0.4891069829463959\n",
      "Epoch 1350 loss: 0.4890533983707428\n",
      "Epoch 1351 loss: 0.4889998733997345\n",
      "Epoch 1352 loss: 0.4889463782310486\n",
      "Epoch 1353 loss: 0.48889294266700745\n",
      "Epoch 1354 loss: 0.4888395369052887\n",
      "Epoch 1355 loss: 0.48878613114356995\n",
      "Epoch 1356 loss: 0.48873284459114075\n",
      "Epoch 1357 loss: 0.48867955803871155\n",
      "Epoch 1358 loss: 0.4886263310909271\n",
      "Epoch 1359 loss: 0.4885731637477875\n",
      "Epoch 1360 loss: 0.4885200262069702\n",
      "Epoch 1361 loss: 0.48846691846847534\n",
      "Epoch 1362 loss: 0.48841390013694763\n",
      "Epoch 1363 loss: 0.48836085200309753\n",
      "Epoch 1364 loss: 0.4883078932762146\n",
      "Epoch 1365 loss: 0.48825499415397644\n",
      "Epoch 1366 loss: 0.48820212483406067\n",
      "Epoch 1367 loss: 0.4881492853164673\n",
      "Epoch 1368 loss: 0.4880964457988739\n",
      "Epoch 1369 loss: 0.48804378509521484\n",
      "Epoch 1370 loss: 0.487991064786911\n",
      "Epoch 1371 loss: 0.48793843388557434\n",
      "Epoch 1372 loss: 0.4878857731819153\n",
      "Epoch 1373 loss: 0.48783326148986816\n",
      "Epoch 1374 loss: 0.4877806603908539\n",
      "Epoch 1375 loss: 0.48772817850112915\n",
      "Epoch 1376 loss: 0.4876757264137268\n",
      "Epoch 1377 loss: 0.4876234233379364\n",
      "Epoch 1378 loss: 0.48757103085517883\n",
      "Epoch 1379 loss: 0.48751869797706604\n",
      "Epoch 1380 loss: 0.48746639490127563\n",
      "Epoch 1381 loss: 0.4874142110347748\n",
      "Epoch 1382 loss: 0.4873620569705963\n",
      "Epoch 1383 loss: 0.48730990290641785\n",
      "Epoch 1384 loss: 0.4872577488422394\n",
      "Epoch 1385 loss: 0.48720574378967285\n",
      "Epoch 1386 loss: 0.4871537387371063\n",
      "Epoch 1387 loss: 0.4871017634868622\n",
      "Epoch 1388 loss: 0.4870498478412628\n",
      "Epoch 1389 loss: 0.4869979918003082\n",
      "Epoch 1390 loss: 0.48694607615470886\n",
      "Epoch 1391 loss: 0.48689430952072144\n",
      "Epoch 1392 loss: 0.486842542886734\n",
      "Epoch 1393 loss: 0.48679086565971375\n",
      "Epoch 1394 loss: 0.4867391884326935\n",
      "Epoch 1395 loss: 0.4866875410079956\n",
      "Epoch 1396 loss: 0.48663589358329773\n",
      "Epoch 1397 loss: 0.4865843951702118\n",
      "Epoch 1398 loss: 0.48653289675712585\n",
      "Epoch 1399 loss: 0.4864814877510071\n",
      "Epoch 1400 loss: 0.48643001914024353\n",
      "Epoch 1401 loss: 0.48637866973876953\n",
      "Epoch 1402 loss: 0.48632729053497314\n",
      "Epoch 1403 loss: 0.4862760603427887\n",
      "Epoch 1404 loss: 0.4862247705459595\n",
      "Epoch 1405 loss: 0.4861735701560974\n",
      "Epoch 1406 loss: 0.48612239956855774\n",
      "Epoch 1407 loss: 0.48607125878334045\n",
      "Epoch 1408 loss: 0.48602011799812317\n",
      "Epoch 1409 loss: 0.4859691262245178\n",
      "Epoch 1410 loss: 0.4859181046485901\n",
      "Epoch 1411 loss: 0.4858671724796295\n",
      "Epoch 1412 loss: 0.48581624031066895\n",
      "Epoch 1413 loss: 0.4857653081417084\n",
      "Epoch 1414 loss: 0.48571446537971497\n",
      "Epoch 1415 loss: 0.4856637120246887\n",
      "Epoch 1416 loss: 0.4856128692626953\n",
      "Epoch 1417 loss: 0.48556220531463623\n",
      "Epoch 1418 loss: 0.48551151156425476\n",
      "Epoch 1419 loss: 0.48546090722084045\n",
      "Epoch 1420 loss: 0.48541027307510376\n",
      "Epoch 1421 loss: 0.4853597581386566\n",
      "Epoch 1422 loss: 0.4853091835975647\n",
      "Epoch 1423 loss: 0.48525869846343994\n",
      "Epoch 1424 loss: 0.48520833253860474\n",
      "Epoch 1425 loss: 0.48515790700912476\n",
      "Epoch 1426 loss: 0.48510754108428955\n",
      "Epoch 1427 loss: 0.48505720496177673\n",
      "Epoch 1428 loss: 0.48500698804855347\n",
      "Epoch 1429 loss: 0.4849567413330078\n",
      "Epoch 1430 loss: 0.48490652441978455\n",
      "Epoch 1431 loss: 0.48485636711120605\n",
      "Epoch 1432 loss: 0.4848062992095947\n",
      "Epoch 1433 loss: 0.484756201505661\n",
      "Epoch 1434 loss: 0.48470616340637207\n",
      "Epoch 1435 loss: 0.4846561849117279\n",
      "Epoch 1436 loss: 0.48460620641708374\n",
      "Epoch 1437 loss: 0.4845563471317291\n",
      "Epoch 1438 loss: 0.48450639843940735\n",
      "Epoch 1439 loss: 0.4844566285610199\n",
      "Epoch 1440 loss: 0.4844067692756653\n",
      "Epoch 1441 loss: 0.4843570590019226\n",
      "Epoch 1442 loss: 0.48430728912353516\n",
      "Epoch 1443 loss: 0.48425763845443726\n",
      "Epoch 1444 loss: 0.48420798778533936\n",
      "Epoch 1445 loss: 0.48415839672088623\n",
      "Epoch 1446 loss: 0.4841088652610779\n",
      "Epoch 1447 loss: 0.48405933380126953\n",
      "Epoch 1448 loss: 0.48400986194610596\n",
      "Epoch 1449 loss: 0.48396036028862\n",
      "Epoch 1450 loss: 0.4839109778404236\n",
      "Epoch 1451 loss: 0.4838615655899048\n",
      "Epoch 1452 loss: 0.48381227254867554\n",
      "Epoch 1453 loss: 0.48376303911209106\n",
      "Epoch 1454 loss: 0.4837137460708618\n",
      "Epoch 1455 loss: 0.4836645722389221\n",
      "Epoch 1456 loss: 0.48361536860466003\n",
      "Epoch 1457 loss: 0.48356619477272034\n",
      "Epoch 1458 loss: 0.4835171103477478\n",
      "Epoch 1459 loss: 0.48346805572509766\n",
      "Epoch 1460 loss: 0.4834190905094147\n",
      "Epoch 1461 loss: 0.4833700656890869\n",
      "Epoch 1462 loss: 0.4833211600780487\n",
      "Epoch 1463 loss: 0.4832721948623657\n",
      "Epoch 1464 loss: 0.4832233488559723\n",
      "Epoch 1465 loss: 0.48317450284957886\n",
      "Epoch 1466 loss: 0.483125776052475\n",
      "Epoch 1467 loss: 0.4830770194530487\n",
      "Epoch 1468 loss: 0.48302826285362244\n",
      "Epoch 1469 loss: 0.4829796254634857\n",
      "Epoch 1470 loss: 0.4829309284687042\n",
      "Epoch 1471 loss: 0.4828823506832123\n",
      "Epoch 1472 loss: 0.48283377289772034\n",
      "Epoch 1473 loss: 0.48278525471687317\n",
      "Epoch 1474 loss: 0.48273682594299316\n",
      "Epoch 1475 loss: 0.48268836736679077\n",
      "Epoch 1476 loss: 0.4826399087905884\n",
      "Epoch 1477 loss: 0.48259156942367554\n",
      "Epoch 1478 loss: 0.4825432300567627\n",
      "Epoch 1479 loss: 0.48249495029449463\n",
      "Epoch 1480 loss: 0.48244673013687134\n",
      "Epoch 1481 loss: 0.4823984205722809\n",
      "Epoch 1482 loss: 0.48235026001930237\n",
      "Epoch 1483 loss: 0.48230206966400146\n",
      "Epoch 1484 loss: 0.48225393891334534\n",
      "Epoch 1485 loss: 0.48220592737197876\n",
      "Epoch 1486 loss: 0.4821578860282898\n",
      "Epoch 1487 loss: 0.4821099042892456\n",
      "Epoch 1488 loss: 0.4820619225502014\n",
      "Epoch 1489 loss: 0.482014000415802\n",
      "Epoch 1490 loss: 0.481966108083725\n",
      "Epoch 1491 loss: 0.48191824555397034\n",
      "Epoch 1492 loss: 0.4818704128265381\n",
      "Epoch 1493 loss: 0.4818226397037506\n",
      "Epoch 1494 loss: 0.4817749559879303\n",
      "Epoch 1495 loss: 0.4817271828651428\n",
      "Epoch 1496 loss: 0.4816795289516449\n",
      "Epoch 1497 loss: 0.48163190484046936\n",
      "Epoch 1498 loss: 0.48158425092697144\n",
      "Epoch 1499 loss: 0.48153671622276306\n",
      "Epoch 1500 loss: 0.4814891815185547\n",
      "Epoch 1501 loss: 0.4814417064189911\n",
      "Epoch 1502 loss: 0.4813942313194275\n",
      "Epoch 1503 loss: 0.48134690523147583\n",
      "Epoch 1504 loss: 0.4812994599342346\n",
      "Epoch 1505 loss: 0.4812520742416382\n",
      "Epoch 1506 loss: 0.4812048375606537\n",
      "Epoch 1507 loss: 0.4811575412750244\n",
      "Epoch 1508 loss: 0.4811103045940399\n",
      "Epoch 1509 loss: 0.4810630679130554\n",
      "Epoch 1510 loss: 0.48101601004600525\n",
      "Epoch 1511 loss: 0.4809688329696655\n",
      "Epoch 1512 loss: 0.4809217154979706\n",
      "Epoch 1513 loss: 0.4808746874332428\n",
      "Epoch 1514 loss: 0.4808276295661926\n",
      "Epoch 1515 loss: 0.4807806611061096\n",
      "Epoch 1516 loss: 0.4807337522506714\n",
      "Epoch 1517 loss: 0.48068681359291077\n",
      "Epoch 1518 loss: 0.4806399345397949\n",
      "Epoch 1519 loss: 0.4805930554866791\n",
      "Epoch 1520 loss: 0.4805462658405304\n",
      "Epoch 1521 loss: 0.4804995059967041\n",
      "Epoch 1522 loss: 0.4804527759552002\n",
      "Epoch 1523 loss: 0.4804060757160187\n",
      "Epoch 1524 loss: 0.48035937547683716\n",
      "Epoch 1525 loss: 0.4803127646446228\n",
      "Epoch 1526 loss: 0.48026615381240845\n",
      "Epoch 1527 loss: 0.48021963238716125\n",
      "Epoch 1528 loss: 0.48017311096191406\n",
      "Epoch 1529 loss: 0.48012658953666687\n",
      "Epoch 1530 loss: 0.48008015751838684\n",
      "Epoch 1531 loss: 0.4800336956977844\n",
      "Epoch 1532 loss: 0.4799872934818268\n",
      "Epoch 1533 loss: 0.4799410104751587\n",
      "Epoch 1534 loss: 0.4798946678638458\n",
      "Epoch 1535 loss: 0.47984838485717773\n",
      "Epoch 1536 loss: 0.4798021614551544\n",
      "Epoch 1537 loss: 0.4797559082508087\n",
      "Epoch 1538 loss: 0.47970980405807495\n",
      "Epoch 1539 loss: 0.479663610458374\n",
      "Epoch 1540 loss: 0.47961750626564026\n",
      "Epoch 1541 loss: 0.4795713424682617\n",
      "Epoch 1542 loss: 0.4795253872871399\n",
      "Epoch 1543 loss: 0.4794793725013733\n",
      "Epoch 1544 loss: 0.47943341732025146\n",
      "Epoch 1545 loss: 0.47938743233680725\n",
      "Epoch 1546 loss: 0.4793415665626526\n",
      "Epoch 1547 loss: 0.47929567098617554\n",
      "Epoch 1548 loss: 0.47924989461898804\n",
      "Epoch 1549 loss: 0.4792040288448334\n",
      "Epoch 1550 loss: 0.4791582524776459\n",
      "Epoch 1551 loss: 0.47911256551742554\n",
      "Epoch 1552 loss: 0.4790668487548828\n",
      "Epoch 1553 loss: 0.47902119159698486\n",
      "Epoch 1554 loss: 0.4789755642414093\n",
      "Epoch 1555 loss: 0.4789299964904785\n",
      "Epoch 1556 loss: 0.47888442873954773\n",
      "Epoch 1557 loss: 0.47883889079093933\n",
      "Epoch 1558 loss: 0.47879335284233093\n",
      "Epoch 1559 loss: 0.4787479043006897\n",
      "Epoch 1560 loss: 0.47870248556137085\n",
      "Epoch 1561 loss: 0.4786570966243744\n",
      "Epoch 1562 loss: 0.4786117672920227\n",
      "Epoch 1563 loss: 0.47856637835502625\n",
      "Epoch 1564 loss: 0.47852107882499695\n",
      "Epoch 1565 loss: 0.4784758388996124\n",
      "Epoch 1566 loss: 0.4784306287765503\n",
      "Epoch 1567 loss: 0.47838541865348816\n",
      "Epoch 1568 loss: 0.4783402681350708\n",
      "Epoch 1569 loss: 0.47829514741897583\n",
      "Epoch 1570 loss: 0.47825002670288086\n",
      "Epoch 1571 loss: 0.47820496559143066\n",
      "Epoch 1572 loss: 0.47815996408462524\n",
      "Epoch 1573 loss: 0.47811493277549744\n",
      "Epoch 1574 loss: 0.4780700206756592\n",
      "Epoch 1575 loss: 0.47802504897117615\n",
      "Epoch 1576 loss: 0.4779801368713379\n",
      "Epoch 1577 loss: 0.477935254573822\n",
      "Epoch 1578 loss: 0.4778904616832733\n",
      "Epoch 1579 loss: 0.477845698595047\n",
      "Epoch 1580 loss: 0.4778009057044983\n",
      "Epoch 1581 loss: 0.477756142616272\n",
      "Epoch 1582 loss: 0.4777114987373352\n",
      "Epoch 1583 loss: 0.47766685485839844\n",
      "Epoch 1584 loss: 0.4776221811771393\n",
      "Epoch 1585 loss: 0.4775775969028473\n",
      "Epoch 1586 loss: 0.4775330126285553\n",
      "Epoch 1587 loss: 0.4774884879589081\n",
      "Epoch 1588 loss: 0.4774439334869385\n",
      "Epoch 1589 loss: 0.4773994982242584\n",
      "Epoch 1590 loss: 0.477355033159256\n",
      "Epoch 1591 loss: 0.4773106276988983\n",
      "Epoch 1592 loss: 0.47726625204086304\n",
      "Epoch 1593 loss: 0.47722193598747253\n",
      "Epoch 1594 loss: 0.4771776497364044\n",
      "Epoch 1595 loss: 0.4771333336830139\n",
      "Epoch 1596 loss: 0.4770890772342682\n",
      "Epoch 1597 loss: 0.47704488039016724\n",
      "Epoch 1598 loss: 0.4770006239414215\n",
      "Epoch 1599 loss: 0.4769565463066101\n",
      "Epoch 1600 loss: 0.47691240906715393\n",
      "Epoch 1601 loss: 0.47686830163002014\n",
      "Epoch 1602 loss: 0.47682422399520874\n",
      "Epoch 1603 loss: 0.4767802357673645\n",
      "Epoch 1604 loss: 0.4767362177371979\n",
      "Epoch 1605 loss: 0.4766922891139984\n",
      "Epoch 1606 loss: 0.47664833068847656\n",
      "Epoch 1607 loss: 0.47660449147224426\n",
      "Epoch 1608 loss: 0.4765605926513672\n",
      "Epoch 1609 loss: 0.4765167534351349\n",
      "Epoch 1610 loss: 0.4764729142189026\n",
      "Epoch 1611 loss: 0.47642916440963745\n",
      "Epoch 1612 loss: 0.4763854444026947\n",
      "Epoch 1613 loss: 0.47634175419807434\n",
      "Epoch 1614 loss: 0.476298063993454\n",
      "Epoch 1615 loss: 0.4762544631958008\n",
      "Epoch 1616 loss: 0.4762107729911804\n",
      "Epoch 1617 loss: 0.476167231798172\n",
      "Epoch 1618 loss: 0.4761236310005188\n",
      "Epoch 1619 loss: 0.47608011960983276\n",
      "Epoch 1620 loss: 0.4760366380214691\n",
      "Epoch 1621 loss: 0.47599321603775024\n",
      "Epoch 1622 loss: 0.4759497344493866\n",
      "Epoch 1623 loss: 0.4759063422679901\n",
      "Epoch 1624 loss: 0.4758630096912384\n",
      "Epoch 1625 loss: 0.4758196473121643\n",
      "Epoch 1626 loss: 0.4757763147354126\n",
      "Epoch 1627 loss: 0.47573307156562805\n",
      "Epoch 1628 loss: 0.4756897985935211\n",
      "Epoch 1629 loss: 0.4756466746330261\n",
      "Epoch 1630 loss: 0.47560346126556396\n",
      "Epoch 1631 loss: 0.4755602777004242\n",
      "Epoch 1632 loss: 0.4755171835422516\n",
      "Epoch 1633 loss: 0.4754740595817566\n",
      "Epoch 1634 loss: 0.47543102502822876\n",
      "Epoch 1635 loss: 0.4753879904747009\n",
      "Epoch 1636 loss: 0.4753449857234955\n",
      "Epoch 1637 loss: 0.4753020107746124\n",
      "Epoch 1638 loss: 0.47525909543037415\n",
      "Epoch 1639 loss: 0.4752161502838135\n",
      "Epoch 1640 loss: 0.47517329454421997\n",
      "Epoch 1641 loss: 0.47513043880462646\n",
      "Epoch 1642 loss: 0.47508761286735535\n",
      "Epoch 1643 loss: 0.4750448763370514\n",
      "Epoch 1644 loss: 0.47500211000442505\n",
      "Epoch 1645 loss: 0.4749594032764435\n",
      "Epoch 1646 loss: 0.4749166667461395\n",
      "Epoch 1647 loss: 0.47487398982048035\n",
      "Epoch 1648 loss: 0.47483140230178833\n",
      "Epoch 1649 loss: 0.47478875517845154\n",
      "Epoch 1650 loss: 0.4747461974620819\n",
      "Epoch 1651 loss: 0.4747036397457123\n",
      "Epoch 1652 loss: 0.4746612012386322\n",
      "Epoch 1653 loss: 0.4746186435222626\n",
      "Epoch 1654 loss: 0.4745761752128601\n",
      "Epoch 1655 loss: 0.4745337963104248\n",
      "Epoch 1656 loss: 0.4744914174079895\n",
      "Epoch 1657 loss: 0.4744490087032318\n",
      "Epoch 1658 loss: 0.4744066298007965\n",
      "Epoch 1659 loss: 0.474364310503006\n",
      "Epoch 1660 loss: 0.47432205080986023\n",
      "Epoch 1661 loss: 0.47427982091903687\n",
      "Epoch 1662 loss: 0.4742376506328583\n",
      "Epoch 1663 loss: 0.4741954207420349\n",
      "Epoch 1664 loss: 0.4741533100605011\n",
      "Epoch 1665 loss: 0.4741111397743225\n",
      "Epoch 1666 loss: 0.4740690290927887\n",
      "Epoch 1667 loss: 0.47402700781822205\n",
      "Epoch 1668 loss: 0.4739849269390106\n",
      "Epoch 1669 loss: 0.47394290566444397\n",
      "Epoch 1670 loss: 0.4739009141921997\n",
      "Epoch 1671 loss: 0.4738589823246002\n",
      "Epoch 1672 loss: 0.4738170802593231\n",
      "Epoch 1673 loss: 0.47377511858940125\n",
      "Epoch 1674 loss: 0.47373324632644653\n",
      "Epoch 1675 loss: 0.4736914336681366\n",
      "Epoch 1676 loss: 0.4736495912075043\n",
      "Epoch 1677 loss: 0.4736078679561615\n",
      "Epoch 1678 loss: 0.47356611490249634\n",
      "Epoch 1679 loss: 0.4735243618488312\n",
      "Epoch 1680 loss: 0.4734826683998108\n",
      "Epoch 1681 loss: 0.4734410345554352\n",
      "Epoch 1682 loss: 0.47339940071105957\n",
      "Epoch 1683 loss: 0.47335776686668396\n",
      "Epoch 1684 loss: 0.47331616282463074\n",
      "Epoch 1685 loss: 0.4732746481895447\n",
      "Epoch 1686 loss: 0.4732331335544586\n",
      "Epoch 1687 loss: 0.47319161891937256\n",
      "Epoch 1688 loss: 0.4731501340866089\n",
      "Epoch 1689 loss: 0.47310870885849\n",
      "Epoch 1690 loss: 0.4730673134326935\n",
      "Epoch 1691 loss: 0.47302597761154175\n",
      "Epoch 1692 loss: 0.47298458218574524\n",
      "Epoch 1693 loss: 0.4729432463645935\n",
      "Epoch 1694 loss: 0.4729019105434418\n",
      "Epoch 1695 loss: 0.472860723733902\n",
      "Epoch 1696 loss: 0.472819447517395\n",
      "Epoch 1697 loss: 0.47277823090553284\n",
      "Epoch 1698 loss: 0.47273707389831543\n",
      "Epoch 1699 loss: 0.472695916891098\n",
      "Epoch 1700 loss: 0.4726548194885254\n",
      "Epoch 1701 loss: 0.47261372208595276\n",
      "Epoch 1702 loss: 0.47257259488105774\n",
      "Epoch 1703 loss: 0.47253164649009705\n",
      "Epoch 1704 loss: 0.4724905490875244\n",
      "Epoch 1705 loss: 0.47244957089424133\n",
      "Epoch 1706 loss: 0.47240862250328064\n",
      "Epoch 1707 loss: 0.4723677635192871\n",
      "Epoch 1708 loss: 0.4723268151283264\n",
      "Epoch 1709 loss: 0.4722859859466553\n",
      "Epoch 1710 loss: 0.47224515676498413\n",
      "Epoch 1711 loss: 0.4722042679786682\n",
      "Epoch 1712 loss: 0.47216346859931946\n",
      "Epoch 1713 loss: 0.4721227288246155\n",
      "Epoch 1714 loss: 0.4720820188522339\n",
      "Epoch 1715 loss: 0.4720413088798523\n",
      "Epoch 1716 loss: 0.4720006585121155\n",
      "Epoch 1717 loss: 0.4719599485397339\n",
      "Epoch 1718 loss: 0.47191932797431946\n",
      "Epoch 1719 loss: 0.4718787372112274\n",
      "Epoch 1720 loss: 0.47183817625045776\n",
      "Epoch 1721 loss: 0.4717977046966553\n",
      "Epoch 1722 loss: 0.47175708413124084\n",
      "Epoch 1723 loss: 0.47171664237976074\n",
      "Epoch 1724 loss: 0.47167620062828064\n",
      "Epoch 1725 loss: 0.47163575887680054\n",
      "Epoch 1726 loss: 0.4715953469276428\n",
      "Epoch 1727 loss: 0.4715549647808075\n",
      "Epoch 1728 loss: 0.47151461243629456\n",
      "Epoch 1729 loss: 0.4714742600917816\n",
      "Epoch 1730 loss: 0.47143396735191345\n",
      "Epoch 1731 loss: 0.47139376401901245\n",
      "Epoch 1732 loss: 0.4713535010814667\n",
      "Epoch 1733 loss: 0.4713132977485657\n",
      "Epoch 1734 loss: 0.4712730646133423\n",
      "Epoch 1735 loss: 0.47123289108276367\n",
      "Epoch 1736 loss: 0.47119277715682983\n",
      "Epoch 1737 loss: 0.4711526334285736\n",
      "Epoch 1738 loss: 0.47111257910728455\n",
      "Epoch 1739 loss: 0.47107255458831787\n",
      "Epoch 1740 loss: 0.4710325300693512\n",
      "Epoch 1741 loss: 0.4709925055503845\n",
      "Epoch 1742 loss: 0.470952570438385\n",
      "Epoch 1743 loss: 0.4709126055240631\n",
      "Epoch 1744 loss: 0.470872700214386\n",
      "Epoch 1745 loss: 0.47083279490470886\n",
      "Epoch 1746 loss: 0.4707929491996765\n",
      "Epoch 1747 loss: 0.47075313329696655\n",
      "Epoch 1748 loss: 0.4707133173942566\n",
      "Epoch 1749 loss: 0.47067344188690186\n",
      "Epoch 1750 loss: 0.47063374519348145\n",
      "Epoch 1751 loss: 0.47059404850006104\n",
      "Epoch 1752 loss: 0.47055429220199585\n",
      "Epoch 1753 loss: 0.4705146253108978\n",
      "Epoch 1754 loss: 0.4704749584197998\n",
      "Epoch 1755 loss: 0.47043538093566895\n",
      "Epoch 1756 loss: 0.4703957140445709\n",
      "Epoch 1757 loss: 0.47035616636276245\n",
      "Epoch 1758 loss: 0.470316618680954\n",
      "Epoch 1759 loss: 0.4702771008014679\n",
      "Epoch 1760 loss: 0.4702375829219818\n",
      "Epoch 1761 loss: 0.4701980948448181\n",
      "Epoch 1762 loss: 0.4701586663722992\n",
      "Epoch 1763 loss: 0.4701192378997803\n",
      "Epoch 1764 loss: 0.4700798988342285\n",
      "Epoch 1765 loss: 0.47004052996635437\n",
      "Epoch 1766 loss: 0.47000113129615784\n",
      "Epoch 1767 loss: 0.46996185183525085\n",
      "Epoch 1768 loss: 0.46992260217666626\n",
      "Epoch 1769 loss: 0.46988338232040405\n",
      "Epoch 1770 loss: 0.4698440730571747\n",
      "Epoch 1771 loss: 0.4698048532009125\n",
      "Epoch 1772 loss: 0.46976566314697266\n",
      "Epoch 1773 loss: 0.4697265326976776\n",
      "Epoch 1774 loss: 0.4696873724460602\n",
      "Epoch 1775 loss: 0.4696483016014099\n",
      "Epoch 1776 loss: 0.46960917115211487\n",
      "Epoch 1777 loss: 0.469570130109787\n",
      "Epoch 1778 loss: 0.4695311188697815\n",
      "Epoch 1779 loss: 0.4694921672344208\n",
      "Epoch 1780 loss: 0.4694530963897705\n",
      "Epoch 1781 loss: 0.4694141745567322\n",
      "Epoch 1782 loss: 0.46937525272369385\n",
      "Epoch 1783 loss: 0.4693363606929779\n",
      "Epoch 1784 loss: 0.46929746866226196\n",
      "Epoch 1785 loss: 0.4692586660385132\n",
      "Epoch 1786 loss: 0.46921980381011963\n",
      "Epoch 1787 loss: 0.46918100118637085\n",
      "Epoch 1788 loss: 0.46914228796958923\n",
      "Epoch 1789 loss: 0.46910345554351807\n",
      "Epoch 1790 loss: 0.46906471252441406\n",
      "Epoch 1791 loss: 0.46902602910995483\n",
      "Epoch 1792 loss: 0.4689874053001404\n",
      "Epoch 1793 loss: 0.46894875168800354\n",
      "Epoch 1794 loss: 0.4689100980758667\n",
      "Epoch 1795 loss: 0.468871533870697\n",
      "Epoch 1796 loss: 0.46883296966552734\n",
      "Epoch 1797 loss: 0.46879440546035767\n",
      "Epoch 1798 loss: 0.46875590085983276\n",
      "Epoch 1799 loss: 0.4687173664569855\n",
      "Epoch 1800 loss: 0.46867892146110535\n",
      "Epoch 1801 loss: 0.4686404764652252\n",
      "Epoch 1802 loss: 0.4686020612716675\n",
      "Epoch 1803 loss: 0.46856358647346497\n",
      "Epoch 1804 loss: 0.4685252606868744\n",
      "Epoch 1805 loss: 0.4684869349002838\n",
      "Epoch 1806 loss: 0.468448668718338\n",
      "Epoch 1807 loss: 0.46841028332710266\n",
      "Epoch 1808 loss: 0.46837204694747925\n",
      "Epoch 1809 loss: 0.46833375096321106\n",
      "Epoch 1810 loss: 0.4682955741882324\n",
      "Epoch 1811 loss: 0.468257337808609\n",
      "Epoch 1812 loss: 0.46821919083595276\n",
      "Epoch 1813 loss: 0.4681810140609741\n",
      "Epoch 1814 loss: 0.46814295649528503\n",
      "Epoch 1815 loss: 0.46810483932495117\n",
      "Epoch 1816 loss: 0.4680667519569397\n",
      "Epoch 1817 loss: 0.4680286943912506\n",
      "Epoch 1818 loss: 0.4679906666278839\n",
      "Epoch 1819 loss: 0.4679526388645172\n",
      "Epoch 1820 loss: 0.4679146707057953\n",
      "Epoch 1821 loss: 0.46787676215171814\n",
      "Epoch 1822 loss: 0.4678387939929962\n",
      "Epoch 1823 loss: 0.46780091524124146\n",
      "Epoch 1824 loss: 0.4677630066871643\n",
      "Epoch 1825 loss: 0.4677252173423767\n",
      "Epoch 1826 loss: 0.46768733859062195\n",
      "Epoch 1827 loss: 0.46764951944351196\n",
      "Epoch 1828 loss: 0.46761175990104675\n",
      "Epoch 1829 loss: 0.46757400035858154\n",
      "Epoch 1830 loss: 0.46753624081611633\n",
      "Epoch 1831 loss: 0.4674986004829407\n",
      "Epoch 1832 loss: 0.4674608111381531\n",
      "Epoch 1833 loss: 0.4674231708049774\n",
      "Epoch 1834 loss: 0.46738553047180176\n",
      "Epoch 1835 loss: 0.4673479199409485\n",
      "Epoch 1836 loss: 0.46731036901474\n",
      "Epoch 1837 loss: 0.4672728180885315\n",
      "Epoch 1838 loss: 0.4672352373600006\n",
      "Epoch 1839 loss: 0.4671977460384369\n",
      "Epoch 1840 loss: 0.46716025471687317\n",
      "Epoch 1841 loss: 0.46712276339530945\n",
      "Epoch 1842 loss: 0.4670853316783905\n",
      "Epoch 1843 loss: 0.46704792976379395\n",
      "Epoch 1844 loss: 0.467010498046875\n",
      "Epoch 1845 loss: 0.46697306632995605\n",
      "Epoch 1846 loss: 0.46693578362464905\n",
      "Epoch 1847 loss: 0.4668983817100525\n",
      "Epoch 1848 loss: 0.46686112880706787\n",
      "Epoch 1849 loss: 0.46682387590408325\n",
      "Epoch 1850 loss: 0.46678656339645386\n",
      "Epoch 1851 loss: 0.46674931049346924\n",
      "Epoch 1852 loss: 0.46671217679977417\n",
      "Epoch 1853 loss: 0.46667495369911194\n",
      "Epoch 1854 loss: 0.4666377604007721\n",
      "Epoch 1855 loss: 0.4666006863117218\n",
      "Epoch 1856 loss: 0.46656355261802673\n",
      "Epoch 1857 loss: 0.46652647852897644\n",
      "Epoch 1858 loss: 0.46648940443992615\n",
      "Epoch 1859 loss: 0.46645233035087585\n",
      "Epoch 1860 loss: 0.46641531586647034\n",
      "Epoch 1861 loss: 0.4663783013820648\n",
      "Epoch 1862 loss: 0.4663413465023041\n",
      "Epoch 1863 loss: 0.46630436182022095\n",
      "Epoch 1864 loss: 0.466267466545105\n",
      "Epoch 1865 loss: 0.4662305414676666\n",
      "Epoch 1866 loss: 0.46619364619255066\n",
      "Epoch 1867 loss: 0.46615681052207947\n",
      "Epoch 1868 loss: 0.4661199152469635\n",
      "Epoch 1869 loss: 0.4660831689834595\n",
      "Epoch 1870 loss: 0.46604636311531067\n",
      "Epoch 1871 loss: 0.46600961685180664\n",
      "Epoch 1872 loss: 0.4659728705883026\n",
      "Epoch 1873 loss: 0.4659360945224762\n",
      "Epoch 1874 loss: 0.4658994674682617\n",
      "Epoch 1875 loss: 0.4658627510070801\n",
      "Epoch 1876 loss: 0.4658260941505432\n",
      "Epoch 1877 loss: 0.4657894968986511\n",
      "Epoch 1878 loss: 0.46575289964675903\n",
      "Epoch 1879 loss: 0.46571630239486694\n",
      "Epoch 1880 loss: 0.46567976474761963\n",
      "Epoch 1881 loss: 0.4656431972980499\n",
      "Epoch 1882 loss: 0.465606689453125\n",
      "Epoch 1883 loss: 0.4655701816082001\n",
      "Epoch 1884 loss: 0.4655337333679199\n",
      "Epoch 1885 loss: 0.46549728512763977\n",
      "Epoch 1886 loss: 0.4654608368873596\n",
      "Epoch 1887 loss: 0.46542447805404663\n",
      "Epoch 1888 loss: 0.46538808941841125\n",
      "Epoch 1889 loss: 0.4653517007827759\n",
      "Epoch 1890 loss: 0.46531540155410767\n",
      "Epoch 1891 loss: 0.46527907252311707\n",
      "Epoch 1892 loss: 0.46524277329444885\n",
      "Epoch 1893 loss: 0.465206503868103\n",
      "Epoch 1894 loss: 0.465170294046402\n",
      "Epoch 1895 loss: 0.46513405442237854\n",
      "Epoch 1896 loss: 0.4650978446006775\n",
      "Epoch 1897 loss: 0.46506166458129883\n",
      "Epoch 1898 loss: 0.46502554416656494\n",
      "Epoch 1899 loss: 0.4649893641471863\n",
      "Epoch 1900 loss: 0.46495333313941956\n",
      "Epoch 1901 loss: 0.46491721272468567\n",
      "Epoch 1902 loss: 0.46488112211227417\n",
      "Epoch 1903 loss: 0.46484506130218506\n",
      "Epoch 1904 loss: 0.4648090600967407\n",
      "Epoch 1905 loss: 0.4647730886936188\n",
      "Epoch 1906 loss: 0.4647371172904968\n",
      "Epoch 1907 loss: 0.4647011160850525\n",
      "Epoch 1908 loss: 0.4646652042865753\n",
      "Epoch 1909 loss: 0.46462929248809814\n",
      "Epoch 1910 loss: 0.46459341049194336\n",
      "Epoch 1911 loss: 0.4645574986934662\n",
      "Epoch 1912 loss: 0.46452173590660095\n",
      "Epoch 1913 loss: 0.46448585391044617\n",
      "Epoch 1914 loss: 0.46445009112358093\n",
      "Epoch 1915 loss: 0.4644142687320709\n",
      "Epoch 1916 loss: 0.4643785357475281\n",
      "Epoch 1917 loss: 0.46434277296066284\n",
      "Epoch 1918 loss: 0.46430712938308716\n",
      "Epoch 1919 loss: 0.4642713963985443\n",
      "Epoch 1920 loss: 0.46423572301864624\n",
      "Epoch 1921 loss: 0.46420010924339294\n",
      "Epoch 1922 loss: 0.4641645550727844\n",
      "Epoch 1923 loss: 0.46412888169288635\n",
      "Epoch 1924 loss: 0.46409329771995544\n",
      "Epoch 1925 loss: 0.4640577733516693\n",
      "Epoch 1926 loss: 0.4640222191810608\n",
      "Epoch 1927 loss: 0.46398669481277466\n",
      "Epoch 1928 loss: 0.4639512300491333\n",
      "Epoch 1929 loss: 0.46391573548316956\n",
      "Epoch 1930 loss: 0.4638803005218506\n",
      "Epoch 1931 loss: 0.46384483575820923\n",
      "Epoch 1932 loss: 0.46380946040153503\n",
      "Epoch 1933 loss: 0.46377405524253845\n",
      "Epoch 1934 loss: 0.46373867988586426\n",
      "Epoch 1935 loss: 0.4637033939361572\n",
      "Epoch 1936 loss: 0.4636681079864502\n",
      "Epoch 1937 loss: 0.463632732629776\n",
      "Epoch 1938 loss: 0.46359747648239136\n",
      "Epoch 1939 loss: 0.4635621905326843\n",
      "Epoch 1940 loss: 0.46352699398994446\n",
      "Epoch 1941 loss: 0.4634917676448822\n",
      "Epoch 1942 loss: 0.46345657110214233\n",
      "Epoch 1943 loss: 0.4634213447570801\n",
      "Epoch 1944 loss: 0.46338626742362976\n",
      "Epoch 1945 loss: 0.4633511006832123\n",
      "Epoch 1946 loss: 0.46331602334976196\n",
      "Epoch 1947 loss: 0.46328091621398926\n",
      "Epoch 1948 loss: 0.46324586868286133\n",
      "Epoch 1949 loss: 0.4632108211517334\n",
      "Epoch 1950 loss: 0.46317583322525024\n",
      "Epoch 1951 loss: 0.4631407856941223\n",
      "Epoch 1952 loss: 0.46310585737228394\n",
      "Epoch 1953 loss: 0.4630708694458008\n",
      "Epoch 1954 loss: 0.4630359411239624\n",
      "Epoch 1955 loss: 0.4630010426044464\n",
      "Epoch 1956 loss: 0.46296608448028564\n",
      "Epoch 1957 loss: 0.46293121576309204\n",
      "Epoch 1958 loss: 0.4628963768482208\n",
      "Epoch 1959 loss: 0.4628615975379944\n",
      "Epoch 1960 loss: 0.4628267288208008\n",
      "Epoch 1961 loss: 0.46279194951057434\n",
      "Epoch 1962 loss: 0.4627572000026703\n",
      "Epoch 1963 loss: 0.46272242069244385\n",
      "Epoch 1964 loss: 0.46268773078918457\n",
      "Epoch 1965 loss: 0.4626530110836029\n",
      "Epoch 1966 loss: 0.462618350982666\n",
      "Epoch 1967 loss: 0.46258366107940674\n",
      "Epoch 1968 loss: 0.46254903078079224\n",
      "Epoch 1969 loss: 0.46251437067985535\n",
      "Epoch 1970 loss: 0.4624797999858856\n",
      "Epoch 1971 loss: 0.4624451994895935\n",
      "Epoch 1972 loss: 0.4624106287956238\n",
      "Epoch 1973 loss: 0.46237611770629883\n",
      "Epoch 1974 loss: 0.4623415470123291\n",
      "Epoch 1975 loss: 0.46230703592300415\n",
      "Epoch 1976 loss: 0.46227261424064636\n",
      "Epoch 1977 loss: 0.4622381627559662\n",
      "Epoch 1978 loss: 0.462203711271286\n",
      "Epoch 1979 loss: 0.4621692895889282\n",
      "Epoch 1980 loss: 0.4621349275112152\n",
      "Epoch 1981 loss: 0.4621005058288574\n",
      "Epoch 1982 loss: 0.4620662033557892\n",
      "Epoch 1983 loss: 0.4620318114757538\n",
      "Epoch 1984 loss: 0.46199747920036316\n",
      "Epoch 1985 loss: 0.4619631767272949\n",
      "Epoch 1986 loss: 0.46192893385887146\n",
      "Epoch 1987 loss: 0.4618946611881256\n",
      "Epoch 1988 loss: 0.4618604779243469\n",
      "Epoch 1989 loss: 0.46182623505592346\n",
      "Epoch 1990 loss: 0.46179208159446716\n",
      "Epoch 1991 loss: 0.4617578387260437\n",
      "Epoch 1992 loss: 0.4617236852645874\n",
      "Epoch 1993 loss: 0.46168962121009827\n",
      "Epoch 1994 loss: 0.46165546774864197\n",
      "Epoch 1995 loss: 0.46162134408950806\n",
      "Epoch 1996 loss: 0.4615873098373413\n",
      "Epoch 1997 loss: 0.46155330538749695\n",
      "Epoch 1998 loss: 0.4615192115306854\n",
      "Epoch 1999 loss: 0.46148520708084106\n",
      "Epoch 2000 loss: 0.4614512026309967\n",
      "Epoch 2001 loss: 0.4614172577857971\n",
      "Epoch 2002 loss: 0.46138325333595276\n",
      "Epoch 2003 loss: 0.46134936809539795\n",
      "Epoch 2004 loss: 0.46131545305252075\n",
      "Epoch 2005 loss: 0.46128153800964355\n",
      "Epoch 2006 loss: 0.4612477123737335\n",
      "Epoch 2007 loss: 0.4612138569355011\n",
      "Epoch 2008 loss: 0.4611800014972687\n",
      "Epoch 2009 loss: 0.46114620566368103\n",
      "Epoch 2010 loss: 0.4611124098300934\n",
      "Epoch 2011 loss: 0.4610786736011505\n",
      "Epoch 2012 loss: 0.46104487776756287\n",
      "Epoch 2013 loss: 0.46101120114326477\n",
      "Epoch 2014 loss: 0.4609774053096771\n",
      "Epoch 2015 loss: 0.46094369888305664\n",
      "Epoch 2016 loss: 0.46091005206108093\n",
      "Epoch 2017 loss: 0.46087634563446045\n",
      "Epoch 2018 loss: 0.4608428180217743\n",
      "Epoch 2019 loss: 0.46080923080444336\n",
      "Epoch 2020 loss: 0.46077558398246765\n",
      "Epoch 2021 loss: 0.46074196696281433\n",
      "Epoch 2022 loss: 0.46070846915245056\n",
      "Epoch 2023 loss: 0.460674911737442\n",
      "Epoch 2024 loss: 0.46064138412475586\n",
      "Epoch 2025 loss: 0.4606078565120697\n",
      "Epoch 2026 loss: 0.4605744183063507\n",
      "Epoch 2027 loss: 0.4605409502983093\n",
      "Epoch 2028 loss: 0.4605075418949127\n",
      "Epoch 2029 loss: 0.4604741036891937\n",
      "Epoch 2030 loss: 0.4604406952857971\n",
      "Epoch 2031 loss: 0.4604073464870453\n",
      "Epoch 2032 loss: 0.46037399768829346\n",
      "Epoch 2033 loss: 0.4603406488895416\n",
      "Epoch 2034 loss: 0.4603073298931122\n",
      "Epoch 2035 loss: 0.46027401089668274\n",
      "Epoch 2036 loss: 0.4602406919002533\n",
      "Epoch 2037 loss: 0.460207462310791\n",
      "Epoch 2038 loss: 0.46017420291900635\n",
      "Epoch 2039 loss: 0.46014097332954407\n",
      "Epoch 2040 loss: 0.46010780334472656\n",
      "Epoch 2041 loss: 0.46007466316223145\n",
      "Epoch 2042 loss: 0.46004146337509155\n",
      "Epoch 2043 loss: 0.46000829339027405\n",
      "Epoch 2044 loss: 0.45997515320777893\n",
      "Epoch 2045 loss: 0.4599420428276062\n",
      "Epoch 2046 loss: 0.45990896224975586\n",
      "Epoch 2047 loss: 0.4598759412765503\n",
      "Epoch 2048 loss: 0.45984283089637756\n",
      "Epoch 2049 loss: 0.459809809923172\n",
      "Epoch 2050 loss: 0.45977678894996643\n",
      "Epoch 2051 loss: 0.45974379777908325\n",
      "Epoch 2052 loss: 0.45971086621284485\n",
      "Epoch 2053 loss: 0.45967787504196167\n",
      "Epoch 2054 loss: 0.45964494347572327\n",
      "Epoch 2055 loss: 0.45961201190948486\n",
      "Epoch 2056 loss: 0.45957911014556885\n",
      "Epoch 2057 loss: 0.4595462381839752\n",
      "Epoch 2058 loss: 0.4595133662223816\n",
      "Epoch 2059 loss: 0.45948055386543274\n",
      "Epoch 2060 loss: 0.4594477117061615\n",
      "Epoch 2061 loss: 0.45941492915153503\n",
      "Epoch 2062 loss: 0.45938214659690857\n",
      "Epoch 2063 loss: 0.4593493938446045\n",
      "Epoch 2064 loss: 0.459316611289978\n",
      "Epoch 2065 loss: 0.45928388833999634\n",
      "Epoch 2066 loss: 0.45925116539001465\n",
      "Epoch 2067 loss: 0.45921847224235535\n",
      "Epoch 2068 loss: 0.45918577909469604\n",
      "Epoch 2069 loss: 0.4591531455516815\n",
      "Epoch 2070 loss: 0.459120512008667\n",
      "Epoch 2071 loss: 0.4590878486633301\n",
      "Epoch 2072 loss: 0.4590552747249603\n",
      "Epoch 2073 loss: 0.4590227007865906\n",
      "Epoch 2074 loss: 0.4589901268482208\n",
      "Epoch 2075 loss: 0.45895758271217346\n",
      "Epoch 2076 loss: 0.4589250683784485\n",
      "Epoch 2077 loss: 0.4588925540447235\n",
      "Epoch 2078 loss: 0.4588600695133209\n",
      "Epoch 2079 loss: 0.4588276147842407\n",
      "Epoch 2080 loss: 0.4587952196598053\n",
      "Epoch 2081 loss: 0.4587627649307251\n",
      "Epoch 2082 loss: 0.4587303102016449\n",
      "Epoch 2083 loss: 0.45869794487953186\n",
      "Epoch 2084 loss: 0.45866554975509644\n",
      "Epoch 2085 loss: 0.4586332440376282\n",
      "Epoch 2086 loss: 0.45860084891319275\n",
      "Epoch 2087 loss: 0.4585685431957245\n",
      "Epoch 2088 loss: 0.4585362374782562\n",
      "Epoch 2089 loss: 0.45850396156311035\n",
      "Epoch 2090 loss: 0.4584716856479645\n",
      "Epoch 2091 loss: 0.458439439535141\n",
      "Epoch 2092 loss: 0.4584071934223175\n",
      "Epoch 2093 loss: 0.4583750069141388\n",
      "Epoch 2094 loss: 0.4583428204059601\n",
      "Epoch 2095 loss: 0.458310604095459\n",
      "Epoch 2096 loss: 0.45827847719192505\n",
      "Epoch 2097 loss: 0.4582463204860687\n",
      "Epoch 2098 loss: 0.45821425318717957\n",
      "Epoch 2099 loss: 0.45818209648132324\n",
      "Epoch 2100 loss: 0.45815005898475647\n",
      "Epoch 2101 loss: 0.4581179618835449\n",
      "Epoch 2102 loss: 0.45808595418930054\n",
      "Epoch 2103 loss: 0.4580538868904114\n",
      "Epoch 2104 loss: 0.4580218493938446\n",
      "Epoch 2105 loss: 0.4579898715019226\n",
      "Epoch 2106 loss: 0.4579578936100006\n",
      "Epoch 2107 loss: 0.4579259157180786\n",
      "Epoch 2108 loss: 0.4578940272331238\n",
      "Epoch 2109 loss: 0.45786213874816895\n",
      "Epoch 2110 loss: 0.45783019065856934\n",
      "Epoch 2111 loss: 0.4577983021736145\n",
      "Epoch 2112 loss: 0.45776644349098206\n",
      "Epoch 2113 loss: 0.457734614610672\n",
      "Epoch 2114 loss: 0.45770278573036194\n",
      "Epoch 2115 loss: 0.4576709270477295\n",
      "Epoch 2116 loss: 0.45763909816741943\n",
      "Epoch 2117 loss: 0.4576074182987213\n",
      "Epoch 2118 loss: 0.45757561922073364\n",
      "Epoch 2119 loss: 0.45754384994506836\n",
      "Epoch 2120 loss: 0.45751214027404785\n",
      "Epoch 2121 loss: 0.45748043060302734\n",
      "Epoch 2122 loss: 0.4574487507343292\n",
      "Epoch 2123 loss: 0.4574170410633087\n",
      "Epoch 2124 loss: 0.45738542079925537\n",
      "Epoch 2125 loss: 0.45735374093055725\n",
      "Epoch 2126 loss: 0.4573221504688263\n",
      "Epoch 2127 loss: 0.45729053020477295\n",
      "Epoch 2128 loss: 0.4572589695453644\n",
      "Epoch 2129 loss: 0.4572274684906006\n",
      "Epoch 2130 loss: 0.45719587802886963\n",
      "Epoch 2131 loss: 0.45716431736946106\n",
      "Epoch 2132 loss: 0.45713284611701965\n",
      "Epoch 2133 loss: 0.4571012854576111\n",
      "Epoch 2134 loss: 0.45706984400749207\n",
      "Epoch 2135 loss: 0.45703843235969543\n",
      "Epoch 2136 loss: 0.45700693130493164\n",
      "Epoch 2137 loss: 0.4569755494594574\n",
      "Epoch 2138 loss: 0.45694413781166077\n",
      "Epoch 2139 loss: 0.4569127857685089\n",
      "Epoch 2140 loss: 0.4568813443183899\n",
      "Epoch 2141 loss: 0.4568500220775604\n",
      "Epoch 2142 loss: 0.45681867003440857\n",
      "Epoch 2143 loss: 0.4567872881889343\n",
      "Epoch 2144 loss: 0.45675596594810486\n",
      "Epoch 2145 loss: 0.45672476291656494\n",
      "Epoch 2146 loss: 0.45669350028038025\n",
      "Epoch 2147 loss: 0.45666223764419556\n",
      "Epoch 2148 loss: 0.45663100481033325\n",
      "Epoch 2149 loss: 0.45659977197647095\n",
      "Epoch 2150 loss: 0.45656856894493103\n",
      "Epoch 2151 loss: 0.4565373957157135\n",
      "Epoch 2152 loss: 0.45650625228881836\n",
      "Epoch 2153 loss: 0.4564751088619232\n",
      "Epoch 2154 loss: 0.4564439654350281\n",
      "Epoch 2155 loss: 0.4564128518104553\n",
      "Epoch 2156 loss: 0.45638179779052734\n",
      "Epoch 2157 loss: 0.45635074377059937\n",
      "Epoch 2158 loss: 0.4563196003437042\n",
      "Epoch 2159 loss: 0.45628854632377625\n",
      "Epoch 2160 loss: 0.45625755190849304\n",
      "Epoch 2161 loss: 0.45622652769088745\n",
      "Epoch 2162 loss: 0.456195592880249\n",
      "Epoch 2163 loss: 0.45616456866264343\n",
      "Epoch 2164 loss: 0.456133633852005\n",
      "Epoch 2165 loss: 0.4561026990413666\n",
      "Epoch 2166 loss: 0.45607179403305054\n",
      "Epoch 2167 loss: 0.45604079961776733\n",
      "Epoch 2168 loss: 0.45600998401641846\n",
      "Epoch 2169 loss: 0.4559790790081024\n",
      "Epoch 2170 loss: 0.45594823360443115\n",
      "Epoch 2171 loss: 0.4559173583984375\n",
      "Epoch 2172 loss: 0.4558865427970886\n",
      "Epoch 2173 loss: 0.45585575699806213\n",
      "Epoch 2174 loss: 0.45582500100135803\n",
      "Epoch 2175 loss: 0.4557941257953644\n",
      "Epoch 2176 loss: 0.45576345920562744\n",
      "Epoch 2177 loss: 0.4557327330112457\n",
      "Epoch 2178 loss: 0.4557019770145416\n",
      "Epoch 2179 loss: 0.4556713104248047\n",
      "Epoch 2180 loss: 0.455640584230423\n",
      "Epoch 2181 loss: 0.45560988783836365\n",
      "Epoch 2182 loss: 0.4555792212486267\n",
      "Epoch 2183 loss: 0.45554858446121216\n",
      "Epoch 2184 loss: 0.45551797747612\n",
      "Epoch 2185 loss: 0.45548734068870544\n",
      "Epoch 2186 loss: 0.45545679330825806\n",
      "Epoch 2187 loss: 0.45542624592781067\n",
      "Epoch 2188 loss: 0.4553956985473633\n",
      "Epoch 2189 loss: 0.4553651213645935\n",
      "Epoch 2190 loss: 0.4553346633911133\n",
      "Epoch 2191 loss: 0.4553040862083435\n",
      "Epoch 2192 loss: 0.4552736282348633\n",
      "Epoch 2193 loss: 0.4552431106567383\n",
      "Epoch 2194 loss: 0.45521268248558044\n",
      "Epoch 2195 loss: 0.4551822245121002\n",
      "Epoch 2196 loss: 0.45515182614326477\n",
      "Epoch 2197 loss: 0.45512136816978455\n",
      "Epoch 2198 loss: 0.45509105920791626\n",
      "Epoch 2199 loss: 0.4550606310367584\n",
      "Epoch 2200 loss: 0.45503032207489014\n",
      "Epoch 2201 loss: 0.45499998331069946\n",
      "Epoch 2202 loss: 0.4549696147441864\n",
      "Epoch 2203 loss: 0.4549393355846405\n",
      "Epoch 2204 loss: 0.4549090266227722\n",
      "Epoch 2205 loss: 0.4548788070678711\n",
      "Epoch 2206 loss: 0.4548485577106476\n",
      "Epoch 2207 loss: 0.4548183083534241\n",
      "Epoch 2208 loss: 0.45478805899620056\n",
      "Epoch 2209 loss: 0.45475783944129944\n",
      "Epoch 2210 loss: 0.4547276198863983\n",
      "Epoch 2211 loss: 0.45469745993614197\n",
      "Epoch 2212 loss: 0.45466724038124084\n",
      "Epoch 2213 loss: 0.4546371400356293\n",
      "Epoch 2214 loss: 0.4546070098876953\n",
      "Epoch 2215 loss: 0.4545769691467285\n",
      "Epoch 2216 loss: 0.45454683899879456\n",
      "Epoch 2217 loss: 0.454516738653183\n",
      "Epoch 2218 loss: 0.4544866681098938\n",
      "Epoch 2219 loss: 0.4544566571712494\n",
      "Epoch 2220 loss: 0.4544265866279602\n",
      "Epoch 2221 loss: 0.4543966054916382\n",
      "Epoch 2222 loss: 0.4543665647506714\n",
      "Epoch 2223 loss: 0.45433664321899414\n",
      "Epoch 2224 loss: 0.4543066620826721\n",
      "Epoch 2225 loss: 0.4542766809463501\n",
      "Epoch 2226 loss: 0.45424675941467285\n",
      "Epoch 2227 loss: 0.4542168378829956\n",
      "Epoch 2228 loss: 0.45418697595596313\n",
      "Epoch 2229 loss: 0.45415711402893066\n",
      "Epoch 2230 loss: 0.45412716269493103\n",
      "Epoch 2231 loss: 0.45409733057022095\n",
      "Epoch 2232 loss: 0.45406752824783325\n",
      "Epoch 2233 loss: 0.45403772592544556\n",
      "Epoch 2234 loss: 0.4540078938007355\n",
      "Epoch 2235 loss: 0.45397812128067017\n",
      "Epoch 2236 loss: 0.4539482891559601\n",
      "Epoch 2237 loss: 0.45391854643821716\n",
      "Epoch 2238 loss: 0.45388883352279663\n",
      "Epoch 2239 loss: 0.4538591206073761\n",
      "Epoch 2240 loss: 0.4538293480873108\n",
      "Epoch 2241 loss: 0.45379969477653503\n",
      "Epoch 2242 loss: 0.4537700116634369\n",
      "Epoch 2243 loss: 0.45374032855033875\n",
      "Epoch 2244 loss: 0.45371073484420776\n",
      "Epoch 2245 loss: 0.4536811113357544\n",
      "Epoch 2246 loss: 0.45365145802497864\n",
      "Epoch 2247 loss: 0.45362186431884766\n",
      "Epoch 2248 loss: 0.4535922706127167\n",
      "Epoch 2249 loss: 0.4535626769065857\n",
      "Epoch 2250 loss: 0.45353320240974426\n",
      "Epoch 2251 loss: 0.45350363850593567\n",
      "Epoch 2252 loss: 0.45347416400909424\n",
      "Epoch 2253 loss: 0.45344460010528564\n",
      "Epoch 2254 loss: 0.4534150958061218\n",
      "Epoch 2255 loss: 0.4533856213092804\n",
      "Epoch 2256 loss: 0.45335617661476135\n",
      "Epoch 2257 loss: 0.4533267319202423\n",
      "Epoch 2258 loss: 0.45329734683036804\n",
      "Epoch 2259 loss: 0.453267902135849\n",
      "Epoch 2260 loss: 0.45323851704597473\n",
      "Epoch 2261 loss: 0.45320916175842285\n",
      "Epoch 2262 loss: 0.4531797468662262\n",
      "Epoch 2263 loss: 0.4531504213809967\n",
      "Epoch 2264 loss: 0.4531210958957672\n",
      "Epoch 2265 loss: 0.4530917704105377\n",
      "Epoch 2266 loss: 0.4530624449253082\n",
      "Epoch 2267 loss: 0.4530331790447235\n",
      "Epoch 2268 loss: 0.4530038833618164\n",
      "Epoch 2269 loss: 0.45297467708587646\n",
      "Epoch 2270 loss: 0.45294538140296936\n",
      "Epoch 2271 loss: 0.4529161751270294\n",
      "Epoch 2272 loss: 0.4528869688510895\n",
      "Epoch 2273 loss: 0.4528577923774719\n",
      "Epoch 2274 loss: 0.45282861590385437\n",
      "Epoch 2275 loss: 0.4527994394302368\n",
      "Epoch 2276 loss: 0.45277029275894165\n",
      "Epoch 2277 loss: 0.4527411460876465\n",
      "Epoch 2278 loss: 0.45271196961402893\n",
      "Epoch 2279 loss: 0.4526829421520233\n",
      "Epoch 2280 loss: 0.45265382528305054\n",
      "Epoch 2281 loss: 0.45262476801872253\n",
      "Epoch 2282 loss: 0.45259571075439453\n",
      "Epoch 2283 loss: 0.45256662368774414\n",
      "Epoch 2284 loss: 0.4525376260280609\n",
      "Epoch 2285 loss: 0.4525086283683777\n",
      "Epoch 2286 loss: 0.45247963070869446\n",
      "Epoch 2287 loss: 0.45245063304901123\n",
      "Epoch 2288 loss: 0.45242172479629517\n",
      "Epoch 2289 loss: 0.45239272713661194\n",
      "Epoch 2290 loss: 0.4523638188838959\n",
      "Epoch 2291 loss: 0.4523349106311798\n",
      "Epoch 2292 loss: 0.45230597257614136\n",
      "Epoch 2293 loss: 0.4522770941257477\n",
      "Epoch 2294 loss: 0.4522482454776764\n",
      "Epoch 2295 loss: 0.4522193670272827\n",
      "Epoch 2296 loss: 0.4521905481815338\n",
      "Epoch 2297 loss: 0.45216166973114014\n",
      "Epoch 2298 loss: 0.4521328806877136\n",
      "Epoch 2299 loss: 0.45210403203964233\n",
      "Epoch 2300 loss: 0.4520752727985382\n",
      "Epoch 2301 loss: 0.45204654335975647\n",
      "Epoch 2302 loss: 0.45201778411865234\n",
      "Epoch 2303 loss: 0.4519890248775482\n",
      "Epoch 2304 loss: 0.4519602656364441\n",
      "Epoch 2305 loss: 0.45193156599998474\n",
      "Epoch 2306 loss: 0.4519028663635254\n",
      "Epoch 2307 loss: 0.4518741965293884\n",
      "Epoch 2308 loss: 0.45184555649757385\n",
      "Epoch 2309 loss: 0.4518169164657593\n",
      "Epoch 2310 loss: 0.4517882764339447\n",
      "Epoch 2311 loss: 0.4517596960067749\n",
      "Epoch 2312 loss: 0.4517310559749603\n",
      "Epoch 2313 loss: 0.4517024755477905\n",
      "Epoch 2314 loss: 0.45167383551597595\n",
      "Epoch 2315 loss: 0.4516453146934509\n",
      "Epoch 2316 loss: 0.4516167640686035\n",
      "Epoch 2317 loss: 0.4515882432460785\n",
      "Epoch 2318 loss: 0.45155972242355347\n",
      "Epoch 2319 loss: 0.4515312612056732\n",
      "Epoch 2320 loss: 0.4515027701854706\n",
      "Epoch 2321 loss: 0.45147424936294556\n",
      "Epoch 2322 loss: 0.4514458179473877\n",
      "Epoch 2323 loss: 0.45141735672950745\n",
      "Epoch 2324 loss: 0.451388955116272\n",
      "Epoch 2325 loss: 0.4513605535030365\n",
      "Epoch 2326 loss: 0.45133212208747864\n",
      "Epoch 2327 loss: 0.45130372047424316\n",
      "Epoch 2328 loss: 0.45127537846565247\n",
      "Epoch 2329 loss: 0.4512470066547394\n",
      "Epoch 2330 loss: 0.4512186348438263\n",
      "Epoch 2331 loss: 0.45119035243988037\n",
      "Epoch 2332 loss: 0.45116207003593445\n",
      "Epoch 2333 loss: 0.45113372802734375\n",
      "Epoch 2334 loss: 0.4511054754257202\n",
      "Epoch 2335 loss: 0.4510771930217743\n",
      "Epoch 2336 loss: 0.45104897022247314\n",
      "Epoch 2337 loss: 0.4510207176208496\n",
      "Epoch 2338 loss: 0.45099252462387085\n",
      "Epoch 2339 loss: 0.4509643018245697\n",
      "Epoch 2340 loss: 0.45093607902526855\n",
      "Epoch 2341 loss: 0.4509079158306122\n",
      "Epoch 2342 loss: 0.4508797824382782\n",
      "Epoch 2343 loss: 0.45085158944129944\n",
      "Epoch 2344 loss: 0.45082345604896545\n",
      "Epoch 2345 loss: 0.45079532265663147\n",
      "Epoch 2346 loss: 0.4507671594619751\n",
      "Epoch 2347 loss: 0.4507391154766083\n",
      "Epoch 2348 loss: 0.45071107149124146\n",
      "Epoch 2349 loss: 0.45068302750587463\n",
      "Epoch 2350 loss: 0.45065492391586304\n",
      "Epoch 2351 loss: 0.450626939535141\n",
      "Epoch 2352 loss: 0.45059895515441895\n",
      "Epoch 2353 loss: 0.45057088136672974\n",
      "Epoch 2354 loss: 0.4505428969860077\n",
      "Epoch 2355 loss: 0.45051494240760803\n",
      "Epoch 2356 loss: 0.450486958026886\n",
      "Epoch 2357 loss: 0.4504590332508087\n",
      "Epoch 2358 loss: 0.4504310190677643\n",
      "Epoch 2359 loss: 0.4504031538963318\n",
      "Epoch 2360 loss: 0.4503752291202545\n",
      "Epoch 2361 loss: 0.450347363948822\n",
      "Epoch 2362 loss: 0.45031943917274475\n",
      "Epoch 2363 loss: 0.45029157400131226\n",
      "Epoch 2364 loss: 0.45026373863220215\n",
      "Epoch 2365 loss: 0.45023587346076965\n",
      "Epoch 2366 loss: 0.4502081274986267\n",
      "Epoch 2367 loss: 0.4501802921295166\n",
      "Epoch 2368 loss: 0.45015251636505127\n",
      "Epoch 2369 loss: 0.45012474060058594\n",
      "Epoch 2370 loss: 0.4500969350337982\n",
      "Epoch 2371 loss: 0.4500691890716553\n",
      "Epoch 2372 loss: 0.4500414729118347\n",
      "Epoch 2373 loss: 0.4500136971473694\n",
      "Epoch 2374 loss: 0.44998598098754883\n",
      "Epoch 2375 loss: 0.44995829463005066\n",
      "Epoch 2376 loss: 0.4499306082725525\n",
      "Epoch 2377 loss: 0.4499029815196991\n",
      "Epoch 2378 loss: 0.4498753249645233\n",
      "Epoch 2379 loss: 0.4498476982116699\n",
      "Epoch 2380 loss: 0.44982004165649414\n",
      "Epoch 2381 loss: 0.44979244470596313\n",
      "Epoch 2382 loss: 0.44976484775543213\n",
      "Epoch 2383 loss: 0.4497372806072235\n",
      "Epoch 2384 loss: 0.4497097134590149\n",
      "Epoch 2385 loss: 0.4496821165084839\n",
      "Epoch 2386 loss: 0.44965457916259766\n",
      "Epoch 2387 loss: 0.4496270716190338\n",
      "Epoch 2388 loss: 0.4495995342731476\n",
      "Epoch 2389 loss: 0.4495720863342285\n",
      "Epoch 2390 loss: 0.44954463839530945\n",
      "Epoch 2391 loss: 0.44951707124710083\n",
      "Epoch 2392 loss: 0.44948968291282654\n",
      "Epoch 2393 loss: 0.4494622051715851\n",
      "Epoch 2394 loss: 0.449434757232666\n",
      "Epoch 2395 loss: 0.4494073688983917\n",
      "Epoch 2396 loss: 0.44937992095947266\n",
      "Epoch 2397 loss: 0.44935259222984314\n",
      "Epoch 2398 loss: 0.4493251442909241\n",
      "Epoch 2399 loss: 0.44929784536361694\n",
      "Epoch 2400 loss: 0.44927042722702026\n",
      "Epoch 2401 loss: 0.44924309849739075\n",
      "Epoch 2402 loss: 0.449215829372406\n",
      "Epoch 2403 loss: 0.4491885304450989\n",
      "Epoch 2404 loss: 0.44916120171546936\n",
      "Epoch 2405 loss: 0.44913390278816223\n",
      "Epoch 2406 loss: 0.44910669326782227\n",
      "Epoch 2407 loss: 0.4490794241428375\n",
      "Epoch 2408 loss: 0.44905221462249756\n",
      "Epoch 2409 loss: 0.4490249752998352\n",
      "Epoch 2410 loss: 0.44899776577949524\n",
      "Epoch 2411 loss: 0.4489705562591553\n",
      "Epoch 2412 loss: 0.4489433169364929\n",
      "Epoch 2413 loss: 0.44891616702079773\n",
      "Epoch 2414 loss: 0.4488890469074249\n",
      "Epoch 2415 loss: 0.44886189699172974\n",
      "Epoch 2416 loss: 0.44883474707603455\n",
      "Epoch 2417 loss: 0.4488076865673065\n",
      "Epoch 2418 loss: 0.4487805664539337\n",
      "Epoch 2419 loss: 0.4487534463405609\n",
      "Epoch 2420 loss: 0.44872644543647766\n",
      "Epoch 2421 loss: 0.44869932532310486\n",
      "Epoch 2422 loss: 0.44867226481437683\n",
      "Epoch 2423 loss: 0.4486452341079712\n",
      "Epoch 2424 loss: 0.4486182630062103\n",
      "Epoch 2425 loss: 0.4485911726951599\n",
      "Epoch 2426 loss: 0.44856423139572144\n",
      "Epoch 2427 loss: 0.4485372304916382\n",
      "Epoch 2428 loss: 0.4485102593898773\n",
      "Epoch 2429 loss: 0.44848328828811646\n",
      "Epoch 2430 loss: 0.44845637679100037\n",
      "Epoch 2431 loss: 0.4484294354915619\n",
      "Epoch 2432 loss: 0.4484025239944458\n",
      "Epoch 2433 loss: 0.4483756422996521\n",
      "Epoch 2434 loss: 0.4483487606048584\n",
      "Epoch 2435 loss: 0.4483218491077423\n",
      "Epoch 2436 loss: 0.4482950270175934\n",
      "Epoch 2437 loss: 0.44826817512512207\n",
      "Epoch 2438 loss: 0.44824135303497314\n",
      "Epoch 2439 loss: 0.44821450114250183\n",
      "Epoch 2440 loss: 0.4481877386569977\n",
      "Epoch 2441 loss: 0.44816091656684875\n",
      "Epoch 2442 loss: 0.4481341540813446\n",
      "Epoch 2443 loss: 0.44810739159584045\n",
      "Epoch 2444 loss: 0.4480806589126587\n",
      "Epoch 2445 loss: 0.44805386662483215\n",
      "Epoch 2446 loss: 0.4480271339416504\n",
      "Epoch 2447 loss: 0.448000431060791\n",
      "Epoch 2448 loss: 0.4479737877845764\n",
      "Epoch 2449 loss: 0.44794705510139465\n",
      "Epoch 2450 loss: 0.44792041182518005\n",
      "Epoch 2451 loss: 0.44789373874664307\n",
      "Epoch 2452 loss: 0.4478670358657837\n",
      "Epoch 2453 loss: 0.4478404223918915\n",
      "Epoch 2454 loss: 0.44781383872032166\n",
      "Epoch 2455 loss: 0.44778719544410706\n",
      "Epoch 2456 loss: 0.44776058197021484\n",
      "Epoch 2457 loss: 0.4477340579032898\n",
      "Epoch 2458 loss: 0.4477074146270752\n",
      "Epoch 2459 loss: 0.44768086075782776\n",
      "Epoch 2460 loss: 0.4476543664932251\n",
      "Epoch 2461 loss: 0.44762784242630005\n",
      "Epoch 2462 loss: 0.4476013481616974\n",
      "Epoch 2463 loss: 0.44757482409477234\n",
      "Epoch 2464 loss: 0.44754835963249207\n",
      "Epoch 2465 loss: 0.447521835565567\n",
      "Epoch 2466 loss: 0.44749540090560913\n",
      "Epoch 2467 loss: 0.4474688470363617\n",
      "Epoch 2468 loss: 0.4474424421787262\n",
      "Epoch 2469 loss: 0.4474160969257355\n",
      "Epoch 2470 loss: 0.4473896622657776\n",
      "Epoch 2471 loss: 0.4473632276058197\n",
      "Epoch 2472 loss: 0.4473368525505066\n",
      "Epoch 2473 loss: 0.44731050729751587\n",
      "Epoch 2474 loss: 0.44728410243988037\n",
      "Epoch 2475 loss: 0.4472578167915344\n",
      "Epoch 2476 loss: 0.4472314715385437\n",
      "Epoch 2477 loss: 0.4472050964832306\n",
      "Epoch 2478 loss: 0.44717878103256226\n",
      "Epoch 2479 loss: 0.44715243577957153\n",
      "Epoch 2480 loss: 0.44712623953819275\n",
      "Epoch 2481 loss: 0.4470999538898468\n",
      "Epoch 2482 loss: 0.44707366824150085\n",
      "Epoch 2483 loss: 0.4470474421977997\n",
      "Epoch 2484 loss: 0.4470212161540985\n",
      "Epoch 2485 loss: 0.4469950199127197\n",
      "Epoch 2486 loss: 0.44696876406669617\n",
      "Epoch 2487 loss: 0.44694259762763977\n",
      "Epoch 2488 loss: 0.446916401386261\n",
      "Epoch 2489 loss: 0.446890264749527\n",
      "Epoch 2490 loss: 0.44686412811279297\n",
      "Epoch 2491 loss: 0.4468379616737366\n",
      "Epoch 2492 loss: 0.44681182503700256\n",
      "Epoch 2493 loss: 0.44678568840026855\n",
      "Epoch 2494 loss: 0.44675955176353455\n",
      "Epoch 2495 loss: 0.4467334747314453\n",
      "Epoch 2496 loss: 0.44670742750167847\n",
      "Epoch 2497 loss: 0.44668135046958923\n",
      "Epoch 2498 loss: 0.4466552734375\n",
      "Epoch 2499 loss: 0.44662925601005554\n",
      "Epoch 2500 loss: 0.4466032087802887\n",
      "Epoch 2501 loss: 0.4465772211551666\n",
      "Epoch 2502 loss: 0.4465511739253998\n",
      "Epoch 2503 loss: 0.4465252161026001\n",
      "Epoch 2504 loss: 0.44649919867515564\n",
      "Epoch 2505 loss: 0.4464733302593231\n",
      "Epoch 2506 loss: 0.44644731283187866\n",
      "Epoch 2507 loss: 0.446421355009079\n",
      "Epoch 2508 loss: 0.4463954269886017\n",
      "Epoch 2509 loss: 0.4463695287704468\n",
      "Epoch 2510 loss: 0.44634363055229187\n",
      "Epoch 2511 loss: 0.44631773233413696\n",
      "Epoch 2512 loss: 0.44629186391830444\n",
      "Epoch 2513 loss: 0.4462659955024719\n",
      "Epoch 2514 loss: 0.4462401568889618\n",
      "Epoch 2515 loss: 0.4462142884731293\n",
      "Epoch 2516 loss: 0.4461885094642639\n",
      "Epoch 2517 loss: 0.4461626708507538\n",
      "Epoch 2518 loss: 0.44613686203956604\n",
      "Epoch 2519 loss: 0.4461110532283783\n",
      "Epoch 2520 loss: 0.44608524441719055\n",
      "Epoch 2521 loss: 0.44605952501296997\n",
      "Epoch 2522 loss: 0.446033775806427\n",
      "Epoch 2523 loss: 0.44600802659988403\n",
      "Epoch 2524 loss: 0.4459822475910187\n",
      "Epoch 2525 loss: 0.44595658779144287\n",
      "Epoch 2526 loss: 0.4459308683872223\n",
      "Epoch 2527 loss: 0.4459051787853241\n",
      "Epoch 2528 loss: 0.4458794593811035\n",
      "Epoch 2529 loss: 0.4458538293838501\n",
      "Epoch 2530 loss: 0.4458281695842743\n",
      "Epoch 2531 loss: 0.4458025395870209\n",
      "Epoch 2532 loss: 0.44577690958976746\n",
      "Epoch 2533 loss: 0.44575124979019165\n",
      "Epoch 2534 loss: 0.445725679397583\n",
      "Epoch 2535 loss: 0.44570010900497437\n",
      "Epoch 2536 loss: 0.44567447900772095\n",
      "Epoch 2537 loss: 0.4456489682197571\n",
      "Epoch 2538 loss: 0.4456234276294708\n",
      "Epoch 2539 loss: 0.4455978274345398\n",
      "Epoch 2540 loss: 0.4455723166465759\n",
      "Epoch 2541 loss: 0.4455467760562897\n",
      "Epoch 2542 loss: 0.4455212950706482\n",
      "Epoch 2543 loss: 0.4454957842826843\n",
      "Epoch 2544 loss: 0.44547030329704285\n",
      "Epoch 2545 loss: 0.44544485211372375\n",
      "Epoch 2546 loss: 0.4454193413257599\n",
      "Epoch 2547 loss: 0.4453939199447632\n",
      "Epoch 2548 loss: 0.4453684985637665\n",
      "Epoch 2549 loss: 0.4453430473804474\n",
      "Epoch 2550 loss: 0.44531768560409546\n",
      "Epoch 2551 loss: 0.445292204618454\n",
      "Epoch 2552 loss: 0.44526687264442444\n",
      "Epoch 2553 loss: 0.4452415406703949\n",
      "Epoch 2554 loss: 0.4452161192893982\n",
      "Epoch 2555 loss: 0.44519081711769104\n",
      "Epoch 2556 loss: 0.4451654851436615\n",
      "Epoch 2557 loss: 0.44514012336730957\n",
      "Epoch 2558 loss: 0.4451148211956024\n",
      "Epoch 2559 loss: 0.44508951902389526\n",
      "Epoch 2560 loss: 0.4450642168521881\n",
      "Epoch 2561 loss: 0.44503894448280334\n",
      "Epoch 2562 loss: 0.44501373171806335\n",
      "Epoch 2563 loss: 0.4449883997440338\n",
      "Epoch 2564 loss: 0.4449631869792938\n",
      "Epoch 2565 loss: 0.44493794441223145\n",
      "Epoch 2566 loss: 0.44491270184516907\n",
      "Epoch 2567 loss: 0.44488751888275146\n",
      "Epoch 2568 loss: 0.44486233592033386\n",
      "Epoch 2569 loss: 0.44483718276023865\n",
      "Epoch 2570 loss: 0.44481199979782104\n",
      "Epoch 2571 loss: 0.44478684663772583\n",
      "Epoch 2572 loss: 0.4447616934776306\n",
      "Epoch 2573 loss: 0.444736510515213\n",
      "Epoch 2574 loss: 0.4447114169597626\n",
      "Epoch 2575 loss: 0.4446863532066345\n",
      "Epoch 2576 loss: 0.4446612298488617\n",
      "Epoch 2577 loss: 0.44463613629341125\n",
      "Epoch 2578 loss: 0.4446111023426056\n",
      "Epoch 2579 loss: 0.44458597898483276\n",
      "Epoch 2580 loss: 0.4445609450340271\n",
      "Epoch 2581 loss: 0.44453588128089905\n",
      "Epoch 2582 loss: 0.44451087713241577\n",
      "Epoch 2583 loss: 0.4444858431816101\n",
      "Epoch 2584 loss: 0.44446083903312683\n",
      "Epoch 2585 loss: 0.44443583488464355\n",
      "Epoch 2586 loss: 0.44441092014312744\n",
      "Epoch 2587 loss: 0.44438594579696655\n",
      "Epoch 2588 loss: 0.44436097145080566\n",
      "Epoch 2589 loss: 0.44433605670928955\n",
      "Epoch 2590 loss: 0.44431108236312866\n",
      "Epoch 2591 loss: 0.44428616762161255\n",
      "Epoch 2592 loss: 0.4442613422870636\n",
      "Epoch 2593 loss: 0.4442363977432251\n",
      "Epoch 2594 loss: 0.4442114531993866\n",
      "Epoch 2595 loss: 0.44418665766716003\n",
      "Epoch 2596 loss: 0.4441617727279663\n",
      "Epoch 2597 loss: 0.4441368877887726\n",
      "Epoch 2598 loss: 0.44411203265190125\n",
      "Epoch 2599 loss: 0.4440872371196747\n",
      "Epoch 2600 loss: 0.44406238198280334\n",
      "Epoch 2601 loss: 0.44403761625289917\n",
      "Epoch 2602 loss: 0.4440128207206726\n",
      "Epoch 2603 loss: 0.44398802518844604\n",
      "Epoch 2604 loss: 0.44396331906318665\n",
      "Epoch 2605 loss: 0.4439385235309601\n",
      "Epoch 2606 loss: 0.4439137578010559\n",
      "Epoch 2607 loss: 0.4438890516757965\n",
      "Epoch 2608 loss: 0.44386428594589233\n",
      "Epoch 2609 loss: 0.4438396096229553\n",
      "Epoch 2610 loss: 0.4438149034976959\n",
      "Epoch 2611 loss: 0.4437902271747589\n",
      "Epoch 2612 loss: 0.4437655806541443\n",
      "Epoch 2613 loss: 0.4437408745288849\n",
      "Epoch 2614 loss: 0.44371622800827026\n",
      "Epoch 2615 loss: 0.443691611289978\n",
      "Epoch 2616 loss: 0.443666934967041\n",
      "Epoch 2617 loss: 0.44364234805107117\n",
      "Epoch 2618 loss: 0.44361770153045654\n",
      "Epoch 2619 loss: 0.4435930848121643\n",
      "Epoch 2620 loss: 0.44356855750083923\n",
      "Epoch 2621 loss: 0.443543940782547\n",
      "Epoch 2622 loss: 0.4435194134712219\n",
      "Epoch 2623 loss: 0.44349485635757446\n",
      "Epoch 2624 loss: 0.4434703588485718\n",
      "Epoch 2625 loss: 0.4434458315372467\n",
      "Epoch 2626 loss: 0.44342130422592163\n",
      "Epoch 2627 loss: 0.44339674711227417\n",
      "Epoch 2628 loss: 0.44337230920791626\n",
      "Epoch 2629 loss: 0.44334784150123596\n",
      "Epoch 2630 loss: 0.44332340359687805\n",
      "Epoch 2631 loss: 0.443298876285553\n",
      "Epoch 2632 loss: 0.44327449798583984\n",
      "Epoch 2633 loss: 0.44325003027915955\n",
      "Epoch 2634 loss: 0.44322556257247925\n",
      "Epoch 2635 loss: 0.4432011842727661\n",
      "Epoch 2636 loss: 0.443176805973053\n",
      "Epoch 2637 loss: 0.44315242767333984\n",
      "Epoch 2638 loss: 0.44312795996665955\n",
      "Epoch 2639 loss: 0.4431036114692688\n",
      "Epoch 2640 loss: 0.4430793225765228\n",
      "Epoch 2641 loss: 0.4430549740791321\n",
      "Epoch 2642 loss: 0.44303059577941895\n",
      "Epoch 2643 loss: 0.4430062770843506\n",
      "Epoch 2644 loss: 0.442982017993927\n",
      "Epoch 2645 loss: 0.44295769929885864\n",
      "Epoch 2646 loss: 0.4429333508014679\n",
      "Epoch 2647 loss: 0.4429090917110443\n",
      "Epoch 2648 loss: 0.4428848326206207\n",
      "Epoch 2649 loss: 0.4428606331348419\n",
      "Epoch 2650 loss: 0.44283634424209595\n",
      "Epoch 2651 loss: 0.44281211495399475\n",
      "Epoch 2652 loss: 0.44278788566589355\n",
      "Epoch 2653 loss: 0.44276365637779236\n",
      "Epoch 2654 loss: 0.4427395164966583\n",
      "Epoch 2655 loss: 0.4427153170108795\n",
      "Epoch 2656 loss: 0.4426911175251007\n",
      "Epoch 2657 loss: 0.4426669478416443\n",
      "Epoch 2658 loss: 0.44264283776283264\n",
      "Epoch 2659 loss: 0.4426186680793762\n",
      "Epoch 2660 loss: 0.4425945281982422\n",
      "Epoch 2661 loss: 0.44257044792175293\n",
      "Epoch 2662 loss: 0.4425463378429413\n",
      "Epoch 2663 loss: 0.44252216815948486\n",
      "Epoch 2664 loss: 0.4424981474876404\n",
      "Epoch 2665 loss: 0.44247400760650635\n",
      "Epoch 2666 loss: 0.44244998693466187\n",
      "Epoch 2667 loss: 0.44242599606513977\n",
      "Epoch 2668 loss: 0.44240185618400574\n",
      "Epoch 2669 loss: 0.44237783551216125\n",
      "Epoch 2670 loss: 0.4423538148403168\n",
      "Epoch 2671 loss: 0.4423298239707947\n",
      "Epoch 2672 loss: 0.4423058331012726\n",
      "Epoch 2673 loss: 0.4422818422317505\n",
      "Epoch 2674 loss: 0.4422578811645508\n",
      "Epoch 2675 loss: 0.4422338902950287\n",
      "Epoch 2676 loss: 0.4422098994255066\n",
      "Epoch 2677 loss: 0.4421859681606293\n",
      "Epoch 2678 loss: 0.44216209650039673\n",
      "Epoch 2679 loss: 0.44213810563087463\n",
      "Epoch 2680 loss: 0.4421142637729645\n",
      "Epoch 2681 loss: 0.44209036231040955\n",
      "Epoch 2682 loss: 0.4420664310455322\n",
      "Epoch 2683 loss: 0.44204258918762207\n",
      "Epoch 2684 loss: 0.4420187473297119\n",
      "Epoch 2685 loss: 0.44199487566947937\n",
      "Epoch 2686 loss: 0.44197097420692444\n",
      "Epoch 2687 loss: 0.44194722175598145\n",
      "Epoch 2688 loss: 0.4419233500957489\n",
      "Epoch 2689 loss: 0.44189950823783875\n",
      "Epoch 2690 loss: 0.44187578558921814\n",
      "Epoch 2691 loss: 0.4418518841266632\n",
      "Epoch 2692 loss: 0.441828191280365\n",
      "Epoch 2693 loss: 0.44180434942245483\n",
      "Epoch 2694 loss: 0.4417806565761566\n",
      "Epoch 2695 loss: 0.4417569041252136\n",
      "Epoch 2696 loss: 0.44173312187194824\n",
      "Epoch 2697 loss: 0.44170942902565\n",
      "Epoch 2698 loss: 0.4416857063770294\n",
      "Epoch 2699 loss: 0.4416620135307312\n",
      "Epoch 2700 loss: 0.441638320684433\n",
      "Epoch 2701 loss: 0.44161465764045715\n",
      "Epoch 2702 loss: 0.44159096479415894\n",
      "Epoch 2703 loss: 0.4415673613548279\n",
      "Epoch 2704 loss: 0.44154366850852966\n",
      "Epoch 2705 loss: 0.4415200352668762\n",
      "Epoch 2706 loss: 0.44149643182754517\n",
      "Epoch 2707 loss: 0.4414727985858917\n",
      "Epoch 2708 loss: 0.4414491653442383\n",
      "Epoch 2709 loss: 0.441425621509552\n",
      "Epoch 2710 loss: 0.44140201807022095\n",
      "Epoch 2711 loss: 0.4413784146308899\n",
      "Epoch 2712 loss: 0.4413548409938812\n",
      "Epoch 2713 loss: 0.44133126735687256\n",
      "Epoch 2714 loss: 0.4413077235221863\n",
      "Epoch 2715 loss: 0.44128426909446716\n",
      "Epoch 2716 loss: 0.4412607252597809\n",
      "Epoch 2717 loss: 0.4412371814250946\n",
      "Epoch 2718 loss: 0.4412136673927307\n",
      "Epoch 2719 loss: 0.4411902129650116\n",
      "Epoch 2720 loss: 0.4411666989326477\n",
      "Epoch 2721 loss: 0.441143274307251\n",
      "Epoch 2722 loss: 0.4411197900772095\n",
      "Epoch 2723 loss: 0.44109636545181274\n",
      "Epoch 2724 loss: 0.441072940826416\n",
      "Epoch 2725 loss: 0.4410495162010193\n",
      "Epoch 2726 loss: 0.4410260319709778\n",
      "Epoch 2727 loss: 0.44100266695022583\n",
      "Epoch 2728 loss: 0.4409792721271515\n",
      "Epoch 2729 loss: 0.44095587730407715\n",
      "Epoch 2730 loss: 0.4409325420856476\n",
      "Epoch 2731 loss: 0.44090917706489563\n",
      "Epoch 2732 loss: 0.44088584184646606\n",
      "Epoch 2733 loss: 0.4408624470233917\n",
      "Epoch 2734 loss: 0.44083914160728455\n",
      "Epoch 2735 loss: 0.440815806388855\n",
      "Epoch 2736 loss: 0.4407925307750702\n",
      "Epoch 2737 loss: 0.440769225358963\n",
      "Epoch 2738 loss: 0.4407459497451782\n",
      "Epoch 2739 loss: 0.44072264432907104\n",
      "Epoch 2740 loss: 0.44069936871528625\n",
      "Epoch 2741 loss: 0.44067612290382385\n",
      "Epoch 2742 loss: 0.44065284729003906\n",
      "Epoch 2743 loss: 0.44062966108322144\n",
      "Epoch 2744 loss: 0.44060641527175903\n",
      "Epoch 2745 loss: 0.4405832290649414\n",
      "Epoch 2746 loss: 0.4405600428581238\n",
      "Epoch 2747 loss: 0.440536767244339\n",
      "Epoch 2748 loss: 0.44051364064216614\n",
      "Epoch 2749 loss: 0.4404904544353485\n",
      "Epoch 2750 loss: 0.4404672682285309\n",
      "Epoch 2751 loss: 0.44044411182403564\n",
      "Epoch 2752 loss: 0.4404209554195404\n",
      "Epoch 2753 loss: 0.44039785861968994\n",
      "Epoch 2754 loss: 0.4403747618198395\n",
      "Epoch 2755 loss: 0.44035160541534424\n",
      "Epoch 2756 loss: 0.44032853841781616\n",
      "Epoch 2757 loss: 0.4403054714202881\n",
      "Epoch 2758 loss: 0.44028240442276\n",
      "Epoch 2759 loss: 0.44025927782058716\n",
      "Epoch 2760 loss: 0.44023624062538147\n",
      "Epoch 2761 loss: 0.4402132034301758\n",
      "Epoch 2762 loss: 0.4401901364326477\n",
      "Epoch 2763 loss: 0.4401671588420868\n",
      "Epoch 2764 loss: 0.44014406204223633\n",
      "Epoch 2765 loss: 0.44012102484703064\n",
      "Epoch 2766 loss: 0.4400981068611145\n",
      "Epoch 2767 loss: 0.4400750696659088\n",
      "Epoch 2768 loss: 0.44005218148231506\n",
      "Epoch 2769 loss: 0.440029114484787\n",
      "Epoch 2770 loss: 0.44000619649887085\n",
      "Epoch 2771 loss: 0.4399832785129547\n",
      "Epoch 2772 loss: 0.4399603009223938\n",
      "Epoch 2773 loss: 0.43993741273880005\n",
      "Epoch 2774 loss: 0.4399144947528839\n",
      "Epoch 2775 loss: 0.4398915767669678\n",
      "Epoch 2776 loss: 0.439868688583374\n",
      "Epoch 2777 loss: 0.4398458003997803\n",
      "Epoch 2778 loss: 0.4398229122161865\n",
      "Epoch 2779 loss: 0.43980005383491516\n",
      "Epoch 2780 loss: 0.4397771954536438\n",
      "Epoch 2781 loss: 0.4397543966770172\n",
      "Epoch 2782 loss: 0.43973153829574585\n",
      "Epoch 2783 loss: 0.43970873951911926\n",
      "Epoch 2784 loss: 0.4396859109401703\n",
      "Epoch 2785 loss: 0.4396631121635437\n",
      "Epoch 2786 loss: 0.43964025378227234\n",
      "Epoch 2787 loss: 0.4396175146102905\n",
      "Epoch 2788 loss: 0.43959474563598633\n",
      "Epoch 2789 loss: 0.4395720064640045\n",
      "Epoch 2790 loss: 0.43954920768737793\n",
      "Epoch 2791 loss: 0.4395264983177185\n",
      "Epoch 2792 loss: 0.4395037591457367\n",
      "Epoch 2793 loss: 0.43948104977607727\n",
      "Epoch 2794 loss: 0.43945831060409546\n",
      "Epoch 2795 loss: 0.4394356310367584\n",
      "Epoch 2796 loss: 0.439412921667099\n",
      "Epoch 2797 loss: 0.43939030170440674\n",
      "Epoch 2798 loss: 0.4393675923347473\n",
      "Epoch 2799 loss: 0.43934494256973267\n",
      "Epoch 2800 loss: 0.43932223320007324\n",
      "Epoch 2801 loss: 0.43929964303970337\n",
      "Epoch 2802 loss: 0.4392769932746887\n",
      "Epoch 2803 loss: 0.4392543435096741\n",
      "Epoch 2804 loss: 0.4392317235469818\n",
      "Epoch 2805 loss: 0.4392092227935791\n",
      "Epoch 2806 loss: 0.4391865134239197\n",
      "Epoch 2807 loss: 0.4391639828681946\n",
      "Epoch 2808 loss: 0.4391413927078247\n",
      "Epoch 2809 loss: 0.43911880254745483\n",
      "Epoch 2810 loss: 0.4390963017940521\n",
      "Epoch 2811 loss: 0.439073771238327\n",
      "Epoch 2812 loss: 0.43905121088027954\n",
      "Epoch 2813 loss: 0.43902871012687683\n",
      "Epoch 2814 loss: 0.4390062093734741\n",
      "Epoch 2815 loss: 0.4389837086200714\n",
      "Epoch 2816 loss: 0.4389612078666687\n",
      "Epoch 2817 loss: 0.4389386475086212\n",
      "Epoch 2818 loss: 0.43891623616218567\n",
      "Epoch 2819 loss: 0.43889379501342773\n",
      "Epoch 2820 loss: 0.4388713240623474\n",
      "Epoch 2821 loss: 0.4388488829135895\n",
      "Epoch 2822 loss: 0.43882644176483154\n",
      "Epoch 2823 loss: 0.4388040006160736\n",
      "Epoch 2824 loss: 0.43878158926963806\n",
      "Epoch 2825 loss: 0.4387592375278473\n",
      "Epoch 2826 loss: 0.43873679637908936\n",
      "Epoch 2827 loss: 0.4387144148349762\n",
      "Epoch 2828 loss: 0.4386920630931854\n",
      "Epoch 2829 loss: 0.43866971135139465\n",
      "Epoch 2830 loss: 0.4386473298072815\n",
      "Epoch 2831 loss: 0.4386250078678131\n",
      "Epoch 2832 loss: 0.43860262632369995\n",
      "Epoch 2833 loss: 0.43858033418655396\n",
      "Epoch 2834 loss: 0.43855804204940796\n",
      "Epoch 2835 loss: 0.4385357201099396\n",
      "Epoch 2836 loss: 0.43851345777511597\n",
      "Epoch 2837 loss: 0.4384911060333252\n",
      "Epoch 2838 loss: 0.43846890330314636\n",
      "Epoch 2839 loss: 0.43844661116600037\n",
      "Epoch 2840 loss: 0.43842434883117676\n",
      "Epoch 2841 loss: 0.4384021461009979\n",
      "Epoch 2842 loss: 0.43837982416152954\n",
      "Epoch 2843 loss: 0.4383576810359955\n",
      "Epoch 2844 loss: 0.43833544850349426\n",
      "Epoch 2845 loss: 0.43831321597099304\n",
      "Epoch 2846 loss: 0.4382910132408142\n",
      "Epoch 2847 loss: 0.4382688105106354\n",
      "Epoch 2848 loss: 0.43824660778045654\n",
      "Epoch 2849 loss: 0.43822452425956726\n",
      "Epoch 2850 loss: 0.4382023215293884\n",
      "Epoch 2851 loss: 0.43818017840385437\n",
      "Epoch 2852 loss: 0.43815797567367554\n",
      "Epoch 2853 loss: 0.43813592195510864\n",
      "Epoch 2854 loss: 0.438113808631897\n",
      "Epoch 2855 loss: 0.4380916357040405\n",
      "Epoch 2856 loss: 0.438069611787796\n",
      "Epoch 2857 loss: 0.43804746866226196\n",
      "Epoch 2858 loss: 0.4380253553390503\n",
      "Epoch 2859 loss: 0.438003271818161\n",
      "Epoch 2860 loss: 0.4379812777042389\n",
      "Epoch 2861 loss: 0.437959223985672\n",
      "Epoch 2862 loss: 0.4379371404647827\n",
      "Epoch 2863 loss: 0.4379151165485382\n",
      "Epoch 2864 loss: 0.4378931224346161\n",
      "Epoch 2865 loss: 0.4378710687160492\n",
      "Epoch 2866 loss: 0.4378490746021271\n",
      "Epoch 2867 loss: 0.43782711029052734\n",
      "Epoch 2868 loss: 0.43780508637428284\n",
      "Epoch 2869 loss: 0.4377831220626831\n",
      "Epoch 2870 loss: 0.4377611577510834\n",
      "Epoch 2871 loss: 0.43773916363716125\n",
      "Epoch 2872 loss: 0.4377172887325287\n",
      "Epoch 2873 loss: 0.43769529461860657\n",
      "Epoch 2874 loss: 0.4376733601093292\n",
      "Epoch 2875 loss: 0.4376514256000519\n",
      "Epoch 2876 loss: 0.4376295804977417\n",
      "Epoch 2877 loss: 0.43760764598846436\n",
      "Epoch 2878 loss: 0.4375857710838318\n",
      "Epoch 2879 loss: 0.43756386637687683\n",
      "Epoch 2880 loss: 0.43754199147224426\n",
      "Epoch 2881 loss: 0.43752017617225647\n",
      "Epoch 2882 loss: 0.4374982714653015\n",
      "Epoch 2883 loss: 0.4374764561653137\n",
      "Epoch 2884 loss: 0.43745461106300354\n",
      "Epoch 2885 loss: 0.43743279576301575\n",
      "Epoch 2886 loss: 0.43741098046302795\n",
      "Epoch 2887 loss: 0.43738916516304016\n",
      "Epoch 2888 loss: 0.43736737966537476\n",
      "Epoch 2889 loss: 0.43734562397003174\n",
      "Epoch 2890 loss: 0.43732374906539917\n",
      "Epoch 2891 loss: 0.4373020529747009\n",
      "Epoch 2892 loss: 0.43728023767471313\n",
      "Epoch 2893 loss: 0.4372585415840149\n",
      "Epoch 2894 loss: 0.43723681569099426\n",
      "Epoch 2895 loss: 0.43721503019332886\n",
      "Epoch 2896 loss: 0.4371933043003082\n",
      "Epoch 2897 loss: 0.4371716380119324\n",
      "Epoch 2898 loss: 0.43714988231658936\n",
      "Epoch 2899 loss: 0.4371281862258911\n",
      "Epoch 2900 loss: 0.43710649013519287\n",
      "Epoch 2901 loss: 0.4370848834514618\n",
      "Epoch 2902 loss: 0.43706321716308594\n",
      "Epoch 2903 loss: 0.4370415210723877\n",
      "Epoch 2904 loss: 0.4370199143886566\n",
      "Epoch 2905 loss: 0.43699830770492554\n",
      "Epoch 2906 loss: 0.4369766414165497\n",
      "Epoch 2907 loss: 0.4369550049304962\n",
      "Epoch 2908 loss: 0.43693339824676514\n",
      "Epoch 2909 loss: 0.43691182136535645\n",
      "Epoch 2910 loss: 0.436890184879303\n",
      "Epoch 2911 loss: 0.4368686378002167\n",
      "Epoch 2912 loss: 0.436847060918808\n",
      "Epoch 2913 loss: 0.4368254840373993\n",
      "Epoch 2914 loss: 0.436803936958313\n",
      "Epoch 2915 loss: 0.4367824196815491\n",
      "Epoch 2916 loss: 0.43676087260246277\n",
      "Epoch 2917 loss: 0.43673935532569885\n",
      "Epoch 2918 loss: 0.43671780824661255\n",
      "Epoch 2919 loss: 0.43669626116752625\n",
      "Epoch 2920 loss: 0.4366748034954071\n",
      "Epoch 2921 loss: 0.4366533160209656\n",
      "Epoch 2922 loss: 0.43663185834884644\n",
      "Epoch 2923 loss: 0.4366104006767273\n",
      "Epoch 2924 loss: 0.4365888833999634\n",
      "Epoch 2925 loss: 0.4365674555301666\n",
      "Epoch 2926 loss: 0.4365459680557251\n",
      "Epoch 2927 loss: 0.4365245997905731\n",
      "Epoch 2928 loss: 0.436503142118454\n",
      "Epoch 2929 loss: 0.436481773853302\n",
      "Epoch 2930 loss: 0.43646034598350525\n",
      "Epoch 2931 loss: 0.4364389479160309\n",
      "Epoch 2932 loss: 0.4364175498485565\n",
      "Epoch 2933 loss: 0.43639615178108215\n",
      "Epoch 2934 loss: 0.43637484312057495\n",
      "Epoch 2935 loss: 0.4363534450531006\n",
      "Epoch 2936 loss: 0.4363320767879486\n",
      "Epoch 2937 loss: 0.4363107681274414\n",
      "Epoch 2938 loss: 0.4362894296646118\n",
      "Epoch 2939 loss: 0.43626806139945984\n",
      "Epoch 2940 loss: 0.43624675273895264\n",
      "Epoch 2941 loss: 0.4362254738807678\n",
      "Epoch 2942 loss: 0.4362041652202606\n",
      "Epoch 2943 loss: 0.43618282675743103\n",
      "Epoch 2944 loss: 0.436161607503891\n",
      "Epoch 2945 loss: 0.4361403286457062\n",
      "Epoch 2946 loss: 0.43611904978752136\n",
      "Epoch 2947 loss: 0.43609780073165894\n",
      "Epoch 2948 loss: 0.4360765516757965\n",
      "Epoch 2949 loss: 0.4360552728176117\n",
      "Epoch 2950 loss: 0.43603411316871643\n",
      "Epoch 2951 loss: 0.4360128343105316\n",
      "Epoch 2952 loss: 0.43599170446395874\n",
      "Epoch 2953 loss: 0.4359704852104187\n",
      "Epoch 2954 loss: 0.43594932556152344\n",
      "Epoch 2955 loss: 0.4359281063079834\n",
      "Epoch 2956 loss: 0.43590694665908813\n",
      "Epoch 2957 loss: 0.43588578701019287\n",
      "Epoch 2958 loss: 0.4358646273612976\n",
      "Epoch 2959 loss: 0.43584349751472473\n",
      "Epoch 2960 loss: 0.43582236766815186\n",
      "Epoch 2961 loss: 0.435801237821579\n",
      "Epoch 2962 loss: 0.4357801377773285\n",
      "Epoch 2963 loss: 0.435759037733078\n",
      "Epoch 2964 loss: 0.43573787808418274\n",
      "Epoch 2965 loss: 0.4357168674468994\n",
      "Epoch 2966 loss: 0.43569570779800415\n",
      "Epoch 2967 loss: 0.43567466735839844\n",
      "Epoch 2968 loss: 0.43565356731414795\n",
      "Epoch 2969 loss: 0.43563252687454224\n",
      "Epoch 2970 loss: 0.4356114864349365\n",
      "Epoch 2971 loss: 0.4355904757976532\n",
      "Epoch 2972 loss: 0.4355694055557251\n",
      "Epoch 2973 loss: 0.4355483949184418\n",
      "Epoch 2974 loss: 0.43552738428115845\n",
      "Epoch 2975 loss: 0.4355064332485199\n",
      "Epoch 2976 loss: 0.4354854226112366\n",
      "Epoch 2977 loss: 0.43546441197395325\n",
      "Epoch 2978 loss: 0.4354434609413147\n",
      "Epoch 2979 loss: 0.43542250990867615\n",
      "Epoch 2980 loss: 0.4354015290737152\n",
      "Epoch 2981 loss: 0.4353805482387543\n",
      "Epoch 2982 loss: 0.4353596568107605\n",
      "Epoch 2983 loss: 0.43533870577812195\n",
      "Epoch 2984 loss: 0.4353177547454834\n",
      "Epoch 2985 loss: 0.4352968633174896\n",
      "Epoch 2986 loss: 0.43527597188949585\n",
      "Epoch 2987 loss: 0.4352550804615021\n",
      "Epoch 2988 loss: 0.4352341890335083\n",
      "Epoch 2989 loss: 0.4352132976055145\n",
      "Epoch 2990 loss: 0.43519243597984314\n",
      "Epoch 2991 loss: 0.43517157435417175\n",
      "Epoch 2992 loss: 0.43515077233314514\n",
      "Epoch 2993 loss: 0.43512991070747375\n",
      "Epoch 2994 loss: 0.43510913848876953\n",
      "Epoch 2995 loss: 0.43508827686309814\n",
      "Epoch 2996 loss: 0.43506741523742676\n",
      "Epoch 2997 loss: 0.43504664301872253\n",
      "Epoch 2998 loss: 0.4350258409976959\n",
      "Epoch 2999 loss: 0.4350050687789917\n",
      "final loss: 0.4350\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "model = nn.Linear(108, 1)\n",
    "activation = nn.Sigmoid()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = activation(model(X_train))\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "\n",
    "print(f\"final loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz trzeba sprawdzić, jak poszło naszej sieci. W PyTorchu sieć pracuje zawsze w jednym z dwóch trybów: treningowym lub ewaluacyjnym (predykcyjnym). Ten drugi wyłącza niektóre mechanizmy, które są używane tylko podczas treningu, w szczególności regularyzację dropout. Do przełączania służą metody modelu `.train()` i `.eval()`.\n",
    "\n",
    "Dodatkowo podczas liczenia predykcji dobrze jest wyłączyć liczenie gradientów, bo nie będą potrzebne, a oszczędza to czas i pamięć. Używa się do tego menadżera kontekstu `with torch.no_grad():`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zH37zDX4LAs2",
    "outputId": "b1f93309-6f04-4ffc-b0ca-08d0a32120a0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 85.23%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_score = activation(model(X_test))\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_score)\n",
    "print(f\"AUROC: {100 * auroc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest to całkiem dobry wynik, a może być jeszcze lepszy. Sprawdźmy dla pewności jeszcze inne metryki: precyzję, recall oraz F1-score. Dodatkowo narysujemy krzywą precision-recall, czyli jak zmieniają się te metryki w zależności od przyjętego progu (threshold) prawdopodobieństwa, powyżej którego przyjmujemy klasę pozytywną. Taką krzywą należy rysować na zbiorze walidacyjnym, bo później chcemy wykorzystać tę informację do doboru progu, a nie chcemy mieć wycieku danych testowych (data leakage).\n",
    "\n",
    "Poniżej zaimplementowano także funkcję `get_optimal_threshold()`, która sprawdza, dla którego progu uzyskujemy maksymalny F1-score, i zwraca indeks oraz wartość optymalnego progu. Przyda ci się ona w dalszej części laboratorium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "\n",
    "def get_optimal_threshold(\n",
    "    precisions: np.array, \n",
    "    recalls: np.array, \n",
    "    thresholds: np.array\n",
    ") -> Tuple[int, float]:\n",
    "    f1_scores = 2 * precisions * recalls / (precisions + recalls)\n",
    "    \n",
    "    optimal_idx = np.nanargmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    return optimal_idx, optimal_threshold\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_pred_score) -> None:\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_score)\n",
    "    optimal_idx, optimal_threshold = get_optimal_threshold(precisions, recalls, thresholds)\n",
    "\n",
    "    disp = PrecisionRecallDisplay(precisions, recalls)\n",
    "    disp.plot()\n",
    "    plt.title(f\"Precision-recall curve (opt. thresh.: {optimal_threshold:.4f})\")\n",
    "    plt.axvline(recalls[optimal_idx], color=\"green\", linestyle=\"-.\")\n",
    "    plt.axhline(precisions[optimal_idx], color=\"green\", linestyle=\"-.\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb Komórka 55\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     y_pred_valid_score \u001b[39m=\u001b[39m activation(model(X_valid))\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m plot_precision_recall_curve(y_valid, y_pred_valid_score)\n",
      "\u001b[1;32m/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb Komórka 55\u001b[0m in \u001b[0;36mplot_precision_recall_curve\u001b[0;34m(y_true, y_pred_score)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_precision_recall_curve\u001b[39m(y_true, y_pred_score) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     precisions, recalls, thresholds \u001b[39m=\u001b[39m precision_recall_curve(y_true, y_pred_score)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     optimal_idx, optimal_threshold \u001b[39m=\u001b[39m get_optimal_threshold(precisions, recalls, thresholds)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     disp \u001b[39m=\u001b[39m PrecisionRecallDisplay(precisions, recalls)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:858\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_recall_curve\u001b[39m(y_true, probas_pred, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    780\u001b[0m     \u001b[39m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \n\u001b[1;32m    782\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m \n\u001b[1;32m    857\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[1;32m    859\u001b[0m         y_true, probas_pred, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    862\u001b[0m     precision \u001b[39m=\u001b[39m tps \u001b[39m/\u001b[39m (tps \u001b[39m+\u001b[39m fps)\n\u001b[1;32m    863\u001b[0m     precision[np\u001b[39m.\u001b[39misnan(precision)] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:737\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    735\u001b[0m y_score \u001b[39m=\u001b[39m column_or_1d(y_score)\n\u001b[1;32m    736\u001b[0m assert_all_finite(y_true)\n\u001b[0;32m--> 737\u001b[0m assert_all_finite(y_score)\n\u001b[1;32m    739\u001b[0m \u001b[39m# Filter out zero-weighted samples, as they should not impact the result\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:134\u001b[0m, in \u001b[0;36massert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39massert_all_finite\u001b[39m(X, \u001b[39m*\u001b[39m, allow_nan\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    126\u001b[0m     \u001b[39m\"\"\"Throw a ValueError if X contains NaN or infinity.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m    allow_nan : bool, default=False\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     _assert_all_finite(X\u001b[39m.\u001b[39;49mdata \u001b[39mif\u001b[39;49;00m sp\u001b[39m.\u001b[39;49missparse(X) \u001b[39melse\u001b[39;49;00m X, allow_nan)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m         allow_nan\n\u001b[1;32m    109\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[1;32m    110\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    111\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[1;32m    112\u001b[0m     ):\n\u001b[1;32m    113\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_valid_score = activation(model(X_valid))\n",
    "\n",
    "plot_precision_recall_curve(y_valid, y_pred_valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfQPIUQ_LAs2"
   },
   "source": [
    "Jak widać, chociaż AUROC jest wysokie, to dla optymalnego F1-score recall nie jest zbyt wysoki, a precyzja jest już dość niska. Być może wynik uda się poprawić, używając modelu o większej pojemności - pełnej, głębokiej sieci neuronowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci neuronowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP298w6Cq7T6"
   },
   "source": [
    "Wszystko zaczęło się od inspirowanych biologią [sztucznych neuronów](https://en.wikipedia.org/wiki/Artificial_neuron), których próbowano użyć do symulacji mózgu. Naukowcy szybko odeszli od tego podejścia (sam problem modelowania okazał się też znacznie trudniejszy, niż sądzono), zamiast tego używając neuronów jako jednostek reprezentującą dowolną funkcję parametryczną $f(x, \\Theta)$. Każdy neuron jest zatem bardzo elastyczny, bo jedyne wymagania to funkcja różniczkowalna, a mamy do tego wektor parametrów $\\Theta$.\n",
    "\n",
    "W praktyce najczęściej można spotkać się z kilkoma rodzinami sieci neuronowych:\n",
    "1. Perceptrony wielowarstwowe (*MultiLayer Perceptron*, MLP) - najbardziej podobne do powyższego opisu, niezbędne do klasyfikacji i regresji\n",
    "2. Konwolucyjne (*Convolutional Neural Networks*, CNNs) - do przetwarzania danych z zależnościami przestrzennymi, np. obrazów czy dźwięku\n",
    "3. Rekurencyjne (*Recurrent Neural Networks*, RNNs) - do przetwarzania danych z zależnościami sekwencyjnymi, np. szeregi czasowe, oraz kiedyś do języka naturalnego\n",
    "4. Transformacyjne (*Transformers*), oparte o mechanizm atencji (*attention*) - do przetwarzania języka naturalnego (NLP), z którego wyparły RNNs, a coraz częściej także do wszelkich innych danych, np. obrazów, dźwięku\n",
    "5. Grafowe (*Graph Neural Networks*, GNNS) - do przetwarzania grafów\n",
    "\n",
    "Na tym laboratorium skupimy się na najprostszej architekturze, czyli MLP. Jest ona powszechnie łączona z wszelkimi innymi architekturami, bo pozwala dokonywać klasyfikacji i regresji. Przykładowo, klasyfikacja obrazów to zwykle CNN + MLP, klasyfikacja tekstów to transformer + MLP, a regresja na grafach to GNN + MLP.\n",
    "\n",
    "Dodatkowo, pomimo prostoty MLP są bardzo potężne - udowodniono, że perceptrony (ich powszechna nazwa) są [uniwersalnym aproksymatorem](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208), będącym w stanie przybliżyć dowolną funkcję z odpowiednio małym błędem, zakładając wystarczającą wielkość warstw sieci. Szczególne ich wersje potrafią nawet [reprezentować drzewa decyzyjne](https://www.youtube.com/watch?v=_okxGdHM5b8).\n",
    "\n",
    "Dla zainteresowanych polecamy [doskonałą książkę \"Dive into Deep Learning\", z implementacjami w PyTorchu](https://d2l.ai/chapter_multilayer-perceptrons/index.html), [klasyczną książkę \"Deep Learning Book\"](https://www.deeplearningbook.org/contents/mlp.html), oraz [ten filmik](https://www.youtube.com/watch?v=BFHrIxKcLjA), jeśli zastanawiałeś/-aś się, czemu używamy deep learning, a nie naprzykład (wide?) learning. (aka. czemu staramy się budować głębokie sieci, a nie płytkie za to szerokie)"
   ]
  },
  {
   "attachments": {
    "1_x-3NGQv0pRIab8xDT-f_Hg.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "S_ZjoGBU5upj"
   },
   "source": [
    "### Sieci MLP\n",
    "\n",
    "Dla przypomnienia, na wejściu mamy punkty ze zbioru treningowego, czyli $d$-wymiarowe wektory. W klasyfikacji chcemy znaleźć granicę decyzyjną, czyli krzywą, która oddzieli od siebie klasy. W wejściowej przestrzeni może być to trudne, bo chmury punktów z poszczególnych klas mogą być ze sobą dość pomieszane. Pamiętajmy też, że regresja logistyczna jest klasyfikatorem liniowym, czyli w danej przestrzeni potrafi oddzielić punkty tylko linią prostą.\n",
    "\n",
    "Sieć MLP składa się z warstw. Każda z nich dokonuje nieliniowego przekształcenia przestrzeni (można o tym myśleć jak o składaniu przestrzeni jakąś prostą/łamaną), tak, aby w finalnej przestrzeni nasze punkty były możliwie liniowo separowalne. Wtedy ostatnia warstwa z sigmoidą będzie potrafiła je rozdzielić od siebie.\n",
    "\n",
    "![1_x-3NGQv0pRIab8xDT-f_Hg.png](attachment:1_x-3NGQv0pRIab8xDT-f_Hg.png)\n",
    "\n",
    "Poszczególne neurony składają się z iloczynu skalarnego wejść z wagami neuronu, oraz nieliniowej funkcji aktywacji. W PyTorchu są to osobne obiekty - `nn.Linear` oraz np. `nn.Sigmoid`. Funkcja aktywacji przyjmuje wynik iloczynu skalarnego i przekształca go, aby sprawdzić, jak mocno reaguje neuron na dane wejście. Musi być nieliniowa z dwóch powodów. Po pierwsze, tylko nieliniowe przekształcenia są na tyle potężne, żeby umożliwić liniową separację danych w ostatniej warstwie. Po drugie, liniowe przekształcenia zwyczajnie nie działają. Aby zrozumieć czemu, trzeba zobaczyć, co matematycznie oznacza sieć MLP.\n",
    "\n",
    "![perceptron](https://www.saedsayad.com/images/Perceptron_bkp_1.png)\n",
    "\n",
    "Zapisane matematycznie MLP to:\n",
    "$\n",
    "h_1 = f_1(x) \\\\\n",
    "h_2 = f_2(h_1) \\\\\n",
    "h_3 = f_3(h_2) \\\\\n",
    "...\n",
    "h_n = f_n(h_{n-1})\n",
    "$\n",
    "gdzie $x$ to wejście $f_i$ to funkcja aktywacji $i$-tej warstwy, a $h_i$ to wyjście $i$-tej warstwy, nazywane **ukrytą reprezentacją (hidden representation)**, lub *latent representation*. Nazwa bierze się z tego, że w środku sieci wyciągamy cechy i wzorce w danych, które nie są widoczne na pierwszy rzut oka na wejściu.\n",
    "\n",
    "Załóżmy, że nie mamy funkcji aktywacji, czyli mamy aktywację liniową $f(x) = x$. Zobaczmy na początku sieci:\n",
    "$\n",
    "h_1 = f_1(x) = x\n",
    "h_2 = f_2(f_1) = f_2(x) = x\n",
    "...\n",
    "h_n = f_n(f_{n-1}) = f_n(x) = x\n",
    "$\n",
    "Jak widać, taka sieć niczego się nie nauczy. Wynika to z tego, że złożenie funkcji liniowych jest także funkcją liniową - patrz notatki z algebry :)\n",
    "\n",
    "Jeżeli natomiast użyjemy nieliniowej funkcji aktywacji, często oznaczanej jako $\\sigma$, to wszystko będzie działać. Co ważne, ostatnia warstwa, dająca wyjście sieci, ma zwykle inną aktywację od warstw wewnątrz sieci, bo też ma inne zadanie - zwrócić wartość dla klasyfikacji lub regresji. Na wyjściu korzysta się z funkcji liniowej (regresja), sigmoidalnej (klasyfikacja binarna) lub softmax (klasyfikacja wieloklasowa).\n",
    "\n",
    "Wewnątrz sieci używano kiedyś sigmoidy oraz tangensa hiperbolicznego `tanh`, ale okazało się to nieefektywne przy uczeniu głębokich sieci o wielu warstwach. Nowoczesne sieci korzystają zwykle z funkcji ReLU (*rectified linear unit*), która jest zaskakująco prosta: $ReLU(x) = \\max(0, x)$. Okazało się, że bardzo dobrze nadaje się do treningu nawet bardzo głębokich sieci neuronowych. Nowsze funkcje aktywacji są głównie modyfikacjami ReLU.\n",
    "\n",
    "![relu](https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP w PyTorchu\n",
    "\n",
    "Warstwę neuronów w MLP nazywa się warstwą gęstą (*dense layer*) lub warstwą w pełni połączoną (*fully-connected layer*), i taki opis oznacza zwykle same neurony oraz funkcję aktywacji. PyTorch, jak już widzieliśmy, definiuje osobno transformację liniową oraz aktywację, a więc jedna warstwa składa się de facto z 2 obiektów, wywoływanych jeden po drugim. Inne frameworki, szczególnie wysokopoziomowe (np. Keras) łączą to często w jeden obiekt.\n",
    "\n",
    "MLP składa się zatem z sekwencji obiektów, które potem wywołuje się jeden po drugim, gdzie wyjście poprzedniego to wejście kolejnego. Ale nie można tutaj używać Pythonowych list! Z perspektywy PyTorcha to wtedy niezależne obiekty i nie zostanie wtedy przekazany między nimi gradient. Trzeba tutaj skorzystać z `nn.Sequential`, aby tworzyć taki pipeline.\n",
    "\n",
    "Rozmiary wejścia i wyjścia dla każdej warstwy trzeba w PyTorchu podawać explicite. Jest to po pierwsze edukacyjne, a po drugie często ułatwia wnioskowanie o działaniu sieci oraz jej debugowanie - mamy jasno podane, czego oczekujemy. Niektóre frameworki (np. Keras) obliczają to automatycznie.\n",
    "\n",
    "Co ważne, ostatnia warstwa zwykle nie ma funkcji aktywacji. Wynika to z tego, że obliczanie wielu funkcji kosztu (np. entropii krzyżowej) na aktywacjach jest często niestabilne numerycznie. Z tego powodu PyTorch oferuje funkcje kosztu zawierające w środku aktywację dla ostatniej warstwy, a ich implementacje są stabilne numerycznie. Przykładowo, `nn.BCELoss` przyjmuje wejście z zaaplikowanymi już aktywacjami, ale może skutkować under/overflow, natomiast `nn.BCEWithLogitsLoss` przyjmuje wejście bez aktywacji, a w środku ma specjalną implementację łączącą binarną entropię krzyżową z aktywacją sigmoidalną. Oczywiście w związku z tym aby dokonać potem predykcji w praktyce, trzeba pamiętać o użyciu funkcji aktywacji. Często korzysta się przy tym z funkcji z modułu `torch.nn.functional`, które są w tym wypadku nieco wygodniejsze od klas wywoływalnych z `torch.nn`.\n",
    "\n",
    "Całe sieci w PyTorchu tworzy się jako klasy dziedziczące po `nn.Module`. Co ważne, obiekty, z których tworzymy sieć, np. `nn.Linear`, także dziedziczą po tej klasie. Pozwala to na bardzo modułową budowę kodu, zgodną z zasadami OOP. W konstruktorze najpierw trzeba zawsze wywołać konstruktor rodzica - `super().__init__()`, a później tworzy się potrzebne obiekty i zapisuje jako atrybuty. Musimy też zdefiniować metodę `forward()`, która przyjmuje tensor `x` i zwraca wynik. Typowo ta metoda po prostu używa obiektów zdefiniowanych w konstruktorze.\n",
    "\n",
    "\n",
    "**UWAGA: nigdy w normalnych warunkach się nie woła metody `forward` ręcznie**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8niDgExAMDO"
   },
   "source": [
    "#### Zadanie 4 (1 punkt)\n",
    "\n",
    "Uzupełnij implementację 3-warstwowej sieci MLP. Użyj rozmiarów:\n",
    "* pierwsza warstwa: input_size x 256\n",
    "* druga warstwa: 256 x 128\n",
    "* trzecia warstwa: 128 x 1\n",
    "\n",
    "Użyj funkcji aktywacji ReLU.\n",
    "\n",
    "Przydatne klasy:\n",
    "- `nn.Sequential`\n",
    "- `nn.Linear`\n",
    "- `nn.ReLU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZpuVDz1ALU5",
    "outputId": "cfe0d6ac-d2ce-43dd-cc22-837063f0f6bf"
   },
   "outputs": [],
   "source": [
    "from torch import sigmoid\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        first_second = 256\n",
    "        second_third = 128\n",
    "        last = 1\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_size, first_second),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(first_second, second_third),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(second_third, last),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp.forward(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return torch.argmax(y_pred_score, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.6888\n",
      "Epoch 200 train loss: 0.6665\n",
      "Epoch 400 train loss: 0.6473\n",
      "Epoch 600 train loss: 0.6306\n",
      "Epoch 800 train loss: 0.6158\n",
      "Epoch 1000 train loss: 0.6027\n",
      "Epoch 1200 train loss: 0.5911\n",
      "Epoch 1400 train loss: 0.5807\n",
      "Epoch 1600 train loss: 0.5714\n",
      "Epoch 1800 train loss: 0.5629\n",
      "final loss: 0.5553\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "model = MLP(input_size=X_train.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# note that we are using loss function with sigmoid built in\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "num_epochs = 2000\n",
    "evaluation_steps = 200\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % evaluation_steps == 0:\n",
    "        print(f\"Epoch {i} train loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"final loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LP5GSup24dXU",
    "outputId": "05f332c4-5d94-41f6-f85b-17793d3c4b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 85.23%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb Komórka 64\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m auroc \u001b[39m=\u001b[39m roc_auc_score(y_test, y_score)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAUROC: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m auroc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m plot_precision_recall_curve(y_valid, y_pred_valid_score)\n",
      "\u001b[1;32m/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb Komórka 64\u001b[0m in \u001b[0;36mplot_precision_recall_curve\u001b[0;34m(y_true, y_pred_score)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_precision_recall_curve\u001b[39m(y_true, y_pred_score) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     precisions, recalls, thresholds \u001b[39m=\u001b[39m precision_recall_curve(y_true, y_pred_score)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     optimal_idx, optimal_threshold \u001b[39m=\u001b[39m get_optimal_threshold(precisions, recalls, thresholds)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     disp \u001b[39m=\u001b[39m PrecisionRecallDisplay(precisions, recalls)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:858\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_recall_curve\u001b[39m(y_true, probas_pred, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    780\u001b[0m     \u001b[39m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \n\u001b[1;32m    782\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m \n\u001b[1;32m    857\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[1;32m    859\u001b[0m         y_true, probas_pred, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    862\u001b[0m     precision \u001b[39m=\u001b[39m tps \u001b[39m/\u001b[39m (tps \u001b[39m+\u001b[39m fps)\n\u001b[1;32m    863\u001b[0m     precision[np\u001b[39m.\u001b[39misnan(precision)] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:737\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    735\u001b[0m y_score \u001b[39m=\u001b[39m column_or_1d(y_score)\n\u001b[1;32m    736\u001b[0m assert_all_finite(y_true)\n\u001b[0;32m--> 737\u001b[0m assert_all_finite(y_score)\n\u001b[1;32m    739\u001b[0m \u001b[39m# Filter out zero-weighted samples, as they should not impact the result\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:134\u001b[0m, in \u001b[0;36massert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39massert_all_finite\u001b[39m(X, \u001b[39m*\u001b[39m, allow_nan\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    126\u001b[0m     \u001b[39m\"\"\"Throw a ValueError if X contains NaN or infinity.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m    allow_nan : bool, default=False\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     _assert_all_finite(X\u001b[39m.\u001b[39;49mdata \u001b[39mif\u001b[39;49;00m sp\u001b[39m.\u001b[39;49missparse(X) \u001b[39melse\u001b[39;49;00m X, allow_nan)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m         allow_nan\n\u001b[1;32m    109\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[1;32m    110\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    111\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[1;32m    112\u001b[0m     ):\n\u001b[1;32m    113\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # positive class probabilities\n",
    "    y_pred_valid_score = model.predict_proba(X_valid)\n",
    "    y_pred_test_score = model.predict_proba(X_test)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred_test_score)\n",
    "print(f\"AUROC: {100 * auroc:.2f}%\")\n",
    "\n",
    "plot_precision_recall_curve(y_valid, y_pred_valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC jest podobne, a precision i recall spadły - wypadamy wręcz gorzej od regresji liniowej! Skoro dodaliśmy więcej warstw, to może pojemność modelu jest teraz za duża i trzeba by go zregularyzować?\n",
    "\n",
    "Sieci neuronowe bardzo łatwo przeuczają, bo są bardzo elastycznymi i pojemnymi modelami. Dlatego mają wiele różnych rodzajów regularyzacji, których używa się razem. Co ciekawe, udowodniono eksperymentalnie, że zbyt duże sieci z mocną regularyzacją działają lepiej niż mniejsze sieci, odpowiedniego rozmiaru, za to ze słabszą regularyzacją.\n",
    "\n",
    "Pierwszy rodzaj regularyzacji to znana nam już **regularyzacja L2**, czyli penalizacja zbyt dużych wag. W kontekście sieci neuronowych nazywa się też ją czasem *weight decay*. W PyTorchu dodaje się ją jako argument do optymalizatora.\n",
    "\n",
    "Regularyzacja specyficzna dla sieci neuronowych to **dropout**. Polega on na losowym wyłączaniu zadanego procenta neuronów podczas treningu. Pomimo prostoty okazała się niesamowicie skuteczna, szczególnie w treningu bardzo głębokich sieci. Co ważne, jest to mechanizm używany tylko podczas treningu - w trakcie predykcji za pomocą sieci wyłącza się ten mechanizm i dokonuje normalnie predykcji całą siecią. Podejście to można potraktować jak ensemble learning, podobny do lasów losowych - wyłączając losowe części sieci, w każdej iteracji trenujemy nieco inną sieć, co odpowiada uśrednianiu predykcji różnych algorytmów. Typowo stosuje się dość mocny dropout, rzędu 25-50%. W PyTorchu implementuje go warstwa `nn.Dropout`, aplikowana zazwyczaj po funkcji aktywacji.\n",
    "\n",
    "Ostatni, a być może najważniejszy rodzaj regularyzacji to **wczesny stop (early stopping)**. W każdym kroku mocniej dostosowujemy terenową sieć do zbioru treningowego, a więc zbyt długi trening będzie skutkował przeuczeniem. W metodzie wczesnego stopu używamy wydzielonego zbioru walidacyjnego (pojedynczego, metoda holdout), sprawdzając co określoną liczbę epok wynik na tym zbiorze. Jeżeli nie uzyskamy wyniku lepszego od najlepszego dotychczas uzyskanego przez określoną liczbę epok, to przerywamy trening. Okres, przez który czekamy na uzyskanie lepszego wyniku, to cierpliwość (*patience*). Im mniejsze, tym mocniejszy jest ten rodzaj regularyzacji, ale trzeba z tym uważać, bo łatwo jest przesadzić i zbyt szybko przerywać trening. Niektóre implementacje uwzględniają tzw. *grace period*, czyli gwarantowaną minimalną liczbę epok, przez którą będziemy trenować sieć, niezależnie od wybranej cierpliwości.\n",
    "\n",
    "Dodatkowo ryzyko przeuczenia można zmniejszyć, używając mniejszej stałej uczącej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 5 (1 punkt)\n",
    "\n",
    "Zaimplementuj funkcję `evaluate_model()`, obliczającą metryki na zbiorze testowym:\n",
    "- wartość funkcji kosztu (loss)\n",
    "- AUROC\n",
    "- optymalny próg\n",
    "- F1-score przy optymalnym progu\n",
    "- precyzję oraz recall dla optymalnego progu\n",
    "\n",
    "Jeżeli podana jest wartość argumentu `threshold`, to użyj jej do zamiany prawdopodobieństw na twarde predykcje. W przeciwnym razie użyj funkcji `get_optimal_threshold` i oblicz optymalną wartość progu.\n",
    "\n",
    "Pamiętaj o przełączeniu modelu w tryb ewaluacji oraz o wyłączeniu obliczania gradientów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch import sigmoid\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module, \n",
    "    X: torch.Tensor, \n",
    "    y: torch.Tensor, \n",
    "    loss_fn: nn.Module,\n",
    "    threshold: Optional[float]= None\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        auroc = roc_auc_score(y, y_pred)\n",
    "        \n",
    "        if threshold is None:\n",
    "            precisions, recalls, thresholds = precision_recall_curve(y, y_pred)\n",
    "            _, threshold = get_optimal_threshold(precisions, recalls, thresholds)\n",
    "        \n",
    "        y_pred = (y_pred > threshold).float()\n",
    "\n",
    "        precision = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)  \n",
    "        f1 = f1_score(y, y_pred)\n",
    "        \n",
    "        results = {\n",
    "            \"loss\": loss,\n",
    "            \"AUROC\": auroc,\n",
    "            \"optimal_threshold\": threshold,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"F1-score\": f1,\n",
    "        }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 6 (1 punkt)\n",
    "\n",
    "Zaimplementuj 3-warstwową sieć MLP z regularyzacją L2 oraz dropout (50%). Rozmiary warstw ukrytych mają wynosić 256 i 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        first_second = 256\n",
    "        second_third = 128\n",
    "        last = 1\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_size, first_second),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(first_second, second_third),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(second_third, last),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp.forward(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return torch.argmax(y_pred_score, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEk9azaULAsz"
   },
   "source": [
    "Opisaliśmy wcześniej podstawowy optymalizator w sieciach neuronowych - spadek wzdłuż gradientu. Jednak wymaga on użycia całego zbioru danych, aby obliczyć gradient, co jest często niewykonalne przez rozmiar zbioru. Dlatego wymyślono **stochastyczny spadek wzdłuż gradientu (stochastic gradient descent, SGD)**, w którym używamy 1 przykładu naraz, liczymy gradient tylko po nim i aktualizujemy parametry. Jest to oczywiście dość grube przybliżenie gradientu, ale pozwala robić szybko dużo małych kroków. Kompromisem, którego używa się w praktyce, jest **minibatch gradient descent**, czyli używanie batchy np. 32, 64 czy 128 przykładów.\n",
    "\n",
    "Rzadko wspominanym, a ważnym faktem jest także to, że stochastyczność metody optymalizacji jest sama w sobie też [metodą regularyzacji](https://arxiv.org/abs/2101.12176), a więc `batch_size` to także hiperparametr.\n",
    "\n",
    "Obecnie najpopularniejszą odmianą SGD jest [Adam](https://arxiv.org/abs/1412.6980), gdyż uczy on szybko sieć oraz daje bardzo dobre wyniki nawet przy niekoniecznie idealnie dobranych hiperparametrach. W PyTorchu najlepiej korzystać z jego implementacji `AdamW`, która jest nieco lepsza niż implementacja `Adam`. Jest to zasadniczo zawsze wybór domyślny przy treningu współczesnych sieci neuronowych.\n",
    "\n",
    "Na razie użyjemy jednak minibatch SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej znajduje się implementacja prostej klasy dziedziczącej po `Dataset` - tak w PyTorchu implementuje się własne zbiory danych. Użycie takich klas umożliwia użycie klas ładujących dane (`DataLoader`), które z kolei pozwalają łatwo ładować batche danych. Trzeba w takiej klasie zaimplementować metody:\n",
    "- `__len__` - zwraca ilość punktów w zbiorze\n",
    "- `__getitem__` - zwraca przykład ze zbioru pod danym indeksem oraz jego klasę\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, y):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 7 (2 punkty)\n",
    "\n",
    "Zaimplementuj pętlę treningowo-walidacyjną dla sieci neuronowej. Wykorzystaj podane wartości hiperparametrów do treningu (stała ucząca, prawdopodobieństwo dropoutu, regularyzacja L2, rozmiar batcha, maksymalna liczba epok). Użyj optymalizatora SGD.\n",
    "\n",
    "Dodatkowo zaimplementuj regularyzację przez early stopping. Sprawdzaj co epokę wynik na zbiorze walidacyjnym. Użyj podanej wartości patience, a jako metryki po prostu wartości funkcji kosztu. Może się tutaj przydać zaimplementowana funkcja `evaluate_model()`.\n",
    "\n",
    "Pamiętaj o tym, aby przechowywać najlepszy dotychczasowy wynik walidacyjny oraz najlepszy dotychczasowy model. Zapamiętaj też optymalny próg do klasyfikacji dla najlepszego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "dropout_p = 0.5\n",
    "l2_reg = 1e-4\n",
    "batch_size = 128\n",
    "max_epochs = 300\n",
    "\n",
    "early_stopping_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb Komórka 75\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# model evaluation, early stopping\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# implement me!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m valid_metrics \u001b[39m=\u001b[39m evaluate_model(model, X_valid, y_valid, loss_fn)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m valid_metrics[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m<\u001b[39m best_val_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     best_val_loss \u001b[39m=\u001b[39m valid_metrics[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;32m/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb Komórka 75\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, X, y, loss_fn, threshold)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m auroc \u001b[39m=\u001b[39m roc_auc_score(y, y_pred)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mif\u001b[39;00m threshold \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/xkspo/Desktop/sztuczna-inteligencja/lab3/lab_3.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     precisions, recalls, thresholds \u001b[39m=\u001b[39m precision_recall_curve(y, y_pred)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:546\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    544\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y_true)\n\u001b[1;32m    545\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 546\u001b[0m y_score \u001b[39m=\u001b[39m check_array(y_score, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    548\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    549\u001b[0m     y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m y_score\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m y_score\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    550\u001b[0m ):\n\u001b[1;32m    551\u001b[0m     \u001b[39m# do not support partial ROC computation for multiclass\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m max_fpr \u001b[39m!=\u001b[39m \u001b[39m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    795\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    797\u001b[0m         )\n\u001b[1;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 800\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    802\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    803\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m         allow_nan\n\u001b[1;32m    109\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[1;32m    110\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    111\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[1;32m    112\u001b[0m     ):\n\u001b[1;32m    113\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "model = RegularizedMLP(\n",
    "    input_size=X_train.shape[1], \n",
    "    dropout_p=dropout_p\n",
    ")\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=l2_reg\n",
    ")\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "steps_without_improvement = 0\n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_model = None\n",
    "best_threshold = None\n",
    "\n",
    "for epoch_num in range(max_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # note that we are using DataLoader to get batches\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        # model training\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    # model evaluation, early stopping\n",
    "    # implement me!\n",
    "    \n",
    "    model.eval()\n",
    "    valid_metrics = evaluate_model(model, X_valid, y_valid, loss_fn)\n",
    "    if valid_metrics[\"loss\"] < best_val_loss:\n",
    "        best_val_loss = valid_metrics[\"loss\"]\n",
    "        best_model = deepcopy(model)\n",
    "        best_threshold = valid_metrics[\"optimal_threshold\"]\n",
    "        steps_without_improvement = 0\n",
    "    else:\n",
    "        steps_without_improvement += 1\n",
    "        if early_stopping_patience <= steps_without_improvement:\n",
    "            break\n",
    "    \n",
    "    print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = evaluate_model(best_model, X_test, y_test, loss_fn, best_threshold)\n",
    "\n",
    "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
    "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
    "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
    "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki wyglądają już dużo lepiej.\n",
    "\n",
    "Na koniec laboratorium dołożymy do naszego modelu jeszcze 3 powrzechnie używane techniki, które są bardzo proste, a pozwalają często ulepszyć wynik modelu.\n",
    "\n",
    "Pierwszą z nich są **warstwy normalizacji (normalization layers)**. Powstały one początkowo z założeniem, że przez przekształcenia przestrzeni dokonywane przez sieć zmienia się rozkład prawdopodobieństw pomiędzy warstwami, czyli tzw. *internal covariate shift*. Później okazało się, że zastosowanie takiej normalizacji wygładza powierzchnię funkcji kosztu, co ułatwia i przyspiesza optymalizację. Najpowszechniej używaną normalizacją jest **batch normalization (batch norm)**.\n",
    "\n",
    "Drugim ulepszeniem jest dodanie **wag klas (class weights)**. Mamy do czynienia z problemem klasyfikacji niezbalansowanej, więc klasa mniejszościowa, ważniejsza dla nas, powinna dostać większą wagę. Implementuje się to trywialnie prosto - po prostu mnożymy wartość funkcji kosztu dla danego przykładu przez wagę dla prawdziwej klasy tego przykładu. Praktycznie każdy klasyfikator operujący na jakiejś ważonej funkcji może działać w ten sposób, nie tylko sieci neuronowe.\n",
    "\n",
    "Ostatnim ulepszeniem jest zamiana SGD na optymalizator Adam, a konkretnie na optymalizator `AdamW`. Jest to przykład **optymalizatora adaptacyjnego (adaptive optimizer)**, który potrafi zaadaptować stałą uczącą dla każdego parametru z osobna w trakcie treningu. Wykorzystuje do tego gradienty - w uproszczeniu, im większa wariancja gradientu, tym mniejsze kroki w tym kierunku robimy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 8 (1 punkt)\n",
    "\n",
    "Zaimplementuj model `NormalizingMLP`, o takiej samej strukturze jak `RegularizedMLP`, ale dodatkowo z warstwami `BatchNorm1d` pomiędzy warstwami `Linear` oraz `ReLU`.\n",
    "\n",
    "Za pomocą funkcji `compute_class_weight()` oblicz wagi dla poszczególnych klas. Użyj opcji `\"balanced\"`. Przekaż do funkcji kosztu wagę klasy pozytywnej (pamiętaj, aby zamienić ją na tensor).\n",
    "\n",
    "Zamień używany optymalizator na `AdamW`.\n",
    "\n",
    "Na koniec skopiuj resztę kodu do treningu z poprzedniego zadania, wytrenuj sieć i oblicz wyniki na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        first_second = 256\n",
    "        second_third = 128\n",
    "        last = 1\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_size, first_second),\n",
    "            nn.BatchNorm1d(first_second),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(first_second, second_third),\n",
    "            nn.BatchNorm1d(second_third),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(second_third, last)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp.forward(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return torch.argmax(y_pred_score, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "weights = compute_class_weight(\n",
    "    class_weight = \"balanced\",\n",
    "    classes = np.unique(y_train),\n",
    "    y = y_train\n",
    ")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "dropout_p = 0.5\n",
    "l2_reg = 1e-4\n",
    "batch_size = 128\n",
    "max_epochs = 300\n",
    "\n",
    "early_stopping_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NormalizingMLP(\n",
    "    input_size=X_train.shape[1], \n",
    "    dropout_p=dropout_p\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=l2_reg\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(weights)[1])\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "steps_without_improvement = 0\n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_model = None\n",
    "best_threshold = None\n",
    "\n",
    "for epoch_num in range(max_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # note that we are using DataLoader to get batches\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # model evaluation, early stopping\n",
    "    # implement me!\n",
    "    \n",
    "    model.eval()\n",
    "    valid_metrics = evaluate_model(model, X_valid, y_valid, loss_fn)\n",
    "    if valid_metrics[\"loss\"] < best_val_loss:\n",
    "        best_val_loss = valid_metrics[\"loss\"]\n",
    "        best_model = deepcopy(model)\n",
    "        best_threshold = valid_metrics[\"optimal_threshold\"]\n",
    "        steps_without_improvement = 0\n",
    "    else:\n",
    "        steps_without_improvement += 1\n",
    "        if early_stopping_patience <= steps_without_improvement:\n",
    "            break\n",
    "    \n",
    "    print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_metrics = evaluate_model(best_model, X_test, y_test, loss_fn, best_threshold)\n",
    "\n",
    "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
    "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
    "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
    "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytania kontrolne (1 punkt)\n",
    "\n",
    "1. Wymień 4 najważniejsze twoim zdaniem hiperparametry sieci neuronowej.\n",
    "2. Czy widzisz jakiś problem w użyciu regularyzacji L1 w treningu sieci neuronowych? Czy dropout może twoim zdaniem stanowić alternatywę dla tego rodzaju regularyzacji?\n",
    "3. Czy użycie innej metryki do wczesnego stopu da taki sam model końcowy? Czemu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyoRnHT4GFR9"
   },
   "source": [
    "## Akceleracja sprzętowa (dla zainteresowanych)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak wcześniej wspominaliśmy, użycie akceleracji sprzętowej, czyli po prostu GPU do obliczeń, jest bardzo efektywne w przypadku sieci neuronowych. Karty graficzne bardzo efektywnie mnożą macierze, a sieci neuronowe to, jak można było się przekonać, dużo mnożenia macierzy.\n",
    "\n",
    "W PyTorchu jest to dosyć łatwe, ale trzeba robić to explicite. Służy do tego metoda `.to()`, która przenosi tensory między CPU i GPU. Poniżej przykład, jak to się robi (oczywiście trzeba mieć skonfigurowane GPU, żeby działało):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "model = NormalizingMLP(\n",
    "    input_size=X_train.shape[1], \n",
    "    dropout_p=dropout_p\n",
    ").to('cuda')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# note that we are using loss function with sigmoid built in\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(weights)[1].to('cuda'))\n",
    "\n",
    "step_counter = 0\n",
    "time_from_eval = time.time()\n",
    "for epoch_id in range(30):\n",
    "    for batch_x, batch_y in train_dataloader:\n",
    "        batch_x = batch_x.to('cuda')\n",
    "        batch_y = batch_y.to('cuda')\n",
    "        \n",
    "        loss = loss_fn(model(batch_x), batch_y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if step_counter % evaluation_steps == 0:\n",
    "            print(f\"Epoch {epoch_id} train loss: {loss.item():.4f}, time: {time.time() - time_from_eval}\")\n",
    "            time_from_eval = time.time()\n",
    "\n",
    "        step_counter += 1\n",
    "\n",
    "test_res = evaluate_model(model.to('cpu'), X_test, y_test, loss_fn.to('cpu'), threshold=0.5)\n",
    "\n",
    "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
    "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki mogą się różnić z modelem na CPU, zauważ o ile szybszy jest ten model w porównaniu z CPU (przynajmniej w przypadków scenariuszy tak będzie ;)).\n",
    "\n",
    "Dla zainteresowanych polecamy [tę serie artykułów](https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie dla chętnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzieliśmy, sieci neuronowe mają bardzo dużo hiperparametrów. Przeszukiwanie ich grid search'em jest więc niewykonalne, a chociaż random search by działał, to potrzebowałby wielu iteracji, co też jest kosztowne obliczeniowo.\n",
    "\n",
    "Zaimplementuj inteligentne przeszukiwanie przestrzeni hiperparametrów za pomocą biblioteki [Optuna](https://optuna.org/). Implementuje ona między innymi algorytm Tree Parzen Estimator (TPE), należący do grupy algorytmów typu Bayesian search. Typowo osiągają one bardzo dobre wyniki, a właściwie zawsze lepsze od przeszukiwania losowego. Do tego wystarcza im często niewielka liczba kroków.\n",
    "\n",
    "Zaimplementuj 3-warstwową sieć MLP, gdzie pierwsza warstwa ma rozmiar ukryty N, a druga N // 2. Ucz ją optymalizatorem Adam przez maksymalnie 300 epok z cierpliwością 10.\n",
    "\n",
    "Przeszukaj wybrane zakresy dla hiperparametrów:\n",
    "- rozmiar warstw ukrytych (N)\n",
    "- stała ucząca\n",
    "- batch size\n",
    "- siła regularyzacji L2\n",
    "- prawdopodobieństwo dropoutu\n",
    "\n",
    "Wykorzystaj przynajmniej 30 iteracji. Następnie przełącz algorytm na losowy (Optuna także jego implementuje), wykonaj 30 iteracji i porównaj jakość wyników.\n",
    "\n",
    "Przydatne materiały:\n",
    "- [Optuna code examples - PyTorch](https://optuna.org/#code_examples)\n",
    "- [Auto-Tuning Hyperparameters with Optuna and PyTorch](https://www.youtube.com/watch?v=P6NwZVl8ttc)\n",
    "- [Hyperparameter Tuning of Neural Networks with Optuna and PyTorch](https://towardsdatascience.com/hyperparameter-tuning-of-neural-networks-with-optuna-and-pytorch-22e179efc837)\n",
    "- [Using Optuna to Optimize PyTorch Hyperparameters](https://medium.com/pytorch/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PSI",
   "language": "python",
   "name": "psi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.8.13"
=======
   "version": "3.9.12"
>>>>>>> Stashed changes
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "5fe657646842b17350e8781cbdb97152dd69dc224f68f932d8fe4e7c2930368b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
